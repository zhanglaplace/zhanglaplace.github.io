<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZhangFeng&#39;s Blog</title>
  
  <subtitle>HuaZhong University Of Science And Technology</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.enjoyai.site/"/>
  <updated>2018-01-08T03:42:50.758Z</updated>
  <id>http://www.enjoyai.site/</id>
  
  <author>
    <name>ZhangLaplace</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>acmi 2015 Image based Static Facial Expression Recognition with Multiple Deep Network Learning</title>
    <link href="http://www.enjoyai.site/2018/01/08/%5Bacmi%202015%5DImage%20based%20Static%20Facial%20Expression%20Recognition%20with%20Multiple%20Deep%20Network%20Learning/"/>
    <id>http://www.enjoyai.site/2018/01/08/[acmi 2015]Image based Static Facial Expression Recognition with Multiple Deep Network Learning/</id>
    <published>2018-01-08T02:16:01.000Z</published>
    <updated>2018-01-08T03:42:50.758Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h3><p>  该文章作者为EmotiW2015比赛静态表情识别的亚军，采用的方法为cnn的级联，人脸检测方面也采用了当时3种算法的共同检测，<br>通过在FER2013数据库上进行模型预训练，并在SFEW2.0（比赛数据）上fine-tune,从而在比赛的验证集和测试集上取得55.96%和61.29%<br>的准确率,远远超过比赛的baseline(35.96%，39.13%)。<br>  作者本文主要贡献如下:</p><ul><li>1.实现了CNN架构，在表情识别方面性能卓越。</li><li>2.提出了一种数据增强和投票模式，应有提高CNN的性能。</li><li>3.提出了一种优化方法自动的决定级联CNN的权重分配问题。</li></ul><h3 id="FaceDetection"><a href="#FaceDetection" class="headerlink" title="FaceDetection"></a>FaceDetection</h3><p>  由于SFEW数据库给出的静态图像,背景非常复杂，同时为了后续的CNN表情分类，人脸的检测与对齐是非常重要的,因此作者级联了三个state-of-the-art<br>的人脸检测算法，从而保证人脸检测的正确性.三种检测算法为(JDA,DCNN,MoT),图像事先resize为1024x576.总共帧为372,实验结果如下表所示:<br></p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">JDA</th><th style="text-align:center">DCNN</th><th style="text-align:center">MoT</th><th style="text-align:center">JDA+DCNN</th><th style="text-align:center">JDA+DCNN+MoT</th></tr></thead><tbody><tr><td style="text-align:center">  Det #</td><td style="text-align:center">333</td><td style="text-align:center">358</td><td style="text-align:center">352</td><td style="text-align:center">363</td><td style="text-align:center">371</td></tr></tbody></table><h3 id="FacePreprocessing"><a href="#FacePreprocessing" class="headerlink" title="FacePreprocessing"></a>FacePreprocessing</h3><p>  数据预处理对后续的识别有极大的影响，良好的数据预处理可以去除样本间的无关噪声，并能够一定程度的做到数据增强。图像尺寸归一化(48x48)<br>直方图均衡化,去均值除方差。<br>  样本扩增(论文5.2),由于FER数据库包含35000+的图片，因此作者采用fer数据库进行预训练，作者对数据进行了随机的旋转，从而生成了更多的样本,使得网络训练的结果更具有鲁棒性。，样本生成公式以及效果图如下图所示:</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113004285-968705114.png" alt=""></p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113010863-1970781972.png" alt=""></p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113023254-1626393969.png" alt=""></p><p> $$ 样本添加扰动后生成的图样与原始图样的对比 $$</p><h3 id="CNNModel"><a href="#CNNModel" class="headerlink" title="CNNModel"></a>CNNModel</h3><h4 id="基本网络结构"><a href="#基本网络结构" class="headerlink" title="基本网络结构"></a>基本网络结构</h4><p>  5个conv+relu(step:1),3个stochastic pooling层(kernel_size:3*3,step:2),3个全连接层次(带relu+dropout)+softmax,随机初始化参数.<br> 采用sgd优化方式,batch_size：128.</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113037488-914499567.png" alt=""></p><p>$$ CNN网络结构图 $$</p><h4 id="损失函数与级联思想"><a href="#损失函数与级联思想" class="headerlink" title="损失函数与级联思想"></a>损失函数与级联思想</h4><p>   损失函数为softmaxWithLoss，只不过计算的时候一个样本会与其生成的样本loss一起计算，整个网络用FER数据库进行预训练(base_lr:0.005)，当loss突然增加25%或者连续5次观察loss发现loss上升，则手动的减小学习率，最小的学习率设置为0.0001.<br>   由于随机初始化参数，因此作者测试的时候，对多个网络进行级联，从而提高测试的准确率.</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113048472-149667955.png" alt=""></p><p>   $$ 测试时级联CNN网络结构图 $$<br>   相比较简单的加权投票平均的思想，作者认为可以通过学习策略，来决定网络的具体权重。所以提出了一个级联的似然函数，实际可以理解为根据各个网络的输出去学习一个全连接层.其中||w|| == 1.</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>  作者分别列出了在FER,SFEW上数据库的单独训练结果以及，采用提升的级联方式对最终结果的提高。</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113056644-615149323.png" alt=""></p><p>$$ FER数据集train \, val结果 $$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113206004-2123991112.png" alt="">  </p><p>   $$ FER数据集不同Loss与级联方式的结果 $$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113215207-949416415.png" alt=""></p><p>$$ SFEW数据集Vote \, No Vote的结果 $$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113222957-119038902.png" alt=""></p><p>$$ SFEW数据集不同Loss与级联方式的结果 $$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201801/888534-20180108113150363-1709391722.png" alt=""></p><p>$$ SFEW数据库上的测试混淆矩阵 $$</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>  预训练+提升的级联方式对最终的识别效果有效。同时，样本扩增对实验提升也是有作用的。</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="http://www.enjoyai.site/2018/01/08/[acmi%202015]Image%20based%20Static%20Facial%20Expression%20Recognition%20with%20Multiple%20Deep%20Network%20Learning/">http://www.enjoyai.site/2018/01/08/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;ABSTRACT&lt;/h3&gt;&lt;p&gt;  该文章作者为EmotiW2015比赛静态表情识别的亚军，采用的方法为cnn的级联，人脸检
      
    
    </summary>
    
      <category term="Expression" scheme="http://www.enjoyai.site/categories/Expression/"/>
    
    
      <category term="Paper" scheme="http://www.enjoyai.site/tags/Paper/"/>
    
      <category term="Expression" scheme="http://www.enjoyai.site/tags/Expression/"/>
    
  </entry>
  
  <entry>
    <title>Caffe经典网络-cifar10验证</title>
    <link href="http://www.enjoyai.site/2017/12/04/CAFFE-Net-Result/"/>
    <id>http://www.enjoyai.site/2017/12/04/CAFFE-Net-Result/</id>
    <published>2017-12-04T03:44:00.000Z</published>
    <updated>2017-12-09T15:50:43.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>  本文对经典的网络模型在$cifar10$上进行了大致的验证，对比数据包括占用的显存，测试的准确率，训练时间、测试时间以及模型大小进行对比。由于部分网络的显存占用大，因此无法保证batch_size的设置一致，电脑显卡为$GTX1080Ti$，$11G$显存，迭代次数均为64000，初步的实验结果如下表:</p><table><thead><tr><th style="text-align:center">modelStrut</th><th style="text-align:center">Gpu Memory</th><th style="text-align:center">Batch_size</th><th style="text-align:center">Accuracy</th><th style="text-align:center">model Size</th><th style="text-align:center">train time</th><th>test time</th></tr></thead><tbody><tr><td style="text-align:center">Alex</td><td style="text-align:center">~0.5G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.7243</td><td style="text-align:center">3.0M</td><td style="text-align:center">~7min</td><td>~0.67ms</td></tr><tr><td style="text-align:center">LeNet</td><td style="text-align:center">~0.5G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.7840</td><td style="text-align:center">~0.35M</td><td style="text-align:center">~28min</td><td>~0.84ms</td></tr><tr><td style="text-align:center">BN-LeNet</td><td style="text-align:center">~1.35G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.7985</td><td style="text-align:center">~0.35M</td><td style="text-align:center">~16min</td><td>~0.86ms</td></tr><tr><td style="text-align:center">SqeezeNet_v1.1</td><td style="text-align:center">~1.25G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.8030</td><td style="text-align:center">~2.8M</td><td style="text-align:center">~30min</td><td>~3.02ms</td></tr><tr><td style="text-align:center">ResNet20</td><td style="text-align:center">~1.5G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.8303</td><td style="text-align:center">~1.1M</td><td style="text-align:center">~1h5min</td><td>~4.55ms</td></tr><tr><td style="text-align:center">ResNet32</td><td style="text-align:center">~2.5G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.8741</td><td style="text-align:center">~1.9M</td><td style="text-align:center">~1h53min</td><td>~6.91ms</td></tr><tr><td style="text-align:center">ResNet56</td><td style="text-align:center">~4.0G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.8830</td><td style="text-align:center">~3.4M</td><td style="text-align:center">~3h20min</td><td>~11.65ms</td></tr><tr><td style="text-align:center">WRN28_10</td><td style="text-align:center">~10.5G</td><td style="text-align:center">64+128</td><td style="text-align:center">0.8905</td><td style="text-align:center">~140M</td><td style="text-align:center">~12h55min</td><td>~9.31ms</td></tr><tr><td style="text-align:center">Dense30</td><td style="text-align:center">~10.0G</td><td style="text-align:center">32+64</td><td style="text-align:center">0.9195</td><td style="text-align:center">~4.0M</td><td style="text-align:center">~3h38min</td><td>~14.73ms</td></tr><tr><td style="text-align:center">NIN</td><td style="text-align:center">~1.5G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.8411</td><td style="text-align:center">~25M</td><td style="text-align:center">~46min</td><td>~1.99ms</td></tr><tr><td style="text-align:center">GoolgeNet</td><td style="text-align:center">~1.3G</td><td style="text-align:center">128+128</td><td style="text-align:center">0.7865</td><td style="text-align:center">~25M</td><td style="text-align:center">~30min</td><td>~8.42ms</td></tr></tbody></table><a id="more"></a><p>说明：由于训练的时候节省时间，有的网络一起开始训练的，因此显存占用与训练，测试时间可能与单个测试的时间有差异.</p><table><thead><tr><th>modelStruct</th><th>Gpu Memory</th><th>Batch_size</th><th>mAp</th><th>model_size</th><th>train_time</th><th>test time</th></tr></thead><tbody><tr><td>MobileNet SSD</td><td>~7.4G</td><td>24+8</td><td>0.7243</td><td>~23M</td><td>25k–~17h</td><td>~60ms</td></tr><tr><td>faceBox</td><td>~7.7G</td><td>8+4</td><td></td><td></td><td></td></tr></tbody></table><p>##</p><blockquote><p>本文作者： 张峰<br>本文链接：[<a href="http://www.enjoyai.site/2017/12/04/CAFFE-Net-Result/">http://www.enjoyai.site/2017/12/04/CAFFE-Net-Result/</a> )<br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;  本文对经典的网络模型在$cifar10$上进行了大致的验证，对比数据包括占用的显存，测试的准确率，训练时间、测试时间以及模型大小进行对比。由于部分网络的显存占用大，因此无法保证batch_size的设置一致，电脑显卡为$GTX1080Ti$，$11G$显存，迭代次数均为64000，初步的实验结果如下表:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;modelStrut&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Gpu Memory&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Batch_size&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Accuracy&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;model Size&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;train time&lt;/th&gt;
&lt;th&gt;test time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;Alex&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~0.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.7243&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;3.0M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~7min&lt;/td&gt;
&lt;td&gt;~0.67ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;LeNet&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~0.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.7840&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~0.35M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~28min&lt;/td&gt;
&lt;td&gt;~0.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;BN-LeNet&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.35G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.7985&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~0.35M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~16min&lt;/td&gt;
&lt;td&gt;~0.86ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;SqeezeNet_v1.1&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.25G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8030&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~2.8M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~30min&lt;/td&gt;
&lt;td&gt;~3.02ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ResNet20&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8303&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.1M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1h5min&lt;/td&gt;
&lt;td&gt;~4.55ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ResNet32&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~2.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8741&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.9M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1h53min&lt;/td&gt;
&lt;td&gt;~6.91ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ResNet56&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~4.0G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8830&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~3.4M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~3h20min&lt;/td&gt;
&lt;td&gt;~11.65ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;WRN28_10&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~10.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;64+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8905&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~140M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~12h55min&lt;/td&gt;
&lt;td&gt;~9.31ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;Dense30&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~10.0G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;32+64&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.9195&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~4.0M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~3h38min&lt;/td&gt;
&lt;td&gt;~14.73ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;NIN&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.5G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.8411&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~25M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~46min&lt;/td&gt;
&lt;td&gt;~1.99ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;GoolgeNet&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~1.3G&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;128+128&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.7865&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~25M&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;~30min&lt;/td&gt;
&lt;td&gt;~8.42ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="Facial Expression" scheme="http://www.enjoyai.site/categories/Facial-Expression/"/>
    
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
      <category term="Facial Expression" scheme="http://www.enjoyai.site/tags/Facial-Expression/"/>
    
      <category term="Face Recognition" scheme="http://www.enjoyai.site/tags/Face-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV3 安装</title>
    <link href="http://www.enjoyai.site/2017/11/23/InstallOpenCV/"/>
    <id>http://www.enjoyai.site/2017/11/23/InstallOpenCV/</id>
    <published>2017-11-23T07:40:10.000Z</published>
    <updated>2017-11-23T13:38:37.270Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Opencv-安装"><a href="#Opencv-安装" class="headerlink" title="Opencv 安装"></a>Opencv 安装</h1><p>  本文主要说明了在ubuntu上通过源码安装Opencv3，包含各种独立接口.具体可以参照LearnOpencv:<br>  <a href="https://www.learnopencv.com/install-opencv3-on-ubuntu/" target="_blank" rel="external">https://www.learnopencv.com/install-opencv3-on-ubuntu/</a><br><a id="more"></a></p><h2 id="更新系统源"><a href="#更新系统源" class="headerlink" title="更新系统源"></a>更新系统源</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div></pre></td></tr></table></figure><h2 id="安装系统依赖库"><a href="#安装系统依赖库" class="headerlink" title="安装系统依赖库"></a>安装系统依赖库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> 移除先前安装的x264&lt;/h3&gt;</div><div class="line">sudo apt-get remove x264 libx264-dev</div><div class="line"></div><div class="line"><span class="meta">#</span>安装依赖项</div><div class="line">sudo apt-get install build-essential checkinstall cmake pkg-config yasm</div><div class="line">sudo apt-get install git gfortran</div><div class="line">sudo apt-get install libjpeg8-dev libjasper-dev libpng12-dev</div><div class="line"></div><div class="line"><span class="meta">#</span>  Ubuntu 14.04</div><div class="line">sudo apt-get install libtiff4-dev</div><div class="line"><span class="meta">#</span>  Ubuntu 16.04</div><div class="line">sudo apt-get install libtiff5-dev</div><div class="line"></div><div class="line">sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libdc1394-22-dev</div><div class="line">sudo apt-get install libxine2-dev libv4l-dev</div><div class="line">sudo apt-get install libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev</div><div class="line">sudo apt-get install libqt5-dev libgtk2.0-dev libtbb-dev</div><div class="line">sudo apt-get install libatlas-base-dev</div><div class="line">sudo apt-get install libfaac-dev libmp3lame-dev libtheora-dev</div><div class="line">sudo apt-get install libvorbis-dev libxvidcore-dev</div><div class="line">sudo apt-get install libopencore-amrnb-dev libopencore-amrwb-dev</div><div class="line">sudo apt-get install x264 v4l-utils</div><div class="line"></div><div class="line"><span class="meta">#</span> 可选依赖项，一般DL都需要</div><div class="line">sudo apt-get install libprotobuf-dev protobuf-compiler</div><div class="line">sudo apt-get install libgoogle-glog-dev libgflags-dev</div><div class="line">sudo apt-get install libgphoto2-dev libeigen3-dev libhdf5-dev doxygen</div></pre></td></tr></table></figure><h2 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>python2.7与3接口</div><div class="line">sudo apt-get install python-dev python-pip python3-dev python3-pip</div><div class="line">sudo -H pip2 install -U pip numpy</div><div class="line">sudo -H pip3 install -U pip numpy</div></pre></td></tr></table></figure><pre><code>这里按照的python库一般在虚拟环境下进行，以防止和实际全局环境发成冲突</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Install virtual environment</div><div class="line">sudo pip2 install virtualenv virtualenvwrapper</div><div class="line">sudo pip3 install virtualenv virtualenvwrapper</div><div class="line">echo "# Virtual Environment Wrapper"  &gt;&gt; ~/.bashrc</div><div class="line">echo "source /usr/local/bin/virtualenvwrapper.sh" &gt;&gt; ~/.bashrc</div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line"><span class="meta">#</span>########### For Python 2 ############</div><div class="line"><span class="meta">#</span> create virtual environment</div><div class="line">mkvirtualenv facecourse-py2 -p python2</div><div class="line">workon facecourse-py2</div><div class="line"></div><div class="line"><span class="meta">#</span> now install python libraries within this virtual environment</div><div class="line">pip install numpy scipy matplotlib scikit-image scikit-learn ipython</div><div class="line"></div><div class="line"><span class="meta">#</span> quit virtual environment</div><div class="line">deactivate</div><div class="line"><span class="meta">#</span>#####################################</div><div class="line"></div><div class="line"><span class="meta">#</span>########### For Python 3 ############</div><div class="line"><span class="meta">#</span> create virtual environment</div><div class="line">mkvirtualenv facecourse-py3 -p python3</div><div class="line">workon facecourse-py3</div><div class="line"></div><div class="line"><span class="meta">#</span> now install python libraries within this virtual environment</div><div class="line">pip install numpy scipy matplotlib scikit-image scikit-learn ipython</div><div class="line"></div><div class="line"><span class="meta">#</span> quit virtual environment</div><div class="line">deactivate</div><div class="line"><span class="meta">#</span>#####################################</div></pre></td></tr></table></figure><h2 id="OpenCV与OpenCV-contrib"><a href="#OpenCV与OpenCV-contrib" class="headerlink" title="OpenCV与OpenCV_contrib"></a>OpenCV与OpenCV_contrib</h2><pre><code>此处均从github的源码下载</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/opencv/opencv.git</div><div class="line">cd opencv</div><div class="line">git checkout 3.3.1</div><div class="line">cd ..</div><div class="line"><span class="meta">#</span>contrib</div><div class="line">git clone https://github.com/opencv/opencv_contrib.git</div><div class="line">cd opencv_contrib</div><div class="line">git checkout 3.3.1</div><div class="line">cd ..</div></pre></td></tr></table></figure><p>   开始编译<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">cd opencv</div><div class="line">mkdir build</div><div class="line">cd build</div><div class="line"></div><div class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</div><div class="line">  -D CMAKE_INSTALL_PREFIX=/usr/local \</div><div class="line">  -D INSTALL_C_EXAMPLES=ON \</div><div class="line">  -D INSTALL_PYTHON_EXAMPLES=ON \</div><div class="line">  -D WITH_TBB=ON \</div><div class="line">  -D WITH_V4L=ON \</div><div class="line">  -D WITH_QT=ON \</div><div class="line">  -D WITH_OPENGL=ON \</div><div class="line">  -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \</div><div class="line">  -D BUILD_EXAMPLES=ON ..</div><div class="line"></div><div class="line"><span class="meta">  #</span> find out number of CPU cores in your machine</div><div class="line"> nproc</div><div class="line"><span class="meta"> #</span> substitute 4 by output of nproc</div><div class="line"> make -j4</div><div class="line"> sudo make install</div><div class="line"> sudo sh -c 'echo "/usr/local/lib" &gt;&gt; /etc/ld.so.conf.d/opencv.conf'</div><div class="line"> sudo ldconfig</div></pre></td></tr></table></figure></p><p>  $python使用opencv需要将opencv编译的cv2.so加入到python环境中，实际放在python \\<br>  下的site-package或者dist-package，查找cv2.so位置$<br>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">find /usr/local/lib -type f -name "cv2*.so"</div><div class="line"></div><div class="line"><span class="meta">#</span> 结果出现可能如下</div><div class="line"><span class="meta">#</span>########### For Python 2 ############</div><div class="line"><span class="meta">#</span># binary installed in dist-packages</div><div class="line">/usr/local/lib/python2.6/dist-packages/cv2.so</div><div class="line">/usr/local/lib/python2.7/dist-packages/cv2.so</div><div class="line"><span class="meta">#</span># binary installed in site-packages</div><div class="line">/usr/local/lib/python2.6/site-packages/cv2.so</div><div class="line">/usr/local/lib/python2.7/site-packages/cv2.so</div><div class="line"></div><div class="line"><span class="meta">#</span>########### For Python 3 ############</div><div class="line"><span class="meta">#</span># binary installed in dist-packages</div><div class="line">/usr/local/lib/python3.5/dist-packages/cv2.cpython-35m-x86_64-linux-gnu.so</div><div class="line">/usr/local/lib/python3.6/dist-packages/cv2.cpython-36m-x86_64-linux-gnu.so</div><div class="line"><span class="meta">#</span># binary installed in site-packages</div><div class="line">/usr/local/lib/python3.5/site-packages/cv2.cpython-35m-x86_64-linux-gnu.so</div><div class="line">/usr/local/lib/python3.6/site-packages/cv2.cpython-36m-x86_64-linux-gnu.so</div></pre></td></tr></table></figure></p><p>  制作软连接，需要注意路径<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">############ For Python 2 ############</span></div><div class="line"><span class="built_in">cd</span> ~/.virtualenvs/facecourse-py2/lib/python2.7/site-packages</div><div class="line">ln -s /usr/<span class="built_in">local</span>/lib/python2.7/dist-packages/cv2.so cv2.so</div><div class="line"></div><div class="line"><span class="comment">############ For Python 3 ############</span></div><div class="line"><span class="built_in">cd</span> ~/.virtualenvs/facecourse-py3/lib/python3.6/site-packages</div><div class="line">ln -s /usr/<span class="built_in">local</span>/lib/python3.6/dist-packages/cv2.cpython-36m-x86_64-linux-gnu.so cv2.so</div></pre></td></tr></table></figure></p><h2 id="测试OpenCV"><a href="#测试OpenCV" class="headerlink" title="测试OpenCV"></a>测试OpenCV</h2><h3 id="C-接口"><a href="#C-接口" class="headerlink" title="C++接口"></a>C++接口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> compile</span></div><div class="line">g++ -std=c++11 removeRedEyes.cpp `pkg-config --libs --cflags opencv` -o removeRedEyes</div><div class="line"><span class="meta">#</span><span class="bash"> run</span></div><div class="line">./removeRedEyes</div></pre></td></tr></table></figure><h3 id="python-接口"><a href="#python-接口" class="headerlink" title="python 接口"></a>python 接口</h3><p>  激活虚拟环境<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"><span class="comment">########### For Python 2 ############</span></span></div><div class="line">workon facecourse-py2</div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"><span class="comment">########### For Python 3 ############</span></span></div><div class="line">workon facecourse-py3</div><div class="line"></div><div class="line">ipython</div><div class="line">import cv2</div><div class="line">print cv2.__version__</div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> 或者执行python removeRedEyes.py</span></div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> 退出虚拟环境</span></div><div class="line">deactivate</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="http://www.enjoyai.site/2017/11/23/InstallOpenCV/">http://www.enjoyai.site/2017/11/23/InstallOpenCV/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Opencv-安装&quot;&gt;&lt;a href=&quot;#Opencv-安装&quot; class=&quot;headerlink&quot; title=&quot;Opencv 安装&quot;&gt;&lt;/a&gt;Opencv 安装&lt;/h1&gt;&lt;p&gt;  本文主要说明了在ubuntu上通过源码安装Opencv3，包含各种独立接口.具体可以参照LearnOpencv:&lt;br&gt;  &lt;a href=&quot;https://www.learnopencv.com/install-opencv3-on-ubuntu/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.learnopencv.com/install-opencv3-on-ubuntu/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Opencv" scheme="http://www.enjoyai.site/categories/Opencv/"/>
    
    
      <category term="OpenCV" scheme="http://www.enjoyai.site/tags/OpenCV/"/>
    
      <category term="Ubuntu" scheme="http://www.enjoyai.site/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Caffe 安装分析</title>
    <link href="http://www.enjoyai.site/2017/11/21/InstallCaffe/"/>
    <id>http://www.enjoyai.site/2017/11/21/InstallCaffe/</id>
    <published>2017-11-21T07:40:10.000Z</published>
    <updated>2017-11-23T13:08:20.323Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe安装"><a href="#Caffe安装" class="headerlink" title="Caffe安装"></a>Caffe安装</h1><p>  实际上在windows上安装过多次caffe了，无论是BLVC版本的还是Microsoft版本的，ubuntu的按照也进行过，这段时间在自己笔记本上<br>又折腾了下caffe安装，发现其实直接照着官方的是最方便快捷的。<br>  具体可以参照 <a href="http://caffe.berkeleyvision.org/install_apt.html" target="_blank" rel="external">Installation_instructions</a><br> <a id="more"></a></p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>  根据系统的不同，ubuntu分为两种安装方式。Ubuntu17.04以即上的可以直接apt-get<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt install caffe-cpu # cpu only</div><div class="line">sudo apt install caffe-cuda # gpu</div></pre></td></tr></table></figure></p><p>  其他版本的ubuntu也可以分为两种方式安装，但是依赖项是必须的，本文仅介绍简易的软件源中快速安装。源码安装可以参考本人的另外一篇博客：<a href="http://www.cnblogs.com/LaplaceAkuir/p/6262632.html" target="_blank" rel="external">ubunt16.04 cud8.0 caffe 安装</a></p><h2 id="Nvidia显卡驱动"><a href="#Nvidia显卡驱动" class="headerlink" title="Nvidia显卡驱动"></a>Nvidia显卡驱动</h2><p>  由于要使用GPU，所以先要查看自己显卡所匹配的显卡驱动，网址：<a href="http://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="external">nvidia</a> ,下载run文件。<br>  由于目前显卡和cuda更新迅速，容易造成笔记本循环登录，因此安装显卡驱动是关闭图形界面。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> ctrl +alt +F1 进入tty1，</div><div class="line">sudo service lightdm stop</div><div class="line">sudo ./Nvidia-.....run 执行安装</div><div class="line">sudo reboot</div></pre></td></tr></table></figure></p><h2 id="Cuda和CuDnn"><a href="#Cuda和CuDnn" class="headerlink" title="Cuda和CuDnn"></a>Cuda和CuDnn</h2><p>  安装较为简单，官网下载，在安装cuda是需要注意显卡安装选项选择no即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">sudo sh cuda_8.0.44_linux.run --override</div><div class="line"><span class="meta">#</span> 安装结束后</div><div class="line">sudo vim ~/.bashrc  //末尾添加</div><div class="line">export CUDA_HOME=/usr/local/cuda-8.0</div><div class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH</div><div class="line">export PATH=/usr/local/cuda-8.0/bin:$PATH</div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line"><span class="meta">#</span> 测试</div><div class="line">cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery</div><div class="line">make -j32</div><div class="line">sudo ./deviceQuery</div></pre></td></tr></table></figure><p>   cudnn下载后接下的include lib拷贝到cuda的安装路径，并设置链接。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo ln -s libcudnn.so.xxx libcudnn</div><div class="line">sudo ln -s libcudnn.so.xx libcudd.so</div><div class="line">sudo ldconfig</div></pre></td></tr></table></figure></p><h1 id="其他依赖项"><a href="#其他依赖项" class="headerlink" title="其他依赖项"></a>其他依赖项</h1><p>  其他依赖项安装可以直接从软件源获取，当然也可以自己源码安装。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> protobuf,snappy,leveldb,opencv,hdf5,boost ,python-opencv,glog ,gflag,lmdb</div><div class="line">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler</div><div class="line">sudo apt-get install --no-install-recommends libboost-all-dev</div><div class="line">sudo apt-get install python-dev python-opencv</div><div class="line">sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</div></pre></td></tr></table></figure></p><p>  关于blas可以选择atlas，openblas和MKL，由于后续cmake方式安装默认atlas，所以本人也用次<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install libatlas-base-dev</div><div class="line"><span class="meta">#</span> openblas也很方便</div><div class="line">sudo apt-get install libopenblas-dev</div></pre></td></tr></table></figure></p><p>  Matlab的接口可以自己先安装matlab ，此处省略，同时python可以安装anconda来管理库</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h2><p>  下载BVLC的caffe<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone --recursive https://github.com/BVLC/caffe</div></pre></td></tr></table></figure></p><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><h3 id="1-Make方式"><a href="#1-Make方式" class="headerlink" title="1.Make方式"></a>1.Make方式</h3><p>  Make方式为官方的编译方式，但是在后续caffe的使用时会稍显麻烦，这里要注意根据安装的库以及自己是否使用gpu、cudnn以及bals的选择等作出修改<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cp Makefile.config.example Makefile.config</div><div class="line"><span class="meta">#</span> For CPU &amp; GPU accelerated Caffe, no changes are needed.</div><div class="line"><span class="meta">#</span>For cuDNN acceleration using NVIDIA’s proprietary cuDNN software, uncomment the USE_CUDNN := 1 switch in #Makefile.config. cuDNN is sometimes but not always faster than Caffe’s GPU acceleration.</div><div class="line"><span class="meta">#</span>For CPU-only Caffe, uncomment CPU_ONLY := 1 in Makefile.config.</div><div class="line"><span class="meta">#</span> Adjust Makefile.config (for example, if using Anaconda Python, or if cuDNN is desired)</div><div class="line">make all -j8</div><div class="line">make test</div><div class="line">make runtest</div></pre></td></tr></table></figure></p><h3 id="2-Cmake方式"><a href="#2-Cmake方式" class="headerlink" title="2.Cmake方式"></a>2.Cmake方式</h3><p>  Cmake方式针对自己使用Caffe以及从软件源安装Caffe的用户来说简直不要更方便.<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir build</div><div class="line">cd build</div><div class="line">cmake ..</div><div class="line">make all</div><div class="line">make install</div><div class="line">make runtest</div></pre></td></tr></table></figure></p><p>  由于自己使用Caffe不仅仅是停留在训练，可能很多都要具体的测试实际的项目，因此相比于Make方式，Cmake的优势就大大体现出来了。具体例子可以在我的github上看到<a href="https://github.com/zhanglaplace/MTCNN-Accelerate-Onet" target="_blank" rel="external">https://github.com/zhanglaplace/MTCNN-Accelerate-Onet</a><br>  编译自己的项目，仅仅需要写一个简单的CMakeLists.txt文件，并且文件内的内容可以保证百分之九十的不变，这使得验证算法和项目变得相当方便.(强烈推荐)<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">cmake_minimum_required(VERSION <span class="number">2.9</span>)</div><div class="line">project(MTCNN_Accelerate-Onet)  <span class="comment">// 根据自己工程名字修改</span></div><div class="line"></div><div class="line"><span class="meta">#set(CMAKE_CXX_FLAGS <span class="meta-string">"$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11"</span>)</span></div><div class="line"><span class="built_in">set</span>(CMAKE_CXX_STANDARD <span class="number">11</span>)</div><div class="line"></div><div class="line">find_package(OpenCV)</div><div class="line"></div><div class="line">find_package(Caffe REQUIRED)</div><div class="line"><span class="meta">#message(FATAL_ERROR $&#123;Caffe_INCLUDE_DIRS&#125;)</span></div><div class="line">include_directories($&#123;Caffe_INCLUDE_DIRS&#125;)</div><div class="line"></div><div class="line"><span class="built_in">set</span>(SOURCE_FILES main.cpp mtcnn.cpp mtcnn.h) <span class="comment">// 根据自己实际源码修改</span></div><div class="line">add_executable(MTCNN_Accelerate-Onet $&#123;SOURCE_FILES&#125;)</div><div class="line"></div><div class="line">target_link_libraries(MTCNN_Accelerate-Onet $&#123;OpenCV_LIBS&#125; )</div><div class="line">target_link_libraries(MTCNN_Accelerate-Onet $&#123;Caffe_LIBRARIES&#125;)</div></pre></td></tr></table></figure></p><p>  可能出现找不到caffe.pb.h的情况，解决方法在caffe的主目录下:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">protoc src/caffe/layers/caffe.proto --cpp_out=.</div><div class="line">cd include/caffe/</div><div class="line">mkdir proto</div><div class="line">cd ../../</div><div class="line">mv src/caffe/proto/caffe.pb.h include/caffe/layers/proto</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="http://www.enjoyai.site/2017/11/21/InstallCaffe/">http://www.enjoyai.site/2017/11/21/InstallCaffe/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe安装&quot;&gt;&lt;a href=&quot;#Caffe安装&quot; class=&quot;headerlink&quot; title=&quot;Caffe安装&quot;&gt;&lt;/a&gt;Caffe安装&lt;/h1&gt;&lt;p&gt;  实际上在windows上安装过多次caffe了，无论是BLVC版本的还是Microsoft版本的，ubuntu的按照也进行过，这段时间在自己笔记本上&lt;br&gt;又折腾了下caffe安装，发现其实直接照着官方的是最方便快捷的。&lt;br&gt;  具体可以参照 &lt;a href=&quot;http://caffe.berkeleyvision.org/install_apt.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Installation_instructions&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Scale层解析</title>
    <link href="http://www.enjoyai.site/2017/11/09/Caffe_Scale%E5%B1%82%E8%A7%A3%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/11/09/Caffe_Scale层解析/</id>
    <published>2017-11-09T01:26:33.000Z</published>
    <updated>2017-11-09T12:35:02.136Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-Scale层解析"><a href="#Caffe-Scale层解析" class="headerlink" title="Caffe Scale层解析"></a>Caffe Scale层解析</h1><p>  前段时间做了caffe的batchnormalization层的解析，由于整体的BN层实现在Caffe是分段实现的，因此今天抽时间总结下Scale层次，也会后续两个层做合并做下铺垫。<br><a id="more"></a></p><h2 id="基本公式梳理"><a href="#基本公式梳理" class="headerlink" title="基本公式梳理"></a>基本公式梳理</h2><p>  Scale层主要完成 $top = alpha*bottom+ beta$的过程，则层中主要有两个参数$alpha$与$beta$,<br>  求导会比较简单。<br>  $$ \frac{\partial y}{\partial x} = alpha ;\quad \frac{\partial y}{\partial alpha} = x;\quad \frac{\partial y}{\partial beta} = 1$$<br>  需要注意的是$alpha$与$beta$均为向量，针对输入的$channels$进行的处理，因此不能简单的认定为一个$float$的实数。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>  该部分将结合源码实现解析$scale$层:<br>  在Caffe proto中ScaleParameter中对Scale有如下几个参数：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">axis [<span class="keyword">default</span> = <span class="number">1</span>] ; 默认的处理维度</div><div class="line">num_axes [<span class="keyword">default</span> = <span class="number">1</span>] ; <span class="comment">//在BN中可以忽略，主要决定第二个bottom</span></div><div class="line">FillerParameter filler ; <span class="comment">//两个FillerParameter即决定初始alpha和beta的填充方式。</span></div><div class="line"><span class="comment">//决定是否学习bias，如果不学习，则可以简化为alpha*x = y</span></div><div class="line">optional <span class="keyword">bool</span> bias_term = <span class="number">4</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line">FillerParameter bias_filler;</div></pre></td></tr></table></figure></p><h3 id="基本成员变量"><a href="#基本成员变量" class="headerlink" title="基本成员变量"></a>基本成员变量</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// caffe的scale层实现+beta调用了bias层。。。。。。。。。。</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt;&gt; bias_layer_; /</div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;bias_bottom_vec_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; bias_propagate_down_;</div><div class="line"><span class="keyword">int</span> bias_param_id_;</div><div class="line"></div><div class="line">Blob&lt;Dtype&gt; sum_multiplier_;</div><div class="line">Blob&lt;Dtype&gt;sum_result_;</div><div class="line">Blob&lt;Dtype&gt; temp_;</div><div class="line"><span class="keyword">int</span> axis_;</div><div class="line"><span class="keyword">int</span> outer_dim_,inner_dim_,scale_dim_;</div></pre></td></tr></table></figure><p> 基本成员变量主要包含了Bias层的参数以及Scale层完成对应通道的标注工作。</p><h3 id="基本成员函数"><a href="#基本成员函数" class="headerlink" title="基本成员函数"></a>基本成员函数</h3><p> 主要包含了LayerSetup,Reshape ,Forward和Backward ，内部调用的时候bias_term为true的时候会调用biasLayer的相关函数.</p><h4 id="LayerSetup-层次的建立"><a href="#LayerSetup-层次的建立" class="headerlink" title="LayerSetup,层次的建立"></a>LayerSetup,层次的建立</h4> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ScaleLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">    <span class="keyword">const</span> ScaleParameter param = <span class="keyword">this</span>-&gt;layer_param_-&gt;scale_param();</div><div class="line">    <span class="keyword">if</span> (bottom.size() == <span class="number">1</span> &amp;&amp; <span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">//区分测试与训练，测试时 blobs-已经有值</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(bottom.size() == <span class="number">1</span>)&#123;</div><div class="line">       <span class="comment">// 考虑BN的scale 不需要考虑axes</span></div><div class="line">       axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());<span class="comment">// 1 通道</span></div><div class="line">       <span class="keyword">const</span> <span class="keyword">int</span> num_axes = param.num_axes(); <span class="comment">// 1</span></div><div class="line">       <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">1</span>);<span class="comment">// alpha;</span></div><div class="line"></div><div class="line">       <span class="comment">//这么大一串，实际就是blobs_[0].reset(new Blob&lt;Dtype&gt;(vector&lt;int&gt;(C)));</span></div><div class="line">       <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_iterator&amp; shape_start =</div><div class="line">       bottom[<span class="number">0</span>]-&gt;shape().begin() + axis_;</div><div class="line">       <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_iterator&amp; shape_end =</div><div class="line">       (num_axes == <span class="number">-1</span>) ? bottom[<span class="number">0</span>]-&gt;shape().end() : (shape_start + num_axes);</div><div class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;scale_shape(shape_start, shape_end);</div><div class="line">       <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(scale_shape));</div><div class="line"></div><div class="line">       FillerParameter filler_param(param.filler());</div><div class="line">       <span class="keyword">if</span> (!param.has_filler()) &#123; <span class="comment">//没写明填充模式</span></div><div class="line">         filler_param.set_type(<span class="string">"constant"</span>);</div><div class="line">         filler_param.set_value(<span class="number">1</span>);</div><div class="line">       &#125;</div><div class="line">       <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt;&gt;filler(GetFiller&lt;Dtype&gt;(filler_param));</div><div class="line">       filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 处理需不需要bias</span></div><div class="line">    <span class="keyword">if</span> (param.bias_term()) &#123;</div><div class="line">      <span class="function">LayerParameter <span class="title">layer_param</span><span class="params">(<span class="keyword">this</span>-&gt;layer_param_)</span></span>;</div><div class="line">      layer_param.set_type(<span class="string">"Bias"</span>);</div><div class="line">      BiasParameter* bias_param = layer_param_.mutable_bias_param();</div><div class="line">      bias_param-&gt;set_axis(param.aixs());</div><div class="line">      <span class="keyword">if</span> (bottom.size() &gt; <span class="number">1</span>) &#123;</div><div class="line">         bias_param-&gt;set_num_axes(bottom[<span class="number">1</span>]-&gt;num_axes());</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span>&#123;</div><div class="line">        bias_param-&gt;set_num_axes(param.num_axes());<span class="comment">//bn层走下面</span></div><div class="line">      &#125;</div><div class="line">      bias_param-&gt;mutable_filler()-&gt;CopyFrom(param.bias_filler());</div><div class="line">      bias_layer_ = LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param);</div><div class="line">      bias_bottom_vec_.resize(<span class="number">1</span>);</div><div class="line">      bias_bottom_vec_[<span class="number">0</span>] = bottom[<span class="number">0</span>];</div><div class="line">      bias_layer_-&gt;Setup(bias_bottom_vec_,top);</div><div class="line">      bias_param_id = <span class="keyword">this</span>-&gt;blobs_.size(); <span class="comment">//1 alpha 此处增加个beta</span></div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(bias_param_id_+<span class="number">1</span>); <span class="comment">// 2</span></div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[bias_param_id] = bias_layer_-&gt;blobs()[<span class="number">0</span>];</div><div class="line">      bias_propagate_down_.resize(<span class="number">1</span>,<span class="literal">false</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">this</span>-&gt;param_propagate_down_.resize(<span class="keyword">this</span>-&gt;blobs_.size(),<span class="literal">true</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p> Scale层的一部分在完整BN中是不需要考虑的，完整BN中bottomSize为1，num_axes默认为1，blobs_[0]为长度为C的向量，bias需要调用caffe的bias层，所以会看着比较麻烦。</p><h4 id="Reshape-调整输入输出与中间变量"><a href="#Reshape-调整输入输出与中间变量" class="headerlink" title="Reshape 调整输入输出与中间变量"></a>Reshape 调整输入输出与中间变量</h4><p>  Reshape层完成许多中间变量的size初始化<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Reshape操作</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ScaleLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">    <span class="keyword">const</span> ScaleParameter param = <span class="keyword">this</span>-&gt;layer_param_-&gt;scale_param();</div><div class="line">    Blob&lt;Dtype&gt;* scale = (bottom.size() &gt; <span class="number">1</span>)?bottom[<span class="number">1</span>]:<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get();</div><div class="line">    axis_=(scale-&gt;num_axes()==<span class="number">0</span>)?<span class="number">0</span>:botom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());</div><div class="line">    <span class="comment">//这里做了下比较 bottom的NCHW axis_ = 1 则 C == C</span></div><div class="line">    CHECK_EQ(bottom[<span class="number">0</span>]-&gt;shape(axis_) = scale-&gt;shape(<span class="number">0</span>));</div><div class="line"></div><div class="line">    outer_dim_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,axis_);<span class="comment">// n</span></div><div class="line">    scale_dim = scale-&gt;count(); <span class="comment">//c</span></div><div class="line">    inner_dim_ = bottom[<span class="number">0</span>]-&gt;count(axis+<span class="number">1</span>);<span class="comment">// hw</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> (bottom[<span class="number">0</span>] == top[<span class="number">0</span>]) &#123;</div><div class="line">      <span class="comment">// Layer得top和bottom同名 in-place computation</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">bool</span> scale_param = (bottom.size() == <span class="number">1</span>); <span class="comment">//true</span></div><div class="line">      <span class="keyword">if</span> (!scale_param || (scale_param &amp;&amp; <span class="keyword">this</span>-&gt;param_propagate_down_[<span class="number">0</span>]) &#123;</div><div class="line">        <span class="comment">// 后面一个条件成立，需要backward</span></div><div class="line">        <span class="comment">//防止修改top时，bottom改变，做临时,因为求导要用到原始的bottom-data</span></div><div class="line">        temp_.ReshapeLike(*bottom[<span class="number">0</span>]);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">      top[<span class="number">0</span>]-&gt;ReshapeLike(*bottom[<span class="number">0</span>]);<span class="comment">//</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">//类似于bn的num-by—tran 保存中间的NC结果 NC*1*1*1</span></div><div class="line">    sum_result_.Reshape(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>,outer_dim_*scale_dim_));</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> sum_mult_size = <span class="built_in">std</span>::max(outer_dim_,inner_dim_);</div><div class="line">    <span class="comment">// 为什么不类似于BN做两个temp vector呢</span></div><div class="line">    sum_multiplier_.Reshape(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>,sum_mult_size));</div><div class="line">    <span class="keyword">if</span> (sum_multiplier_.cpu_data()[sum_mult_size<span class="number">-1</span>] != Dtype(<span class="number">1</span>)) &#123;</div><div class="line">      caffe_set(sum_mult_size,Dtype(<span class="number">1</span>),sum_multiplier_.mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (bias_layer_) &#123;</div><div class="line">      bias_bottom_vec_[<span class="number">0</span>] = top[<span class="number">0</span>];</div><div class="line">      bias_layer_-&gt;Reshape(bias_bottom_vec_,top);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>  Reshape操作同BN的基本相似，只不过此处只是新建了两个中间变量，sum_multiplier_和sum_result_.</p><h4 id="Forward-前向计算"><a href="#Forward-前向计算" class="headerlink" title="Forward 前向计算"></a>Forward 前向计算</h4><p>  前向计算，在BN中国紧跟着BN的归一化输出，完成乘以alpha与+bias的操作，由于alpha与bias均为C的向量，因此需要先进行广播。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ScaleLayer&lt;Dtype&gt;::Forward_cpu(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>] == top[<span class="number">0</span>]) &#123;</div><div class="line">     <span class="comment">// 先进行一次临时拷贝复制</span></div><div class="line">     caffe_copy(bottom[<span class="number">0</span>]-&gt;count(),bottom_data,temp_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> Dtype* scale_data = (bottom.size() &gt; <span class="number">1</span>)?bottom[<span class="number">1</span>]:</div><div class="line">                      <span class="keyword">this</span>-&gt;blios_[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="comment">// 这里的遍历实际上和广播的类似，一种是每次操作inner_dim个元素，一种是讲alpha</span></div><div class="line">  <span class="comment">// 广播到整个feature_map，然后再调用一次cpu_scale</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; outer_dim_; n++) &#123; <span class="comment">// n</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> d = <span class="number">0</span>; d &lt; scale_dim_; d++) &#123; <span class="comment">//c</span></div><div class="line">       <span class="keyword">const</span> Dtype factory = scale_data[d];<span class="comment">// 取某一个通道的值</span></div><div class="line">       caffe_cpu_scale(inner_dim,factory,bottom_data,top_data);</div><div class="line">       top_data += inner_dim_;</div><div class="line">       bottom_data += inner_dim;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (bias_layer_) &#123;</div><div class="line">     bias_layer_-&gt;Forward(bias_bottom_vec_,top);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h4 id="Backward-反向计算"><a href="#Backward-反向计算" class="headerlink" title="Backward 反向计算"></a>Backward 反向计算</h4><p>  主要求解三个梯度，对alpha 、beta和输入的bottom(此处的temp)<br>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> ScaleLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">    <span class="keyword">if</span> (bias_layer_ &amp;&amp;  <span class="comment">// 默认false</span></div><div class="line">      <span class="keyword">this</span>-&gt;param_propagate_down_[<span class="keyword">this</span>-&gt;param_propagate_down_.size()<span class="number">-1</span>]) &#123;</div><div class="line">      bias_layer_-&gt;Backward(top,bias_propagate_down_,bias_bottom_vec_);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">const</span> scale_param = (bottom.size() == <span class="number">1</span>);</div><div class="line">    Blob&lt;Dtype&gt;* scale = scale_param? <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get(),bottom[<span class="number">1</span>];</div><div class="line">    <span class="keyword">if</span> ((!scale_param &amp;&amp; propagate_down[<span class="number">1</span>])|| <span class="comment">//bottomsize大于1的时候判断</span></div><div class="line">      (scale_param&amp;&amp;<span class="keyword">this</span>-&gt;param_propagate_down_[<span class="number">0</span>])) &#123;<span class="comment">// 1个输入是判断alpha</span></div><div class="line">      <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">      Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">      <span class="keyword">const</span> in_place = (bottom[<span class="number">0</span>] == top[<span class="number">0</span>]);</div><div class="line">      <span class="comment">// 需要做备份 如果输入输出同名，需要注意用原来临时的temp</span></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data =in_place?temp_.cpu_data():bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">      <span class="comment">// BN中输入是NCHW,而alpha和beta仅仅针对C</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">bool</span> is_eltwise = (bottom[<span class="number">0</span>]-&gt;count() == scale-&gt;count());<span class="comment">//不相等的</span></div><div class="line">      Dtype* product= is_eltwise?scale_.mutable_cpu_diff():</div><div class="line">      (in_place?temp_.mutable_cpu_data():bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff());</div><div class="line">      caffe_mul(top[<span class="number">0</span>]-&gt;count(),top_diff,bottom_data,product);</div><div class="line">      <span class="keyword">if</span> (!is_eltwise) &#123; <span class="comment">// blobs_与输入对不上</span></div><div class="line">        Dtype* sum_result_ = <span class="literal">NULL</span>;</div><div class="line">        <span class="keyword">if</span> (inner_dim_ == <span class="number">1</span>) &#123;</div><div class="line">          <span class="comment">//H*W == 1;</span></div><div class="line">          sum_result_ = product;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(sum_result_.count() == <span class="number">1</span>)&#123; <span class="comment">// 1*1*1*1</span></div><div class="line">          <span class="keyword">const</span> Dtype* sum_mult_  = sum_multiplier_.cpu_data();</div><div class="line">          Dtype* scale_diff = scale-&gt;mutable_cpu_diff();</div><div class="line">          <span class="keyword">if</span> (scale_param) &#123; <span class="comment">//true</span></div><div class="line">            Dtype result = caffe_cpu_dot(inner_dim,product,sum_mult);</div><div class="line">            *scale_diff += result; <span class="comment">//H*W的相乘</span></div><div class="line">          &#125;</div><div class="line">          <span class="keyword">else</span>&#123;</div><div class="line">            *scale_diff = caffe_cpu_dot(inner_dim_,product,sum_mult);</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span>&#123;</div><div class="line">          <span class="keyword">const</span> Dtype* sum_mult = sum_multiplier_.mutable_cpu_data();</div><div class="line">          sum_result = (outer_dim_ == <span class="number">1</span>)? <span class="comment">// nc如果n==1就直接幅值C</span></div><div class="line">          scale_.mutable_cpu_diff():sum_result_.mutable_cpu_data();</div><div class="line"></div><div class="line">          <span class="comment">//NC HW  * HW*1 = NC*1 HW全1</span></div><div class="line">          caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,sum_result.count(),inner_dim,</div><div class="line">          Dtype(<span class="number">1</span>),product,sum_mult,Dtype(<span class="number">0</span>),Dtype(<span class="number">0</span>),sum_result);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (out_dim_ != <span class="number">1</span>) &#123;</div><div class="line">           <span class="keyword">const</span> Dtype* sum_mult  = sum_multiplier_.cpu_data();</div><div class="line">           Dtype* scale_diff = scale-&gt;mutable_cpu_diff();</div><div class="line">           <span class="keyword">if</span> (scale_dim_ ==<span class="number">1</span>) &#123;</div><div class="line">             <span class="keyword">if</span> (scale_param) &#123; <span class="comment">// C==1直接计算 NC*NC</span></div><div class="line">                Dtype result = caffe_cpu_dot(outer_dim_,sum_mult_,sum_result);</div><div class="line">                *scale_diff += result;</div><div class="line">             &#125;</div><div class="line">             <span class="keyword">else</span>&#123;</div><div class="line">               *scale_diff =  caffe_cpu_dot(outer_dim_,sum_mult_,sum_result);</div><div class="line">             &#125;</div><div class="line">           &#125;</div><div class="line">           <span class="keyword">else</span>&#123;  <span class="comment">//如果C != 1 需要gemv,(num * channels)^t * 1 *num*1</span></div><div class="line">             caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans,outer_dim_,scale_dim,</div><div class="line">             Dtype(<span class="number">1</span>),sum_result,sum_mult,Dtype(scale_param),scale_diff);</div><div class="line">           &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123; <span class="comment">//x求导</span></div><div class="line">     <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">     <span class="keyword">const</span> Dtype* scale_data = scale-&gt;cpu_data();</div><div class="line">     Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">     <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; outer_dim_; n++) &#123;</div><div class="line">       <span class="keyword">for</span> (<span class="keyword">size_t</span> d = <span class="number">0</span>; d &lt; inner_dim_; d++) &#123;</div><div class="line">          <span class="keyword">const</span> Dtype factory = scale_data[d];</div><div class="line">          caffe_cpu_scale(inner_dim_,factory,top_diff,bottom_diff);</div><div class="line">          bottom_diff += inner_dim_;</div><div class="line">          top_diff += inner_dim_;</div><div class="line">       &#125;</div><div class="line">     &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>Caffe中的Scale层由于不仅仅作为BN的后续层，因此看着会比较绕，实际去上去掉很多if else 后会清晰很多</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="http://www.enjoyai.site/2017/11/09/Caffe_Scale%E5%B1%82%E8%A7%A3%E6%9E%90/">http://www.enjoyai.site/2017/11/09</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-Scale层解析&quot;&gt;&lt;a href=&quot;#Caffe-Scale层解析&quot; class=&quot;headerlink&quot; title=&quot;Caffe Scale层解析&quot;&gt;&lt;/a&gt;Caffe Scale层解析&lt;/h1&gt;&lt;p&gt;  前段时间做了caffe的batchnormalization层的解析，由于整体的BN层实现在Caffe是分段实现的，因此今天抽时间总结下Scale层次，也会后续两个层做合并做下铺垫。&lt;br&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用衡量指标</title>
    <link href="http://www.enjoyai.site/2017/11/07/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87/"/>
    <id>http://www.enjoyai.site/2017/11/07/目标检测常用衡量指标/</id>
    <published>2017-11-07T11:16:01.000Z</published>
    <updated>2017-11-08T14:46:09.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标检测常用衡量指标"><a href="#目标检测常用衡量指标" class="headerlink" title="目标检测常用衡量指标"></a>目标检测常用衡量指标</h1><p> 目标检测中，存在很多常见的模型评估与选择的度量方法，本文结合周志华老师的&lt;机器学习&gt;，以及自己的理解对常见的度量方法做一个总结。</p><h2 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h2><p>   常见的评估方法，我们在进行样本学习与测试的过程中，通常采用多种方式来进行样本集合的分类。<br>   (1) 留出法<br>   将样本按比例分为两个子集，一个为训练集，一个为验证集，通常保证训练集和验证集的样本类别服从同分布。多次划分后取平均的实验结果作为最终的结果。<br>   (2) 交叉验证法 (最常用)<br>   通过对数据集划分为k个大小基本相同，分布基本相似的子集，每次从中选取K-1次进行训练，1个进行测试，则可以得到K组结果，最终根据k组的结果进行统计，一般为5折或者10折。<br>   (3) 自助法<br>   数据集较小的时候，通过自身的bootstrapping方法，多次有放回的采样增加样本集合。<br><a id="more"></a></p><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>  通常我们定量一个模型的好坏，根据错误率和准确率来定量，但是在实际问题中，还有很多衡量的指标。</p><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>  (1) 常用均方误差来衡量MSE<br>  $$ E(f;D)= \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2$$<br>  (2) 和方差 SSE<br>  $$E(f;D) = \sum_{i=1}^{m}w_i(f(x_i)-y_i)^2$$<br>  (3) 均方根误差RMSE<br>  $$RMSE = \sqrt{MSE}= \sqrt{\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2}$$</p><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>  (1) 错误率与准确率<br>  $$E(f;D) = \frac{1}{m}\sum_{i=1}^{m} I(f(x_i) \neq y_i)$$<br>  $$acc(f;D) =  \frac{1}{m}\sum_{i=1}^{m} I(f(x_i) = y_i) = 1-E(f;D)$$<br>  (2) 查准率(精确度)和查全率(召回率)<br>  $$表2.1 分类结果混淆矩阵$$<br>| 真实\预测 | 正样本 | 负样本 |<br>| ——— | —— | —— |<br>| 正样本    | TP     | FN     |<br>| 负样本    | FP     | TN      |<br>则，查准率与召回率公式如下<br>$$ P = \frac{TP}{TP+FP}$$<br>$$ R = \frac{TP}{TP+FN}$$<br>一般来说查全率高，召回率往往低，召回率高，查全率就偏低，因此，常用F1Score来衡量:<br>$$ F1 = \frac{2*P*R}{P+R}$$</p><p>通常在做目标检测与分类时，会设定不同的阈值，目标会根据阈值划分到不同的类别，因此通过对分数阈值排序，可以得到多组的PR值，从而可以画出PR曲线，通常用y=x与PR曲线的交点来作为平衡点评估模型的好坏。<br>  (3) ROC于AUC<br>  在做识别任务中，通常产生一个分数值，通过与阈值的对比，从而判断样本属于正例还是负例，而ROC曲线，则用以衡量真正例率与假正例率的比例.<br>  $$ TPR = \frac{TP}{TP+FN} $$<br>  $$ FPR = \frac{FP}{FP+TN} $$<br>  通过设置不同的阈值，可以得到不同的TPR和FPR，从而做出ROC曲线<br>  而AUC用来衡量ROC曲线与坐标轴的面积，面积越大，则代表模型越好，通常:<br>  $$ AUC = \frac{1}{2}\sum_{i=1}^{m}(x_{i+1}-x_i)*(y_{i+1}-y_i) $$<br>  (4) FAR 与 FRR<br>  FAR即(False Acceptance Rate),FRR即(False rejection Rate)，一般用来衡量二分类，例如人脸中，FAR代表不同的人识别为同一个人的概率，而FRR代表一个人识别为不同人的概率.如果300个人，每个人两张图片，则总共的比较次数为 $C_{600}^{2}$,其中应当识别成为同一个人的有300对，应当识别为不同的人的有 $C_{300}^{2}*C_{2}^{1}*C_{2}^{1}$,则通过计算300对中识别成不是一个人的个数与不同人识别为同一个人的概率来衡量模型的好坏。</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="http://www.enjoyai.site/2017/10/30/Paper%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93%20Day1/">http://www.enjoyai.site/2017/10/30</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;目标检测常用衡量指标&quot;&gt;&lt;a href=&quot;#目标检测常用衡量指标&quot; class=&quot;headerlink&quot; title=&quot;目标检测常用衡量指标&quot;&gt;&lt;/a&gt;目标检测常用衡量指标&lt;/h1&gt;&lt;p&gt; 目标检测中，存在很多常见的模型评估与选择的度量方法，本文结合周志华老师的&amp;lt;机器学习&amp;gt;，以及自己的理解对常见的度量方法做一个总结。&lt;/p&gt;
&lt;h2 id=&quot;基础介绍&quot;&gt;&lt;a href=&quot;#基础介绍&quot; class=&quot;headerlink&quot; title=&quot;基础介绍&quot;&gt;&lt;/a&gt;基础介绍&lt;/h2&gt;&lt;p&gt;   常见的评估方法，我们在进行样本学习与测试的过程中，通常采用多种方式来进行样本集合的分类。&lt;br&gt;   (1) 留出法&lt;br&gt;   将样本按比例分为两个子集，一个为训练集，一个为验证集，通常保证训练集和验证集的样本类别服从同分布。多次划分后取平均的实验结果作为最终的结果。&lt;br&gt;   (2) 交叉验证法 (最常用)&lt;br&gt;   通过对数据集划分为k个大小基本相同，分布基本相似的子集，每次从中选取K-1次进行训练，1个进行测试，则可以得到K组结果，最终根据k组的结果进行统计，一般为5折或者10折。&lt;br&gt;   (3) 自助法&lt;br&gt;   数据集较小的时候，通过自身的bootstrapping方法，多次有放回的采样增加样本集合。&lt;br&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Batch Normalization推导</title>
    <link href="http://www.enjoyai.site/2017/11/06/Caffe_BatchNormalization/"/>
    <id>http://www.enjoyai.site/2017/11/06/Caffe_BatchNormalization/</id>
    <published>2017-11-06T05:44:33.000Z</published>
    <updated>2017-11-08T14:45:03.283Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-BatchNormalization-推导"><a href="#Caffe-BatchNormalization-推导" class="headerlink" title="Caffe BatchNormalization 推导"></a>Caffe BatchNormalization 推导</h1><p>  总所周知，BatchNormalization通过对数据分布进行归一化处理，从而使得网络的训练能够快速并简单，在一定程度上还能防止网络的过拟合，通过仔细看过Caffe的源码实现后发现，Caffe是通过BN层和Scale层来完整的实现整个过程的。<br><a id="more"></a></p><h2 id="谈谈理论与公式推导"><a href="#谈谈理论与公式推导" class="headerlink" title="谈谈理论与公式推导"></a>谈谈理论与公式推导</h2><p>  那么再开始前，先进行必要的公式说明：定义$L$为网络的损失函数，BN层的输出为$y$，根据反向传播目前已知 $\frac{\partial L}{\partial y_i}$,其中：<br>   $$y_i = \frac{x_i-\overline{x}}{\sqrt{\delta^2+\epsilon}},\quad\overline x = \frac{1}{m}\sum_{i=1}^{m}x_i,\quad \delta^2 = \frac{1}{m}\sum_{i=1}^{m}(x_i-\overline x)^2,\quad 求\frac{\partial L}{\partial x_i}$$</p><p>  推导的过程中应用了链式法则：<br>  $$ \frac{\partial L}{\partial x_i} = \sum_{j=1}^{m}{\frac{\partial L}{\partial y_j}*\frac{\partial y_j}{\partial x_i}} $$<br>  则只需要着重讨论公式 $\frac{\partial y_j}{\partial x_i}$</p><p>  分布探讨：</p><p>  (1) $\overline x$对$x_i$的导函数<br>  $$\frac{\partial \overline x}{\partial x_i} = \frac{1}{m} $$</p><p>  (2) $\delta^2$对$x_i$的导函数<br>  $$\frac{\partial \delta^2}{\partial x_i} = \frac{1}{m}(\sum_{j=1}^{m}2*(x_j-\overline x)*(-\frac{1}{m}))+2(x_i-\overline x)$$<br>  由于 $\sum_{j=1}^{m}2*(x_j-\overline x) = 2* \sum_{i=1}^{m}x_i - n*\overline x = 0$</p><p>  所以： $\frac{\partial \delta^2}{\partial x_i} = \frac{2}{m}*(x_i-\overline x)$</p><p>  具体推导：<br>  $$\frac{\partial y_j}{\partial x_i} = \frac{\partial{\frac{x_j -\overline x}{\sqrt{\delta^2+\epsilon}}}}{\partial x_i} $$<br>  此处当$j$等于$i$成立时时，分子求导多一个 $x_i$的导数</p><p>  $$\frac{\partial y_j}{\partial x_i} = -\frac{1}{m}(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j - \overline x)\quad\quad i \neq j $$<br>  $$\frac{\partial y_j}{\partial x_i} = (1-\frac{1}{m})(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j - \overline x)\quad\quad i = j$$</p><p>  根据上式子，我们代入链式法则的式子<br>  $$\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i}*(\delta^2+\epsilon)^{-1/2} + \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}*(-\frac{1}{m}(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j-\overline x))$$</p><p>  我们提出 $(\delta^2+\epsilon)^{-1/2}:$<br>  $$\frac{\partial L}{\partial x_i} = (\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}-\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}(\delta^2+\epsilon)^{-1}(x_i-\overline x)(x_j-\overline x))<br>  \\<br>  =(\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}-\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}y_jy_i   \\<br>  =(\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \frac{1}{m}\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}-\frac{1}{m}y_i\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}y_j)$$</p><p>  至此，我们可以对应到caffe的具体实现部分<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// dE(Y)/dX =</span></div><div class="line"><span class="comment">//   (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)</span></div><div class="line"><span class="comment">//     ./ sqrt(var(X) + eps)</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// where \cdot and ./ are hadamard product and elementwise division,</span></div></pre></td></tr></table></figure></p><h2 id="谈谈具体的源码实现"><a href="#谈谈具体的源码实现" class="headerlink" title="谈谈具体的源码实现"></a>谈谈具体的源码实现</h2><p>  知道了BN层的公式与原理，接下来就是具体的源码解析，由于考虑到的情况比较多，所以$Caffe$中的BN的代码实际上不是那么的好理解，需要理解，BN的归一化是如何归一化的：<br>  H<em>W的归一化，求出N</em>C个均值与方差，然后N个均值与方差求出一个均值与方差的Vector，size为C，即相同通道的一个mini_batch的样本求出一个mean和variance</p><h3 id="成员变量"><a href="#成员变量" class="headerlink" title="成员变量"></a>成员变量</h3><p>   BN层的成员变量比较多，由于在bn的实现中，需要记录mean_,variance_,归一化的值，同时根据训练和测试实现也有所差异。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Blob&lt;Dtype&gt; mean_,variance_,temp_,x_norm; <span class="comment">//temp_保存(x-mean_x)^2</span></div><div class="line"><span class="keyword">bool</span> use_global_stats_;<span class="comment">//标注训练与测试阶段</span></div><div class="line">Dtype moving_average_fraction_;</div><div class="line"><span class="keyword">int</span> channels_;</div><div class="line">Dtype eps_; <span class="comment">// 防止分母为0</span></div><div class="line"></div><div class="line"><span class="comment">// 中间变量，理解了BN的具体过程即可明了为什么需要这些</span></div><div class="line">Blob&lt;Dtype&gt; batch_sum_multiplier_; <span class="comment">// 长度为N*1，全为1，用以求和</span></div><div class="line">Blob&lt;Dtype&gt; num_by_chans_; <span class="comment">// 临时保存H*W的结果，length为N*C</span></div><div class="line">Blob&lt;Dtype&gt; spatial_sum_multiplier_; <span class="comment">// 统计HW的均值方差使用</span></div></pre></td></tr></table></figure></p><h3 id="成员函数"><a href="#成员函数" class="headerlink" title="成员函数"></a>成员函数</h3><p>  成员函数主要也是LayerSetUp,Reshape,Forward和Backward,下面是具体的实现：</p><h4 id="LayerSetUp-层次的建立，相应数据的读取"><a href="#LayerSetUp-层次的建立，相应数据的读取" class="headerlink" title="LayerSetUp,层次的建立，相应数据的读取"></a>LayerSetUp,层次的建立，相应数据的读取</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LayerSetUp函数的具体实现</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>&#123;</div><div class="line">    <span class="comment">// 参见proto中添加的BatchNormLayer</span></div><div class="line">    BathcNormParameter param = <span class="keyword">this</span>-&gt;layer_param_.batch_norm_param();</div><div class="line">    moving_average_fraction_ = param.moving_average_fraction();<span class="comment">//默认0.99</span></div><div class="line"></div><div class="line">    <span class="comment">//这里有点多余，好处是防止在测试的时候忘写了use_global_stats时默认true</span></div><div class="line">    use_global_stats_ = <span class="keyword">this</span>-&gt;phase_ == TEST;</div><div class="line">    <span class="keyword">if</span> (param.has_use_global_stat()) &#123;</div><div class="line">       use_global_stats_ = param.use_global_stats();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (bottom[<span class="number">0</span>]-&gt;num_axes() == <span class="number">1</span>) &#123; <span class="comment">//这里基本看不到为什么.....???</span></div><div class="line">       channels_  = <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123; <span class="comment">// 基本走下面的通道，因为输入是NCHW</span></div><div class="line">      channels_ = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    eps_ = param.eps(); <span class="comment">// 默认1e-5</span></div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;  <span class="comment">// 测试的时候有值了，保存了均值方差和系数</span></div><div class="line">      <span class="comment">//保存mean,variance,</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">      <span class="comment">// BN层的内部参数的初始化</span></div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">3</span>); <span class="comment">// 均值滑动，方差滑动，滑动系数</span></div><div class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;sz;</div><div class="line">      sz.push_back(channels_);</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz)); <span class="comment">// C</span></div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz)); <span class="comment">// C</span></div><div class="line">      sz[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz)); <span class="comment">// 1</span></div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</div><div class="line">         caffe_set(<span class="keyword">this</span>-&gt;blobs_[i]-&gt;count(),Dtype(<span class="number">0</span>),</div><div class="line">                   <span class="keyword">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h4 id="Reshape-根据BN层在网络的位置，调整bottom和top的shape"><a href="#Reshape-根据BN层在网络的位置，调整bottom和top的shape" class="headerlink" title="Reshape,根据BN层在网络的位置，调整bottom和top的shape"></a>Reshape,根据BN层在网络的位置，调整bottom和top的shape</h4><p>Reshape层主要是完成中间变量的值，由于是按照通道求取均值和方差，而CaffeBlob是NCHW,因此先求取了HW,后根据BatchN求最后的输出C,因此有了中间的batch_sum_multiplier_和spatial_sum_multiplier_以及num_by_chans_其中num_by_chans_与前两者不想同，前两者为方便计算，初始为1，而num_by_chans_为中间过渡<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">    <span class="keyword">if</span> (bottom[<span class="number">0</span>]-&gt;num_axes() &gt;= <span class="number">1</span>) &#123;</div><div class="line">      CHECK_EQ(bottom[<span class="number">0</span>]-&gt;shape(<span class="number">1</span>),channels_);</div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;ReshapeLike(*bottom[<span class="number">0</span>]); <span class="comment">// Reshape(bottom[0]-&gt;shape());</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;sz;</div><div class="line">    sz.push_back(channels_);</div><div class="line">    mean_.Reshape(sz);</div><div class="line">    variance_.Reshape(sz);</div><div class="line">    temp_.ReshapeLike(*bottom[<span class="number">0</span>]);</div><div class="line">    x_norm_.ReshapeLike(*bottom[<span class="number">0</span>]);</div><div class="line">    sz[<span class="number">0</span>] = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>); <span class="comment">//N</span></div><div class="line">    <span class="comment">// 后续会初始化为1，为求Nbatch的均值和方差</span></div><div class="line">    batch_sum_multiplier_.Reshape(sz);</div><div class="line">    caffe_set(batch_sum_multiplier_.count(),Dtype(<span class="number">1</span>),</div><div class="line">              batch_sum_multiplier_.mutable_cpu_data());</div><div class="line"></div><div class="line">    <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>);<span class="comment">//H*W</span></div><div class="line">    <span class="keyword">if</span> (spatial_sum_multiplier_.num_axes() == <span class="number">0</span> ||</div><div class="line">      spatial_sum_multiplier_.shape(<span class="number">0</span>) != spatial_dim) &#123;</div><div class="line">      sz[<span class="number">0</span>] = spatial_dim;</div><div class="line">      spatial_sum_multiplier_.Reshape(sz); <span class="comment">//初始化1，方便求和</span></div><div class="line">      caffe_set(spatial_sum_multiplier_.count(),Dtype(<span class="number">1</span>)</div><div class="line">              spatial_sum_multiplier_.mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// N*C,保存H*W后的结果,会在计算中结合data与spatial_dim求出</span></div><div class="line">    <span class="keyword">int</span> numbychans = channels_*bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>);</div><div class="line">    <span class="keyword">if</span> (num_by_chans_.num_axes() == <span class="number">0</span> ||</div><div class="line">        num_by_chans_.shape(<span class="number">0</span>) != numbychans) &#123;</div><div class="line">        sz[<span class="number">0</span>] = numbychans;</div><div class="line">        num_by_chans_.Reshape(sz);</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p><h4 id="Forward-前向计算"><a href="#Forward-前向计算" class="headerlink" title="Forward 前向计算"></a>Forward 前向计算</h4><p>前向计算，根据公式完成前计算，x_norm与top相同，均为归一化的值<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">      <span class="comment">// 想要完成前向计算，必须计算相应的均值与方差，此处的均值与方差均为向量的形式c</span></div><div class="line"></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">      Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">      <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>);<span class="comment">// N</span></div><div class="line">      <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>); <span class="comment">//H*W</span></div><div class="line">      <span class="keyword">if</span> (bottom[<span class="number">0</span>] != top[<span class="number">0</span>]) &#123;</div><div class="line">        caffe_copy(top[<span class="number">0</span>]-&gt;count(),bottom_data,top_data);<span class="comment">//先复制一下</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (use_global_stats_) &#123; <span class="comment">// 测试阶段,使用全局的均值</span></div><div class="line">        <span class="keyword">const</span> Dtype scale_factory = this_-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>] == <span class="number">0</span>?</div><div class="line">          <span class="number">0</span>:<span class="number">1</span>/<span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>];</div><div class="line">        <span class="comment">// 直接载入训练的数据 alpha*x = y</span></div><div class="line">        caffe_cpu_scale(mean_.count(),scale_factory,</div><div class="line">          this_blobs_[<span class="number">0</span>]-&gt;cpu_data(),mean_.mutable_cpu_data());</div><div class="line">        caffe_cpu_scale(variance_.count(),scale_factory,</div><div class="line">          this_blobs_[<span class="number">1</span>]-&gt;cpu_data(),variance_.mutable_cpu_data());</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span>&#123; <span class="comment">//训练阶段  compute mean</span></div><div class="line">        <span class="comment">//1.计算均值,先计算HW的，在包含N</span></div><div class="line">        <span class="comment">// caffe_cpu_gemv 实现 y =  alpha*A*x+beta*y;</span></div><div class="line">        <span class="comment">// 输出的是channels_*num,</span></div><div class="line">        <span class="comment">//每次处理的列是spatial_dim，由于spatial_sum_multiplier_初始为1，即NCHW中的</span></div><div class="line">        <span class="comment">// H*W各自相加，得到N*C*average，此处多除以了num，下一步可以不除以</span></div><div class="line">        caffe_cpu_gemv&lt;Dtype&gt;(CBlasNoTrans,channels_*num,spatial_dim,</div><div class="line">          <span class="number">1.</span>/(spatial_dim*num),bottom_data,spatial_sum_multiplier_.cpu_data()</div><div class="line">          ,<span class="number">0.</span>,num_by_chans_.mutable_cpu_data());</div><div class="line"></div><div class="line">        <span class="comment">//2.计算均值，计算N各的平均值.</span></div><div class="line">        <span class="comment">// 由于输出的是channels个均值，因此需要转置</span></div><div class="line">        <span class="comment">// 上一步得到的N*C的均值，再按照num求均值，因为batch_sum全部为1,</span></div><div class="line">        caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,<span class="number">1</span>,</div><div class="line">          num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(),</div><div class="line">          <span class="number">0</span>,mean_.mutable_cpu_data());</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// 此处的均值已经保存在mean_中了</span></div><div class="line">      <span class="comment">// 进行 x - mean_x 操作，需要注意按照通道，即先确定x属于哪个通道.</span></div><div class="line">      <span class="comment">// 因此也是进行两种，先进行H*W的减少均值</span></div><div class="line">      <span class="comment">// caffe_cpu_gemm 实现alpha * A*B + beta* C</span></div><div class="line">      <span class="comment">// 输入是num*1 * 1* channels_,输出是num*channels_</span></div><div class="line">      caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num,channels_,<span class="number">1</span>,<span class="number">1</span>,</div><div class="line">        batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),<span class="number">0</span>,</div><div class="line">        num_by_chans_.mutable_cpu_data());</div><div class="line"></div><div class="line">      <span class="comment">//同上，输入是num*channels_*1 * 1* spatial = NCHW</span></div><div class="line">      <span class="comment">// top_data = top_data - mean;</span></div><div class="line">      caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num*channels_,</div><div class="line">        spatial_dim,<span class="number">1</span>,<span class="number">-1</span>,num_by_chans_.cpu_data(),</div><div class="line">        spatial_sum_multiplier_.cpu_data(),<span class="number">1</span>, top_data());</div><div class="line"></div><div class="line">      <span class="comment">// 解决完均值问题，接下来就是解决方差问题</span></div><div class="line">      <span class="keyword">if</span> (use_global_stats_) &#123; <span class="comment">// 测试的方差上述已经读取了</span></div><div class="line">          <span class="comment">// compute variance using var(X) = E((X-EX)^2)</span></div><div class="line">          <span class="comment">// 此处的top已经为x-mean_x了</span></div><div class="line">          caffe_powx(top[<span class="number">0</span>]-&gt;count(),top_data,Dtype(<span class="number">2</span>),</div><div class="line">            temp_.mutable_cpu_data());<span class="comment">//temp_保存(x-mean_x)^2</span></div><div class="line"></div><div class="line">          <span class="comment">// 同均值一样，此处先计算spatial_dim的值</span></div><div class="line">          caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,num*channels_,spatial_dim,</div><div class="line">            <span class="number">1.</span>/(num*spatial_dim),temp_.cpu_data(),</div><div class="line">            spatial_sum_multiplier_.cpu_data(),<span class="number">0</span>,</div><div class="line">            num_by_chans_.mutable_cpu_data();</div><div class="line">          )</div><div class="line">          caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,<span class="number">1.</span>,</div><div class="line">            num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(),</div><div class="line">            <span class="number">0</span>,variance_.mutable_cpu_data());<span class="comment">// E((X_EX)^2)</span></div><div class="line"></div><div class="line">          <span class="comment">//均值和方差计算完成后，需要更新batch的滑动系数</span></div><div class="line">          <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] *= moving_average_fraction_;</div><div class="line">          <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] += <span class="number">1</span>;</div><div class="line">          caffe_cpu_axpby(mean_.count(),Dtype(<span class="number">1</span>),mean_.cpu_data(),</div><div class="line">            moving_average_fraction_,<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_data());</div><div class="line"></div><div class="line">          <span class="keyword">int</span> m = bottom[<span class="number">0</span>]-&gt;count()/channels_;</div><div class="line">          Dtype bias_correction_factor = m &gt; <span class="number">1</span>? Dtype(m)/(m<span class="number">-1</span>):<span class="number">1</span>;</div><div class="line">          caffe_cpu_axpby(variance_.count(),bias_correction_factor,</div><div class="line">            variance_.cpu_data(),moving_average_fraction_,</div><div class="line">            <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;mutable_cpu_data());</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 方差求个根号,加上eps为防止分母为0</span></div><div class="line">      caffe_add_scalar(variance_.count(),eps_,variance_.mutable_cpu_data());</div><div class="line">      caffe_powx(variance_.count(),variance_.cpu_data(),Dtype(<span class="number">0.5</span>),</div><div class="line">                variance_.mutable_cpu_data());</div><div class="line"></div><div class="line">     <span class="comment">// top_data = x-mean_x/sqrt(variance_),此处的top_data已经转化为x-mean_x了</span></div><div class="line">     <span class="comment">// 同减均值，也要分C--N*C和  N*C --- N*C*H*W</span></div><div class="line">     <span class="comment">// N*1 *  1*C == N*C</span></div><div class="line">     caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num,channels_,<span class="number">1</span>,<span class="number">1</span>,</div><div class="line">          batch_sum_multiplier_.cpu_data(),variance_.cpu_data(),<span class="number">0</span>,</div><div class="line">          num_by_chans_.mutable_cpu_data());</div><div class="line">    <span class="comment">// NC*1 * 1* spatial_dim = NCHW</span></div><div class="line">     caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num*channels_,spatial_dim,</div><div class="line">        <span class="number">1</span>, <span class="number">1.</span>,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(), <span class="number">0</span>,</div><div class="line">        temp_.mutable_cpu_data());</div><div class="line">    <span class="comment">// temp最终保存的是sqrt（方差+eps)</span></div><div class="line">     caffe_cpu_div(top[<span class="number">0</span>].count(),top_data,temp_.cpu_data(),top_data);</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p><p>整个forward过程按照x-mean/variance的过程进行，包含了求mean和variance，他们都是C*1的向量，然后输入的是NCHW,因此通过了gemm操作做广播填充到整个featuremap然后完成减mean和除以方差的操作。同时需要注意caffe的inplace操作，所以用x_norm保存原始的top值，后续修改也不会影响它。</p><h4 id="Backward过程，根据梯度，反向计算"><a href="#Backward过程，根据梯度，反向计算" class="headerlink" title="Backward过程，根据梯度，反向计算"></a>Backward过程，根据梯度，反向计算</h4><p>  Backward过程会根据前面所推导的公式进行计算，具体的实现如下面所示.<br> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff;</div><div class="line">    <span class="keyword">if</span> (bottom[<span class="number">0</span>] != top[<span class="number">0</span>]) &#123; <span class="comment">// 判断是否同名</span></div><div class="line">        top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        caffe_copy(x_norm_.count(),top[<span class="number">0</span>]-&gt;cpu_diff(),x_norm_.mutable_cpu_diff());</div><div class="line">        top_diff = x_norm_.cpu_diff();</div><div class="line">    &#125;</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">if</span> (use_global_stats_) &#123; <span class="comment">// 测试阶段</span></div><div class="line">        caffe_div(temp_.count(),top_diff,temp_.cpu_data(),bottom_diff);</div><div class="line">        <span class="keyword">return</span> ; <span class="comment">// 测试阶段不需要计算梯度。</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">const</span> Dtype* top_data = x_norm_.cpu_data();</div><div class="line">    <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>); <span class="comment">//n</span></div><div class="line">    <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>); <span class="comment">// H*W</span></div><div class="line"></div><div class="line">    <span class="comment">// 根据推导的公式开始具体计算。</span></div><div class="line">    <span class="comment">// dE(Y)/dX =</span></div><div class="line">    <span class="comment">//   (top_diff- mean(top_diff) - mean(top_diff \cdot Y) \cdot Y)</span></div><div class="line">    <span class="comment">//     ./ sqrt(var(X) + eps)</span></div><div class="line"></div><div class="line">    <span class="comment">// sum(top_diff \cdot Y) ,y为x_norm_ NCHW,求取的均先求C通道的均值</span></div><div class="line">    caffe_mul(temp_.count(),top_data,top_diff,bottom_diff);</div><div class="line">    <span class="comment">//NC*HW* HW*1 =  NC*1</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,channels_*num,spatial_dim,<span class="number">1.</span>,</div><div class="line">       bottom_diff,spatial_sum_multiplier_.cpu_data(),<span class="number">0</span>,</div><div class="line">       num_by_chans_.mutable_cpu_data());</div><div class="line">    <span class="comment">// (NC)^T*1 * N*1 =  C*1</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,<span class="number">1.</span>,</div><div class="line">       num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(),</div><div class="line">       <span class="number">0</span>,mean_.mutable_cpu_data());</div><div class="line"></div><div class="line">   <span class="comment">//reshape broadcast</span></div><div class="line">   <span class="comment">// N*1  * 1* C = N* C</span></div><div class="line">   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num,channels_,<span class="number">1</span>,<span class="number">1</span>,</div><div class="line">       batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),<span class="number">0</span>,</div><div class="line">       num_by_chans_.mutable_cpu_data());</div><div class="line">   <span class="comment">// N*C *1  * 1* HW =  NC* HW</span></div><div class="line">   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num*channels_,spatial_dim,</div><div class="line">       <span class="number">1</span>,<span class="number">1.</span>,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(),<span class="number">0</span>,</div><div class="line">       bottom_diff);</div><div class="line">   <span class="comment">//相当与 sum (DE/DY .\cdot Y)</span></div><div class="line"></div><div class="line">   <span class="comment">// sum(dE/dY \cdot Y) \cdot Y</span></div><div class="line">   caffe_mul(temp_.count(), top_data, bottom_diff, bottom_diff);</div><div class="line"></div><div class="line">   <span class="comment">// 完成了右边一个部分，还有前面的 sum(DE/DY)和DE/DY</span></div><div class="line">   <span class="comment">// 再完成sum(DE/DY)</span></div><div class="line">   caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,channels_*num,spatial_dim,<span class="number">1</span>,</div><div class="line">       top_diff,spatial_sum_multiplier_.cpu_data(),<span class="number">0.</span>,</div><div class="line">       num_by_chans_.mutable_cpu_data());</div><div class="line">   caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,<span class="number">1.</span>,</div><div class="line">       num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(),<span class="number">0</span>,</div><div class="line">       mean_.mutable_cpu_data());</div><div class="line">   <span class="comment">//reshape broadcast</span></div><div class="line">   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num,channels_,<span class="number">1</span>,</div><div class="line">       <span class="number">1</span>,batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),<span class="number">0</span>,</div><div class="line">       num_by_chans_.mutable_cpu_data());</div><div class="line">   <span class="comment">// 现在完成了sum(DE/DY)+y*sum(DE/DY.\cdot y)</span></div><div class="line">   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num*channels_,spatial_dim,</div><div class="line">       <span class="number">1</span>,<span class="number">1.</span>,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(),<span class="number">1</span>,</div><div class="line">       bottom_diff);</div><div class="line"></div><div class="line">   <span class="comment">//top_diff - 1/m * (sum(DE/DY)+y*sum(DE/DY.\cdot y))</span></div><div class="line">   caffe_cpu_axpby(bottom[<span class="number">0</span>]-&gt;count(),Dtype(<span class="number">1</span>),top_diff,</div><div class="line">       Dtype(<span class="number">-1</span>/(num*spatial_dim)),bottom_diff);</div><div class="line"></div><div class="line">   <span class="comment">// 前面还有常数项 variance_+eps</span></div><div class="line">   caffe_div(temp_.count(),bottom_diff,temp_.cpu_data(),bottom_diff);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>  backward的过程也是先求出通道的值，然后广播到整个feature_map,来回两次，然后调用axpby完成 top_diff - 1/m<em> (sum(top_diff)+y</em>sum(top_diff*y)))这里的y针对通道进行。</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="http://www.enjoyai.site/2017/11/06/Caffe_BatchNormalization/">http://www.enjoyai.site/2017/11/06/</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-BatchNormalization-推导&quot;&gt;&lt;a href=&quot;#Caffe-BatchNormalization-推导&quot; class=&quot;headerlink&quot; title=&quot;Caffe BatchNormalization 推导&quot;&gt;&lt;/a&gt;Caffe BatchNormalization 推导&lt;/h1&gt;&lt;p&gt;  总所周知，BatchNormalization通过对数据分布进行归一化处理，从而使得网络的训练能够快速并简单，在一定程度上还能防止网络的过拟合，通过仔细看过Caffe的源码实现后发现，Caffe是通过BN层和Scale层来完整的实现整个过程的。&lt;br&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Paper阅读总结Day1</title>
    <link href="http://www.enjoyai.site/2017/10/30/Paper%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93%20Day1/"/>
    <id>http://www.enjoyai.site/2017/10/30/Paper阅读总结 Day1/</id>
    <published>2017-10-30T06:16:01.000Z</published>
    <updated>2017-11-09T12:57:06.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Paper阅读总结Day1"><a href="#Paper阅读总结Day1" class="headerlink" title="Paper阅读总结Day1"></a>Paper阅读总结Day1</h1><h2 id="1-Convolutional-Neural-Networks-For-Facial-Expression-Recognition"><a href="#1-Convolutional-Neural-Networks-For-Facial-Expression-Recognition" class="headerlink" title="1.Convolutional Neural Networks For Facial Expression Recognition"></a>1.Convolutional Neural Networks For Facial Expression Recognition</h2><h3 id="文章思想"><a href="#文章思想" class="headerlink" title="文章思想"></a>文章思想</h3><p>  简单的一篇关于表情识别的文章，运用简单的CNN结构，在文章中对比了深层次的网络结构和浅层次的网络结构的效果，同时将前向的最后一层特征与自己手动提取的Hog特征做了特征融合，并重新训练一个全连接层，得到的效果与不用特征融合效果一致。</p><h3 id="文章使用数据集"><a href="#文章使用数据集" class="headerlink" title="文章使用数据集"></a>文章使用数据集</h3><p>  Fer2013 Database，通过浅层次和深层次的横向对比与 加入hog与不加hog的横向对比</p><h3 id="实验效果与结论"><a href="#实验效果与结论" class="headerlink" title="实验效果与结论"></a>实验效果与结论</h3><p>  深层次的CNN准确率大概是65%，加入HOG与不加效果基本一致，结论是否定了Hog特征融合对表情识别有效果的提升。<br><a id="more"></a></p><h2 id="2-Island-Loss-for-Learning-Discriminative-Features-in-Facial-Expression-Recognition"><a href="#2-Island-Loss-for-Learning-Discriminative-Features-in-Facial-Expression-Recognition" class="headerlink" title="2.Island Loss for Learning Discriminative Features in Facial Expression Recognition"></a>2.Island Loss for Learning Discriminative Features in Facial Expression Recognition</h2><h3 id="文章思想-1"><a href="#文章思想-1" class="headerlink" title="文章思想"></a>文章思想</h3><p>  简单的在centerLoss的基础上，添加了衡量各类别类心的loss，由于centerloss只关注了样本到类心的类内距离，而IslandLoss在关注类心距离的同时，添加了类间距离的loss，采用余弦距离衡量类心的相似程度。<br>$$\zeta = \zeta_S+\lambda (\zeta_C+\lambda_1\zeta_{is})$$</p><h3 id="文章使用的数据集"><a href="#文章使用的数据集" class="headerlink" title="文章使用的数据集"></a>文章使用的数据集</h3><p>  Oulu-CASIA database 、 Extended Cokn-kanada和MMI database，fer2013</p><h3 id="实验效果与结论-1"><a href="#实验效果与结论-1" class="headerlink" title="实验效果与结论"></a>实验效果与结论</h3><p>  在各个数据集上的表现都优于SoftmaxLoss以及 CenterLoss+SoftmaxLoss.需要把握各个loss的权重调节</p><h2 id="3-End-to-End-Deep-Learning-for-Single-Step-Real-Time-Facial-Expression-Recognition"><a href="#3-End-to-End-Deep-Learning-for-Single-Step-Real-Time-Facial-Expression-Recognition" class="headerlink" title="3.End to End Deep Learning for Single Step Real-Time Facial Expression Recognition"></a>3.End to End Deep Learning for Single Step Real-Time Facial Expression Recognition</h2><h3 id="文章思想-2"><a href="#文章思想-2" class="headerlink" title="文章思想"></a>文章思想</h3><p>  实现一个集合人脸检测与人脸表情分类的一体的网络—Faster-RCNN。替换了Faster-RCNN前面的预训练的网络结构，采取了VGG16和ResNet50，做对比后VGG可以达到10fps，ResNet50-5fps，感觉略有水分。</p><h3 id="文章使用数据集-1"><a href="#文章使用数据集-1" class="headerlink" title="文章使用数据集"></a>文章使用数据集</h3><p>  Extended Cokn-kanada 和 FER2013</p><h3 id="实验效果与结论-2"><a href="#实验效果与结论-2" class="headerlink" title="实验效果与结论"></a>实验效果与结论</h3><p>  能够在CK+上10折达到94.7的Accuracy 10fps 实际使用基本不可能，RPN的人脸检测稳定性很低</p><h2 id="4-Comparative-Study-of-Human-Age-Estimation-Based-on-Hand-Crafted-and-Deep-Face-Features"><a href="#4-Comparative-Study-of-Human-Age-Estimation-Based-on-Hand-Crafted-and-Deep-Face-Features" class="headerlink" title="4.Comparative Study of Human Age Estimation Based on Hand-Crafted and Deep Face Features"></a>4.Comparative Study of Human Age Estimation Based on Hand-Crafted and Deep Face Features</h2><h3 id="文章思想-3"><a href="#文章思想-3" class="headerlink" title="文章思想"></a>文章思想</h3><p>  自己提取的特征(LBP、Hog、BSIF)以后CNNs提取的特征(VGG-face、Image-VGG-F、VGG16、DEX-IMDB-WIKI and DEX-ChaLearn-ICCV2015 Features)，五个CNNs网络，有包含图像分类，人脸识别，目标检测与年龄预估。实际上就是特征做融合，然后用PLS regression 偏最小二乘法回归分析。</p><h3 id="文章使用数据集-2"><a href="#文章使用数据集-2" class="headerlink" title="文章使用数据集"></a>文章使用数据集</h3><p>  MORPH和PAL database</p><h3 id="实验效果与结论-3"><a href="#实验效果与结论-3" class="headerlink" title="实验效果与结论"></a>实验效果与结论</h3><p>  实验对比了几种特征单独的实验效果以及crop后的效果，实验说明了最后的回归很重要，然后CNN的特征比这些自己提取的特征好。</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="http://www.enjoyai.site/2017/10/30/Paper%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93%20Day1/">http://www.enjoyai.site/2017/11/06/</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Paper阅读总结Day1&quot;&gt;&lt;a href=&quot;#Paper阅读总结Day1&quot; class=&quot;headerlink&quot; title=&quot;Paper阅读总结Day1&quot;&gt;&lt;/a&gt;Paper阅读总结Day1&lt;/h1&gt;&lt;h2 id=&quot;1-Convolutional-Neural-Networks-For-Facial-Expression-Recognition&quot;&gt;&lt;a href=&quot;#1-Convolutional-Neural-Networks-For-Facial-Expression-Recognition&quot; class=&quot;headerlink&quot; title=&quot;1.Convolutional Neural Networks For Facial Expression Recognition&quot;&gt;&lt;/a&gt;1.Convolutional Neural Networks For Facial Expression Recognition&lt;/h2&gt;&lt;h3 id=&quot;文章思想&quot;&gt;&lt;a href=&quot;#文章思想&quot; class=&quot;headerlink&quot; title=&quot;文章思想&quot;&gt;&lt;/a&gt;文章思想&lt;/h3&gt;&lt;p&gt;  简单的一篇关于表情识别的文章，运用简单的CNN结构，在文章中对比了深层次的网络结构和浅层次的网络结构的效果，同时将前向的最后一层特征与自己手动提取的Hog特征做了特征融合，并重新训练一个全连接层，得到的效果与不用特征融合效果一致。&lt;/p&gt;
&lt;h3 id=&quot;文章使用数据集&quot;&gt;&lt;a href=&quot;#文章使用数据集&quot; class=&quot;headerlink&quot; title=&quot;文章使用数据集&quot;&gt;&lt;/a&gt;文章使用数据集&lt;/h3&gt;&lt;p&gt;  Fer2013 Database，通过浅层次和深层次的横向对比与 加入hog与不加hog的横向对比&lt;/p&gt;
&lt;h3 id=&quot;实验效果与结论&quot;&gt;&lt;a href=&quot;#实验效果与结论&quot; class=&quot;headerlink&quot; title=&quot;实验效果与结论&quot;&gt;&lt;/a&gt;实验效果与结论&lt;/h3&gt;&lt;p&gt;  深层次的CNN准确率大概是65%，加入HOG与不加效果基本一致，结论是否定了Hog特征融合对表情识别有效果的提升。&lt;br&gt;
    
    </summary>
    
      <category term="Facial Expression" scheme="http://www.enjoyai.site/categories/Facial-Expression/"/>
    
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
      <category term="Facial Expression" scheme="http://www.enjoyai.site/tags/Facial-Expression/"/>
    
      <category term="Face Recognition" scheme="http://www.enjoyai.site/tags/Face-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Caffe VisionLayer分析</title>
    <link href="http://www.enjoyai.site/2017/10/24/Caffe%20VisionLayer%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/24/Caffe VisionLayer分析/</id>
    <published>2017-10-24T11:16:01.000Z</published>
    <updated>2017-10-24T14:37:38.655Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-VisionLayer"><a href="#Caffe-VisionLayer" class="headerlink" title="Caffe VisionLayer"></a>Caffe VisionLayer</h3><p>  老版本中Caffe有$VisionLayer$,其中主要包含了卷积层，采样层，$im2col$层等，本文将结合自己的理解对这些层次进行分析，在自己学习总结的同事，写下对源码的理解。</p><a id="more"></a><h4 id="1-im2colLayer"><a href="#1-im2colLayer" class="headerlink" title="$(1) \, im2colLayer$"></a>$(1) \, im2colLayer$</h4><p>  为了提高$conv$计算的速度，$caffe$采取了$im2col$的方式，通过对滤波器$kernel$和$feature map$做形式上的改变，从而达到提高计算的作用,因此在进行$im2col$前，必须要知道$kernel$的尺度，卷积方式，输入输出的通道数目以及$batch_size$的大小。<br>1.基本数据成员</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-VisionLayer&quot;&gt;&lt;a href=&quot;#Caffe-VisionLayer&quot; class=&quot;headerlink&quot; title=&quot;Caffe VisionLayer&quot;&gt;&lt;/a&gt;Caffe VisionLayer&lt;/h3&gt;&lt;p&gt;  老版本中Caffe有$VisionLayer$,其中主要包含了卷积层，采样层，$im2col$层等，本文将结合自己的理解对这些层次进行分析，在自己学习总结的同事，写下对源码的理解。&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Loss分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe Loss分析/</id>
    <published>2017-10-20T11:16:01.000Z</published>
    <updated>2017-10-25T02:46:58.212Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-Loss"><a href="#Caffe-Loss" class="headerlink" title="Caffe_Loss"></a>Caffe_Loss</h3><p>  损失函数为深度学习中重要的一个组成部分，各种优化算法均是基于Loss来的，损失函数的设计好坏很大程度下能够影响最终网络学习的好坏。派生于 $LossLayer$,根据不同的Loss层有不同的参数;</p><h4 id="1-基本函数"><a href="#1-基本函数" class="headerlink" title="1.基本函数"></a>1.基本函数</h4><p>  主要包含构造函数，前向、后向以及Reshape，部分有SetUp的函数，每层都有Loss参数<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">explicit XXXLossLayer(const LayerParameter&amp; param):</div><div class="line">LossLayer&lt;Dtype&gt;(param),diff_() &#123;&#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure></p><a id="more"></a><h4 id="2-常用损失函数"><a href="#2-常用损失函数" class="headerlink" title="2.常用损失函数"></a>2.常用损失函数</h4><p>  通常在训练过程中，采用mini_batch的方式</p><h5 id="1-EuclideanLoss-欧式损失函数，L2损失"><a href="#1-EuclideanLoss-欧式损失函数，L2损失" class="headerlink" title="(1) EuclideanLoss (欧式损失函数，L2损失)"></a>(1) EuclideanLoss (欧式损失函数，L2损失)</h5><p>  $EuclideanLoss$的公式表达为 $loss = \frac{1}{2n}\sum_{i=1}^n{(y_{i}-\hat{y}_{i})^2}$<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">//reshape函数，完成层次的reshape,diff_与输入的N*C维度相同</span></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">     <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">     LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//先调用基类的Reshape函数</span></div><div class="line">     CHECK_EQ(bottom[<span class="number">0</span>]-&gt;count(<span class="number">1</span>),bottom[<span class="number">1</span>]-&gt;count(<span class="number">1</span>));<span class="comment">//label类别</span></div><div class="line">     diff_.Reshape(*bottom[<span class="number">0</span>]);<span class="comment">//一般是N*C*1*1</span></div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">// Forward_cpu 前向 主要计算loss</span></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">       <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    caffe_sub(count,</div><div class="line">              bottom[<span class="number">0</span>]-&gt;cpu_data(),<span class="comment">//网络的输出 N*C</span></div><div class="line">              bottom[<span class="number">1</span>]-&gt;cpu_data(),<span class="comment">//对应label N*C</span></div><div class="line">              diff_.mutable_cpu_data()<span class="comment">//对应的loss差分</span></div><div class="line">          );<span class="comment">//完成 y_&#123;predicy&#125;-y_&#123;label&#125; //bottom[0]-bottom[1]</span></div><div class="line">    Dtype dot = caffe_cpu_dot(count,diff_.cpu_data(),diff_.cpu_data());</div><div class="line">    <span class="comment">//bottom[0]-&gt;num()== bottom[0].shape(0);</span></div><div class="line">    Dtype loss = dot/bottom[<span class="number">0</span>]-&gt;num()/Dtype(<span class="number">2</span>);<span class="comment">//loss/(2*n)</span></div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss;</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="comment">//Backward_cpu f'(x) = 1/n*(y_&#123;predict&#125;-y_&#123;label&#125;)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">   <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp;propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</div><div class="line">       <span class="keyword">if</span> (propagate_down[i]) &#123;<span class="comment">//需要backward</span></div><div class="line">           <span class="comment">//对应predict-label 如果label为bottom[0]就需要乘以-1</span></div><div class="line">           <span class="keyword">const</span> Dtype sign = (i==<span class="number">0</span>) ? <span class="number">1</span> : <span class="number">-1</span>;</div><div class="line">           <span class="comment">//top[0]-&gt;cpu_diff()返回float* length = 1;下式为loss/n;</span></div><div class="line">           <span class="keyword">const</span> Dtype alpha = sign*top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>]/bottom[<span class="number">0</span>]-&gt;num();</div><div class="line">           <span class="comment">//y = ax+by ;</span></div><div class="line">           caffe_cpu_axpby(bottom[<span class="number">0</span>]-&gt;count(),<span class="comment">//count</span></div><div class="line">                           alpha,<span class="comment">// loss/n</span></div><div class="line">                           diff_.cpu_data(),<span class="comment">//y_&#123;predict&#125;-y_&#123;label&#125;</span></div><div class="line">                           Dtype(<span class="number">0</span>),</div><div class="line">                           bottom[i]-&gt;mutable_cpu_diff()</div><div class="line">                       );<span class="comment">//1/n*loss*(y_&#123;predict&#125;-y_&#123;label&#125;)</span></div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line">   <span class="comment">//欧式损失函数形式简单，常用于做回归分析，做分类需要统一量纲。</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="2-SoftmaxWithLoss-Softmax损失函数"><a href="#2-SoftmaxWithLoss-Softmax损失函数" class="headerlink" title="(2)SoftmaxWithLoss Softmax损失函数"></a>(2)SoftmaxWithLoss Softmax损失函数</h5><p>$\qquad softmax函数将输出的各个类别的概率值进行归一化，生成各个类别的prob$<br>$\qquad 常用的分类损失函数，Softmax输出与Multinomial Logistic Loss的结合。公式如下:$<br>$$ y_i = softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}}$$<br>$$loss = -log(y_k) ,k为实际的样本label$$<br>$\qquad 损失函数的推导:\frac{\partial Loss}{\partial x_i}=\sum_{j=1}^{n}{\frac{\partial loss}{\partial y_j}*\frac{\partial y_j}{\partial x_i}}=-\frac{1}{y_k}*\frac{\partial y_k}{\partial x_i} \quad k为实际的label,其他的\frac{\partial loss}{\partial y_j} =0 \\$<br>$$<br>\qquad \frac{\partial y_k}{\partial x_i} = \frac{\partial softmax(x_k)}{\partial x_i}=<br>\begin{cases}<br>  y_k*(1-y_k) \qquad k == i \\\<br>\\<br> -y_k*y_i \qquad \qquad k \,\,!=\,i<br>\end{cases}<br>$$<br>$$<br>整理后可以发现\frac{\partial loss}{\partial x_i}=<br>\begin{cases}<br>  y_k-1 \qquad k \,== \,i ，即i为实际label\\\<br>\\<br>  y_i \qquad \qquad k \,\,!=\,i,即i不是实际label<br>\end{cases}<br>$$<br>    具体代码的实现如下所示:<br>1.SoftmaxWithLossLayer的输入:bottom<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// bottom[0]为前层的特征输出，一般维度为N*C*1*1</span></div><div class="line"><span class="comment">// bottom[1]为来自data层的样本标签，一般维度为N*1*1*1;</span></div><div class="line"><span class="comment">// 申明</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom;</div><div class="line"><span class="comment">//backward部分代码</span></div><div class="line">Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line"><span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">caffe_copy(prob_.count(), prob_data, bottom_diff);</div><div class="line"><span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//label</span></div></pre></td></tr></table></figure></p><p>2.SoftmaxWithLossLayer层的输出:top<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// SoftmaxWithLossLayer的输出其实就是1*1*1*1的最终loss</span></div><div class="line"><span class="comment">// 如果有多个的话实际就是也会保存softmax的输出，但是需要注意的是内部包含了</span></div><div class="line"><span class="comment">//Softmax的FORWAR过程，产生的概率值保存在prob_内</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top;</div><div class="line"><span class="comment">//forward部分代码 ,</span></div><div class="line">top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / get_normalizer(normalization_, count);</div><div class="line"><span class="keyword">if</span> (top.size() == <span class="number">2</span>) &#123;</div><div class="line">    top[<span class="number">1</span>]-&gt;ShareData(prob_);<span class="comment">//top[1]保存softmax的前向概率</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>3.SoftmaxWithLossLayer的关键变量: $softmax_top_vec_,prob_$ 记录中间值<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;</div><div class="line"><span class="comment">/// prob stores the output probability predictions from the SoftmaxLayer.</span></div><div class="line">Blob&lt;Dtype&gt; prob_;</div><div class="line"><span class="comment">/// bottom vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;</div><div class="line"><span class="comment">/// top vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;</div><div class="line"><span class="comment">/// Whether to ignore instances with a certain label.</span></div><div class="line"><span class="keyword">bool</span> has_ignore_label_;</div><div class="line"><span class="comment">/// The label indicating that an instance should be ignored.</span></div><div class="line"><span class="keyword">int</span> ignore_label_;</div><div class="line"><span class="comment">/// How to normalize the output loss.</span></div><div class="line">LossParameter_NormalizationMode normalization_;</div><div class="line"></div><div class="line"><span class="keyword">int</span> softmax_axis_, outer_num_, inner_num_;<span class="comment">//softmax的输出与Loss的维度</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//先调用基类的reshape</span></div><div class="line">    softmax_layer_-&gt;Reshape(softmax_bottom_vec,softmax_top_vec_);</div><div class="line">    <span class="keyword">int</span> axis = <span class="keyword">this</span>-&gt;layer_param_.softmax_param().axis();<span class="comment">//softmaxproto参数(1)</span></div><div class="line">    softmax_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(axis);<span class="comment">//正不变负倒数</span></div><div class="line">    outer_num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,softmax_axis_);<span class="comment">// N mini_batch_size</span></div><div class="line">    inner_num_ = bottom[<span class="number">0</span>]-&gt;count(softmax_axis_+<span class="number">1</span>);<span class="comment">// H*W 一般为1*1</span></div><div class="line">    <span class="comment">//保证outer_num_*inner_num_ = bottom[1]-&gt;count();//bottom[1]为label N</span></div><div class="line">    <span class="keyword">if</span> (top.size() &gt;= <span class="number">2</span>) &#123;<span class="comment">//多个top实际上是并列的，prob_值完全一致</span></div><div class="line">        top[<span class="number">1</span>]-&gt;Reshapelike(*bottom[<span class="number">0</span>]);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//forward是一个计算loss的过程，loss为-log(p_label)</span></div><div class="line"><span class="comment">//由于softmaxWithLoss包含了Softmax所以需要经过Softmax的前向，并得到每个类别概率值</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="comment">//调用Softmax的前向</span></div><div class="line">    softmax_layer_-&gt;Forward(softmax_bottom_vec_,softmax_top_vec_);</div><div class="line">    <span class="comment">//这里等同于softmax_top_vec_[0]-&gt;cpu_data();</span></div><div class="line">    <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//label 一般来自Data层</span></div><div class="line">    <span class="comment">// 一般是N*C(n个样本，每个C个预测概率)/ N == 类别数目</span></div><div class="line">    <span class="keyword">int</span> dim = prob_.count()/out_num_;</div><div class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;<span class="comment">//统计实际参与loss的样本个数</span></div><div class="line">    Dtype loss = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; outer_num_; i++) &#123;<span class="comment">//每个样本遍历</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123; <span class="comment">//可以认为j == 0 绝大多数成立</span></div><div class="line">            <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i*inner_num_+j]);</div><div class="line">            <span class="keyword">if</span>(has_ignore_label_ &amp;&amp; label_value == ignore_label_)&#123;</div><div class="line">                <span class="comment">// softmaxLayer的参数，可以选择不参与loss的类别</span></div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;<span class="comment">//实际需要判断label_value &gt; 0 ,&lt; prob_.shape(1)</span></div><div class="line">                <span class="comment">// -= 因为loss = -log(p_label),prob_data 是n*c的</span></div><div class="line">                loss -= <span class="built_in">log</span>(<span class="built_in">std</span>::max(prob_data[i*dim+label_value*inner_num_+j)],</div><div class="line">                                Dtype(FLT_MIN)));<span class="comment">//防止溢出或prob出现NAN</span></div><div class="line">                ++count;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//全部样本遍历完成后，可以进行归一，其实也挺简单，</span></div><div class="line">    <span class="comment">// top[0]-&gt;mutable_cpu_data[0] = loss/归一化</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Backward_cpu,这里的Backward实际需要更新的是softmax的输入接口的数据，</span></div><div class="line"><span class="comment">// 中间有个y的转化，具体公式上面已经写出</span></div><div class="line"><span class="comment">// bottom_diff = top_diff * softmaxWithloss' = top_diff * &#123;p -1 或者 p&#125;</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="comment">//fc输出与label的位置固定了，因此不需要如同欧式loss去判断label和fc的输入位置</span></div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</div><div class="line">        <span class="comment">//label不需要backpropagate</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;<span class="comment">//输入，需要更新</span></div><div class="line">        Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();<span class="comment">//需要修改的</span></div><div class="line">        <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();<span class="comment">//N*C</span></div><div class="line">        <span class="comment">//这里把diff先确定为softmax输出的y值，即bottom_diff[t] = y_t ;</span></div><div class="line">        caffe_copy(prob_.count(),prob_data,bottom_diff);</div><div class="line">        <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">        <span class="comment">// 也可以替换为bottom[1]-&gt;count(),实际就是类别C</span></div><div class="line">        <span class="keyword">int</span> dim = prob_.count()/ outer_num_;<span class="comment">//NC/C == N</span></div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; outer_num_; i++) &#123; <span class="comment">//n个样本</span></div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123; <span class="comment">// 实际j == 0</span></div><div class="line">                <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i*inner_num_+j]);</div><div class="line">                <span class="keyword">if</span> (has_ignore_label_ &amp;&amp; label_value == ignore_label_) &#123;</div><div class="line">                    <span class="comment">//正好是忽略loss的类别</span></div><div class="line">                    bottom_diff[i*dim+label_vale*inner_num_+j] = <span class="number">0</span>;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span>&#123;</div><div class="line">                    <span class="comment">//这里需要考虑为什么，实际上之前所有的diff初始为y_t，</span></div><div class="line">                    <span class="comment">//根据softmax的偏导知道真实label是y_t -1;</span></div><div class="line">                    bottom_diff[i*dim+label_vale*inner_num_+j] -= <span class="number">1</span>;</div><div class="line">                    ++count;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//这里只完成了loss的一部分，还差top_diff即Loss</span></div><div class="line">        <span class="comment">//如果归一化，就进行归一，同cpu_forward</span></div><div class="line">        <span class="comment">//cpu_diff可以认为是Loss</span></div><div class="line">        <span class="comment">// Dtype loss_weight = top[0]-&gt;cpu_diff()[0]/归一化</span></div><div class="line">        caffe_scal(prob_count(),loss_weight,bottom_diff);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="3-SmoothL1Loss-RCNN提出的Loss"><a href="#3-SmoothL1Loss-RCNN提出的Loss" class="headerlink" title="(3) SmoothL1Loss (RCNN提出的Loss)"></a>(3) SmoothL1Loss (RCNN提出的Loss)</h5><p>  $SmoothL1Loss$为欧式均方误差的修改版，为分段函数，对离散点不敏感,具体的公式如下:<br>$$<br>SmoothL1Loss(x) =<br>\begin{cases}<br>  0.5*(sigma*x)^2 \qquad 其他<br>\\<br>  \left|x\right|-0.5/sigma^2 \qquad \left|x\right| &lt; 1./sigma^2<br>\end{cases}<br>$$<br>整体的公式为:$x_{new} = x_{input}*w_{in},output = w_{out}*SmoothL1loss(x_{new});$<br>1.基本的数据类型和意义:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Blob&lt;Dtype&gt; diff_;<span class="comment">// y_</span></div><div class="line">Blob&lt;Dtype&gt; error_;<span class="comment">//loss</span></div><div class="line">Blob&lt;Dtype&gt; ones_;</div><div class="line"><span class="keyword">bool</span> has_weights_; <span class="comment">// weight权值</span></div><div class="line">Dtype sigma2_ ;<span class="comment">// sigma 默认为1，此处sigma2_ = sigma*simga;</span></div></pre></td></tr></table></figure></p><p>2.基本的功能函数<br>    基本包含了LayerSetup Reshape Forward 和 Backward四个函数,具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//构建layer层次,SmoothL1LossLayer的参数有sigma，默认为1</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    SmoothL1LossParameter loss_param = <span class="keyword">this</span>-&gt;layer_param_.smooth_l1_loss_param();</div><div class="line">    sigma2_ = loss_param.sigma()*loss_param.sigma();</div><div class="line">    has_weights_ = (bottom.size() &gt;= <span class="number">3</span>);<span class="comment">//bottom[3]---为weights</span></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">        <span class="comment">//bottom[3] == out_weight;//w_out</span></div><div class="line">        <span class="comment">//bottom[2] == in_weight;// w_in</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Reshape 根据输入输出调节结构，计算过程进行了拆分</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;</div><div class="line">    bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//基函数</span></div><div class="line">    <span class="comment">//这里判断参数维度,</span></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">        CHECK_EQ(bottom[<span class="number">0</span>]-&gt;count(<span class="number">1</span>) == bottom[<span class="number">2</span>]-&gt;count(<span class="number">1</span>) ==</div><div class="line">        bottom[<span class="number">3</span>].count(<span class="number">1</span>))  ;<span class="comment">//w_in和w_out的权值</span></div><div class="line">    &#125;</div><div class="line">    diff_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// diff_ = w_in*(bottom[0]-bottom[1]);</span></div><div class="line">    error_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// error_ = w_out*smoothL1(w_in*diff_);</span></div><div class="line">    ones_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// one_ = error_*w_out;</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; ones_-&gt;count(); i++) &#123;</div><div class="line">        one_s.mutable_cpu_data()[i] = Dtype(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Forward过程，一步一步操作</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="comment">//bottom[0]和bottom[1]不确定标签和特征的顺序</span></div><div class="line">    caffe_gpu_sub( <span class="comment">// 计算diff_ = bottom[0]-bottom[1];</span></div><div class="line">        count,</div><div class="line">        bottom[<span class="number">0</span>]-&gt;gpu_data(),</div><div class="line">        bottom[<span class="number">1</span>]-&gt;gpu_data(),</div><div class="line">        diff_.mutable_cpu_data()</div><div class="line">    );</div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123; x_new = x_input*in_weight,xinput==diff_</div><div class="line">        caffp_gpu_mul(</div><div class="line">            count,</div><div class="line">            bottom[<span class="number">2</span>]-&gt;gpu_data(),</div><div class="line">            diff_.gpu_data(),</div><div class="line">            diff_.mutable_gpu_data();</div><div class="line">        );</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//此处为SmoothL1的函数前向过程GPU实现</span></div><div class="line">    SmoothL1Forward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count),</div><div class="line">    CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</div><div class="line">        count, diff_.gpu_data(), errors_.mutable_gpu_data(), sigma2_);</div><div class="line">    CUDA_POST_KERNEL_CHECK;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123; <span class="comment">//x_out= SmoothL1(w_in*x_input) * w_out</span></div><div class="line">        caffe_gpu_mul(</div><div class="line">            count,</div><div class="line">            bottom[<span class="number">3</span>]-&gt;gpu_data(),</div><div class="line">            error_.gpu_data(),</div><div class="line">            error_.mutable_gpu_data();</div><div class="line">        ); <span class="comment">// error _ = w_out* error_</span></div><div class="line">    &#125;</div><div class="line">    Dtype loss;</div><div class="line">    caffe_gpu_dot(count,ones_.gpu_data().error_gpu_data(),&amp;loss);<span class="comment">//类似于asum</span></div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_gpu_data()[<span class="number">0</span>] = loss/bottom[<span class="number">0</span>]-&gt;num();<span class="comment">// mini_batch</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// GPU的实现SmoothL1loss,根据公式实现即可</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SmoothL1Forward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></div><div class="line"><span class="function"><span class="params">Dtype sigma2)</span> </span>&#123;</div><div class="line"><span class="comment">// f(x) = 0.5 * (sigma * x)^2          if |x| &lt; 1 / sigma / sigma</span></div><div class="line"><span class="comment">//        |x| - 0.5 / sigma / sigma    otherwise</span></div><div class="line">    CUDA_KERNEL_LOOP(index, n) &#123; <span class="comment">//for loop</span></div><div class="line">        Dtype val = in[index];</div><div class="line">        Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">        <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</div><div class="line">            out[index] = <span class="number">0.5</span> * val * val * sigma2;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            out[index] = abs_val - <span class="number">0.5</span> / sigma2;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>  反向过程中根据求导公式可以得到如下式子，Backward的过程也如下所示<br>$$\frac{\partial Loss}{\partial x} = w_{in}*w_{out}*\frac{\partial SmoothL1(x)}{\partial x}$$</p><p>cpu版本可以自己实现，只需要把$GPU_data_diff$换成$cpu$,以及$gpu$的$smoothL1$写成$CPU$的即可。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">    <span class="comment">//backward过程，根据导函数</span></div><div class="line">    <span class="comment">// f'()</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Backward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">        <span class="keyword">int</span> count = diff_.count();</div><div class="line"></div><div class="line">        <span class="comment">// 反向即公式的smoothL1的偏导</span></div><div class="line">        SmoothL1Backward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count),</div><div class="line">          CAFFE_CUDA_NUM_THREADS &gt;&gt;&gt;(</div><div class="line">            count, diff_.gpu_data(), diff_.mutable_gpu_data(), sigma2_);</div><div class="line">        CUDA_POST_KERNEL_CHECK;</div><div class="line"></div><div class="line">        <span class="comment">//此处的循环loop如同欧式损失函数，因为无法确认bottom[0]和bottom[1]，fc和label</span></div><div class="line">        <span class="comment">//的顺序，forward默认是0-1，因此如果0为label，则sign = -1;</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</div><div class="line">            <span class="keyword">if</span> (propagate_down[i]) &#123;</div><div class="line">                <span class="keyword">const</span> Dtype sign = (i == <span class="number">0</span>) ? <span class="number">1</span>:<span class="number">-1</span>;<span class="comment">//代码默许了label为bottom[1]</span></div><div class="line">                <span class="comment">//sign* loss/n;</span></div><div class="line">                <span class="keyword">const</span> Dtype alpha = sign*top_diff-&gt;gpu_diff()[<span class="number">0</span>]/bottom[i]-&gt;num();</div><div class="line">                <span class="comment">//smoothL1输入的是diff_.gpu_data()</span></div><div class="line">                caffe_cpu_axpby(</div><div class="line">                    count,</div><div class="line">                    alpha,</div><div class="line">                    diff_.gpu_data(),<span class="comment">//此处的data已经是SmoothL1返回的导数了</span></div><div class="line">                    Dtype(<span class="number">0</span>),</div><div class="line">                    bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                );</div><div class="line">                <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">                    caffe_gpu_mul(</div><div class="line">                        count,</div><div class="line">                        bottom[<span class="number">2</span>]-&gt;gpu_data(),</div><div class="line">                        bottom[i]-&gt;gpu_diff(),</div><div class="line">                        bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                    ); 乘以了内层的weight</div><div class="line">                    caffe_gpu_mul(</div><div class="line">                        count,</div><div class="line">                        bottom[<span class="number">3</span>]-&gt;gpu_data(),</div><div class="line">                        bottom[i]-&gt;gpu_diff(),</div><div class="line">                        bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                    ); 乘以了外层的weight</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SmoothL1Backward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></div><div class="line"><span class="function"><span class="params">    Dtype sigma2)</span> </span>&#123;</div><div class="line">  <span class="comment">// f'(x) = sigma * sigma * x         if |x| &lt; 1 / sigma / sigma</span></div><div class="line">  <span class="comment">//       = sign(x)                   otherwise</span></div><div class="line">        CUDA_KERNEL_LOOP(index, n) &#123;</div><div class="line">            Dtype val = in[index];</div><div class="line">            Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">            <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</div><div class="line">              out[index] = sigma2 * val;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">              out[index] = (Dtype(<span class="number">0</span>) &lt; val) - (val &lt; Dtype(<span class="number">0</span>));<span class="comment">//1或者-1</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p><p>cpu版本的SmoothL1前向和后向实现如下,cpu版本速度过慢，不建议使用<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">    <span class="comment">//前向 替换前向GPU中一部分</span></div><div class="line">    <span class="keyword">const</span> Dtype* in = diff_.cpu_data();</div><div class="line">    Dtype* out = errors_.mutable_cpu_data();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; diff_.count(); i++) &#123;</div><div class="line">       Dtype val = in[index];</div><div class="line">       Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">       <span class="keyword">if</span>(abs_val &lt; <span class="number">1.0</span> / sigma2_)&#123;</div><div class="line">           out[index] = <span class="number">0.5</span> * val * val * sigma2_;</div><div class="line">       &#125;</div><div class="line">       <span class="keyword">else</span>&#123;</div><div class="line">           out[index] = abs_val - <span class="number">0.5</span> / sigma2_;</div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">//反向，替换反向GPU的一部分</span></div><div class="line">   <span class="keyword">const</span> Dtype* in = diff_.cpu_data();</div><div class="line">   Dtype* out = diff_.mutable_cpu_data();</div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; diff_.count(); i++) &#123;</div><div class="line">      Dtype val = in[index];</div><div class="line">      Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">      <span class="keyword">if</span>(abs_val &lt; <span class="number">1.0</span> / sigma2_)&#123;</div><div class="line">          out[index] = sigma2_ *  val;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span>&#123;</div><div class="line">          out[index] = (Dtype(<span class="number">0</span>) &lt; val) - (val &lt; Dtype(<span class="number">0</span>));</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line"><span class="comment">// smoothL1在目标检测的时候效果良好，由于多损失函数以及回归点的变换，bottom[2]和</span></div><div class="line"><span class="comment">// bottom[3]基本都存在，由于其函数特性，对偏远的点不敏感，因此可以替换L2loss</span></div></pre></td></tr></table></figure></p><h5 id="4-SigmoidCrossEntropyLoss-交叉熵"><a href="#4-SigmoidCrossEntropyLoss-交叉熵" class="headerlink" title="(4) SigmoidCrossEntropyLoss (交叉熵)"></a>(4) SigmoidCrossEntropyLoss (交叉熵)</h5><p>  交叉熵应用广泛，常作为二分类的损失函数，在$logistic$中使用，由于$sigmoid$的函数的输出特性，能够很好的以输出值代表类别概率。具体的公式如下所示:</p><p>  $$loss =  -\frac{1}{n}\sum_{1}^{n}(\hat{p_i}*log(p_i)+(1-\hat{p_i})*log(1-p_i)))$$<br>$$p_i = \frac{1}{1.+exp(-x_i)}$$<br>$$ \frac{\partial loss}{\partial x_i} = -\frac{1}{n}*\sum_{i=1}^{n}((\hat{p_i}*\frac{1}{p_i}*p_i*(1-p_i)-(1-\hat{p_i})*\frac{1}{1-p_i}*(1-p_i)*p_i)) $$<br>$$= -\frac{1}{n}\sum_{i=1}^{n}(\hat{p_i}-p_i)$$<br>1.基本的数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">shared_ptr</span>&lt;SigmoidLayer&lt;Dtype&gt;&gt;sigmoid_layer_;<span class="comment">//layer参数</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; sigmoid_output_; <span class="comment">// sigmoid输出的值N*C C一般==1</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_bottom_vec_;<span class="comment">// sigmoid函数的输入x</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_top_vec_;<span class="comment">// sigmoid函数的输出</span></div></pre></td></tr></table></figure></p><p>2.基本的成员函数<br>    基本的成员函数为LayerSetup，Reshape ,Forward和Backward,实现如下:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//构建layer 中间有sigmoid函数过度，所以如同softmaxLoss类似过程</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::LayerSetup(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::LayerSetup(bottom,top);</div><div class="line">    sigmoid_bottom_vec_.clear();</div><div class="line">    sigmoid_bottom_vec_.push_back(bottom[<span class="number">0</span>]);</div><div class="line">    sigmoid_top_vec_.clear();</div><div class="line">    sigmoid_top_vec_.push_back(sigmoid_output_.get());<span class="comment">//sigmoid的输出</span></div><div class="line">    sigmoid_layer_-&gt;Setup(sigmoid_bottom_vec_,sigmoid_top_vec_);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Reshape函数 比较简单</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Reshape(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//步骤1</span></div><div class="line">    sigmoid_layer_-&gt;Reshape(sigmoid_bottom_vec_,sigmoid_top_vec_);<span class="comment">//步骤2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>这里Caffe实现的前向计算代码与公式有差异，具体原因如下<br>$\qquad  \hat{p}*log(p)+(1-\hat{p})*log(1-p) \\<br>\qquad \,= \hat{p}*log(\frac{1}{1+e^{-x}})+(1-\hat{p})*log(\frac{e^{-x}}{1+e^{-x}}) \\<br>\qquad =\hat{p}*log(\frac{1}{1+e^{-x}})-\hat{p}*log(\frac{e^{-x}}{1+e^{-x}})+log(\frac{e^{-x}}{1+e^{-x}}) \\<br>\qquad =\hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})$</p><p>当$e^{-x}很大时, \frac{e^{-x}}{1+e^{-x}}$ 计算不准确，因此采用下种计算方式,当 $x&lt;0$时,分子分母同时乘以$e^{x}$,有:</p><p>$$<br>\frac{e^{-x}}{1+e^{-x}}=<br>\begin{cases}<br>  \frac{e^{-x}}{1+e^{-x}} \qquad x\ge0<br>\\<br>  \frac{1}{1+e^{x}} \qquad \,\,\, x&lt;0<br>\end{cases}<br>$$</p><p>从而得到:<br>$$<br>\hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})=<br>\begin{cases}<br> \hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}}) = (\hat{p}-1)<br>*x-log(1+e^{-x}) \quad x\ge0<br>\\<br> \hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})=\hat{p}*x-log(1+e^{x}) \quad\quad \qquad x&lt;0<br>\end{cases}<br>$$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Forward_cpu 前向函数，分布保存临时值，得到loss</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Forward_cpu(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    sigmoid_bottom_vec_[<span class="number">0</span>] = bottom[<span class="number">0</span>];<span class="comment">//这一步多余，setup时已经保持一致了</span></div><div class="line">    sigmoid_layer_-&gt;Forward_cpu(sigmoid_bottom_vec_,sigmoid_top_vec_);<span class="comment">//Sigmoid</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//N*1*1*1，输出一个概率值为预测1的</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;num();</div><div class="line">    <span class="keyword">const</span> Dtype* input_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//真实label</span></div><div class="line">    Dtype loss = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;<span class="comment">//遍历mini_batch</span></div><div class="line">        loss -= input_data[i]*(target[i]-(input_data[i]&gt;=<span class="number">0</span>))-</div><div class="line">                <span class="built_in">log</span>(<span class="number">1.</span>+<span class="built_in">exp</span>(input_data[i]<span class="number">-2</span>*(input_data[i]&gt;=<span class="number">0</span>)));</div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss/num;<span class="comment">//mini_batch</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">//backward的反向更新比较简单，-(target-predict)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Backward_cpu(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom)&#123;</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</div><div class="line">        <span class="comment">//label 不需要更新</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//N*1*1*1</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;num();<span class="comment">// N</span></div><div class="line">        <span class="keyword">const</span> Dtype* sigmoid_output_data = sigmoid_output_.cpu_data();<span class="comment">//预测值</span></div><div class="line">        <span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">        Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">        <span class="comment">// bottom_diff = predict - target_label</span></div><div class="line">        caffe_sub(count,sigmoid_output_data,target,bottom_diff);</div><div class="line">        <span class="keyword">const</span> Dtype loss_weight = top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>];</div><div class="line">        <span class="comment">//bottom_diff = bottom_diff*loss_weight/n</span></div><div class="line">        caffe_scal(count,loss_weight/num,bottom_diff);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h5 id="5-CenterLoss-ECCV2016"><a href="#5-CenterLoss-ECCV2016" class="headerlink" title="(5) CenterLoss (ECCV2016)"></a>(5) CenterLoss (ECCV2016)</h5><p>  ECCV2016年提出的新loss，让softmax能够训练出更好的内聚性的特征，思路比较简单，在SoftmaxLoss的基础上，添加了一个新的loss，Loss的表达式:<br>  $$\zeta_C = \frac{1}{2}*\sum_{i=1}^{n}||x_i-c_{yi}||_2^2$$<br>  思路比较好理解，增加一个loss衡量样本特征与该类类心的距离，更新的公式如下:<br>  $$\frac{\partial \zeta_c}{\partial x_i} = x_i - c_{yi} \\<br>  \triangle c_j = \frac{\sum_{i=1}^{n}\delta{(y_i=j)}*(c_j-x_i)}{1+\sum_{i=1}^{n}\delta{(y_i=j)}}$$<br>   $$c_j^{t+1} = c_j^t-\alpha*\triangle{c_j^t}$$<br>  第二步骤类心特征更新仅仅更新当前样本所属的类别，分母加1为了防止分母为0，因此和softmax整合后整体的Loss如下所示：<br>  $$\zeta = \zeta_S+\lambda \zeta_C$$</p><p>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//基本数据用以保存center_Loss的layer params</span></div><div class="line"><span class="keyword">int</span> N_;<span class="comment">// 对应params的num_output,分类类别</span></div><div class="line"><span class="keyword">int</span> K_;<span class="comment">// 对应fc层的输出特征,</span></div><div class="line"><span class="keyword">int</span> M_;<span class="comment">// 对应于batch_size</span></div><div class="line">Blob&lt;Dtype&gt; distance_;<span class="comment">//样本与类心的距离，distance为x - x_center重点</span></div><div class="line">Blob&lt;Dtype&gt; variation_sum_;<span class="comment">// distance的负数， x_center- x</span></div><div class="line">Blob&lt;Dtype&gt; count_; <span class="comment">// 类心的个数</span></div><div class="line"><span class="built_in">string</span> distance_type_; <span class="comment">// 距离的衡量 默认L2</span></div></pre></td></tr></table></figure></p><p>2.基本的成员函数<br>  与一般的Loss层一样，有LayerSetup,Reshape,Forward,Backward,具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// layersetup过程，center是N个中心，每个类心feature长度K</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CenterLossLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    CenterLossParameter loss_param = <span class="keyword">this</span>-&gt;layer_param_.center_loss_param();</div><div class="line">    N_ = loss_pram.num_output();<span class="comment">//分类的类别，类心的个数,prototxt内设置</span></div><div class="line">    distance_type_ = loss_pram.distance_type();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> axis = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(loss_pram.axis());</div><div class="line">    K_ = bottom[<span class="number">0</span>].count(axis);<span class="comment">//axis 默认为1，K_= fc*1*1,特征的长度</span></div><div class="line">    M_ = bottom[<span class="number">0</span>]-&gt;num(); <span class="comment">// batch_size的大小</span></div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">//层内无参数.</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">1</span>);<span class="comment">//这里放center，各个类别的fc中心</span></div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; center_shape(<span class="number">2</span>);</div><div class="line">        center_shape[<span class="number">0</span>] = N_;</div><div class="line">        center_shape[<span class="number">1</span>] = K_;</div><div class="line">        <span class="comment">// 代表中心是N个中心，每个中心的feature长度为K_</span></div><div class="line">        <span class="keyword">this</span>.blobs_[<span class="number">0</span>].resize(<span class="keyword">new</span> Blob&lt;Dtype&gt;(center_shape));</div><div class="line">        <span class="comment">// 初始中心的填充方式</span></div><div class="line">        <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt;&gt;center_filler(GetFiller&lt;Dtype&gt;(</div><div class="line">            loss_param.center_filler()));</div><div class="line">        )</div><div class="line">        center_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">this</span>-&gt;param_propagate_down_.resize(<span class="keyword">this</span>-&gt;blobs_.size(),<span class="literal">true</span>);<span class="comment">//类心也更新</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Reshape函数</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CenterLossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp;bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);</div><div class="line">    distance_.ReshapeLike(*bottom[<span class="number">0</span>]);<span class="comment">//bottom长度为N_*K_</span></div><div class="line">    variation_sum_.ReshapeLike(*<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]);<span class="comment">//一样的N_*K_</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;count_reshape(<span class="number">1</span>);</div><div class="line">    count_reshape[<span class="number">0</span>]= N_;</div><div class="line">    count_.Reshape(count_reshape);<span class="comment">//N_类心的个数</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Forward_cpu ，得到loss</span></div><div class="line"><span class="comment">// N_类别数，K_特征长度,M_mini_batch的样本个数</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CenterLossLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();<span class="comment">//N_*K_</span></div><div class="line">    <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//N_*1;</span></div><div class="line">    <span class="keyword">const</span> Dtype* center = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data();<span class="comment">//N_K_</span></div><div class="line">    Dtype* distance_data = distance_.mutable_cpu_data();<span class="comment">//</span></div><div class="line">    <span class="comment">// i-t样本的距离</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; M_; i++) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i]);<span class="comment">//真是的样本类别</span></div><div class="line">        <span class="comment">//对应特征相减，用fc特征减去该类的类心，保存在distance_data上</span></div><div class="line">        caffe_sub(K_,bottom+i*K_,center+label_value*K_,distance_data+i*K_);</div><div class="line">    &#125;</div><div class="line">    Dtype dot;</div><div class="line">    Dtype loss;</div><div class="line">    <span class="keyword">if</span> (distance_type_ == <span class="string">"L1"</span>) &#123; <span class="comment">//L1 loss,distance_ sum即可</span></div><div class="line">        <span class="comment">// 也可以写caffe_cpu_asum(M_*K_,distance_data);</span></div><div class="line">        dot = caffe_cpu_asum(M_*K_,distance_.cpu_data());</div><div class="line">        loss = dot/M_;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//L2,loss,distance_data*distance_data,然后M_样本sum</span></div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(distance_type_ == <span class="string">"L2"</span>)&#123;</div><div class="line">        dot = caffe_cpu_dot(M_*K_,distance_.cpu_data(),distance_.cpu_data());</div><div class="line">        loss = dot/M_/Dtype(<span class="number">2</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        <span class="comment">//不支持其他的距离衡量</span></div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Backward_cpu,更新data和center，</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CenterLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;param_propagate_down_[<span class="number">0</span>]) &#123;<span class="comment">//表示更新类心</span></div><div class="line">        <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">        Dtype* center_diff = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">        Dtype* variation_sum_data = variation_sum_.mutable_cpu_data();</div><div class="line">        <span class="keyword">int</span>* count_data = count_.mutable_cpu_data();</div><div class="line">        <span class="keyword">const</span> Dtype* distance_data = distance_.cpu_data();<span class="comment">//fc_center-fc_pre</span></div><div class="line">        <span class="keyword">if</span> (distance_type_ == <span class="string">"L1"</span>) &#123;</div><div class="line">            caffe_cpu_sign(M_*K_,distance_data,distance_.mutable_cpu_data());</div><div class="line">        &#125;</div><div class="line">        caffe_set(N_*K_,Dtype(<span class="number">0</span>),variation_sum_.mutable_cpu_data());</div><div class="line">        caffe.<span class="built_in">set</span>(N_,<span class="number">0</span>,count_.mutable_cpu_data());<span class="comment">//统计每个类别的个数</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; M_; i++) &#123;<span class="comment">//样本循环</span></div><div class="line">            <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i]);</div><div class="line">            <span class="comment">//variation_sum_data 初始为0，distance保存的即使x_i-x_center</span></div><div class="line">            caffe_sub(K_,variation_sum_data+label_value*K_,</div><div class="line">                distance_data+i*K,variation_sum_data+label_value*K);</div><div class="line">            count_data[label_value]++:</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; M_; i++) &#123;</div><div class="line">            <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[<span class="number">0</span>]);</div><div class="line">            <span class="comment">//1/(count+1)*(x_center-x_i)</span></div><div class="line">            caffe_cpu_axpby(K_,Dtype(<span class="number">1</span>)/(count_data[label_value]+<span class="number">1</span>),</div><div class="line">            variation_sum_data+label_value*K,<span class="number">1.</span>,center_diff+label_value*K_);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//类心更新完成后,跟新x</span></div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;<span class="comment">//更新输入x</span></div><div class="line">        <span class="comment">//loss * 1/M * (x - x_center)</span></div><div class="line">        caffe_copy(M_*K_,distance.cpu_data(),bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff());</div><div class="line">        cafe_scal(M_*K_,top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>]/M_,</div><div class="line">        bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</div><div class="line">        <span class="comment">// label不更新</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p> $CenterLoss$在多分类上较$Softmax$有提高，$loss _weight$的设置可以确定$center _loss$和$softmaxloss$的比重，能够很有效的使得网络能够最小化类内距离，加大区分度。</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-Loss&quot;&gt;&lt;a href=&quot;#Caffe-Loss&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Loss&quot;&gt;&lt;/a&gt;Caffe_Loss&lt;/h3&gt;&lt;p&gt;  损失函数为深度学习中重要的一个组成部分，各种优化算法均是基于Loss来的，损失函数的设计好坏很大程度下能够影响最终网络学习的好坏。派生于 $LossLayer$,根据不同的Loss层有不同的参数;&lt;/p&gt;
&lt;h4 id=&quot;1-基本函数&quot;&gt;&lt;a href=&quot;#1-基本函数&quot; class=&quot;headerlink&quot; title=&quot;1.基本函数&quot;&gt;&lt;/a&gt;1.基本函数&lt;/h4&gt;&lt;p&gt;  主要包含构造函数，前向、后向以及Reshape，部分有SetUp的函数，每层都有Loss参数&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;explicit XXXLossLayer(const LayerParameter&amp;amp; param):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;LossLayer&amp;lt;Dtype&amp;gt;(param),diff_() &amp;#123;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Reshape&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe DataLayer分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20Data%E5%B1%82%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe Data层分析/</id>
    <published>2017-10-20T03:16:01.000Z</published>
    <updated>2017-11-06T14:35:26.197Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-DataLayer分析"><a href="#Caffe-DataLayer分析" class="headerlink" title="Caffe DataLayer分析"></a>Caffe DataLayer分析</h3><p>  Caffe的$data$层作为网络的起始部分，是网络的最底层，其不仅提供了数据的输入，也提供了数据的格式转换，而数据的来源可以来自于高效率的数据库 $(lmdb,levelDb)$,也可以来自于内存，$HDF5$文件等。</p><h4 id="（1）-DataLayer"><a href="#（1）-DataLayer" class="headerlink" title="$（1）\,DataLayer$"></a>$（1）\,DataLayer$</h4><p>  $Data$作为最常用的$caffe$网络训练的数据输入，其参数也比较多，具体可以参见$proto$中关于$DataParameter$的定义,通过$Datalayer$可以完成数据的转化，需要指明$source,batch_size$,也可以指明$mirror,crop,backend$等</p><p>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DataRead reader_;<span class="comment">//data_reader.hpp,cpp用以定义读取数据的操作</span></div></pre></td></tr></table></figure></p><p>2.基本成员函数<br>  主要包括$Layersetup$函数，该函数为整个网络架构搭建的开始<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LayerSetup</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> DataLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; top)&#123;</div><div class="line">  <span class="keyword">const</span> DataParamter&amp; param = <span class="keyword">this</span>-&gt;layer_param_-&gt;data_param();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> batch_size = param.batch_size();</div><div class="line">  Datum&amp; datum = *(reader_.full().peek()); <span class="comment">//数据库的读取操作</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Caffe-DataLayer分析&quot;&gt;&lt;a href=&quot;#Caffe-DataLayer分析&quot; class=&quot;headerlink&quot; title=&quot;Caffe DataLayer分析&quot;&gt;&lt;/a&gt;Caffe DataLayer分析&lt;/h3&gt;&lt;p&gt;  Caffe的$d
      
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe CommonLayer分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20CommonLayer%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe CommonLayer分析/</id>
    <published>2017-10-20T03:16:01.000Z</published>
    <updated>2017-11-07T13:57:36.218Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-CommonLayer分析"><a href="#Caffe-CommonLayer分析" class="headerlink" title="Caffe CommonLayer分析"></a>Caffe CommonLayer分析</h3><p>  $Caffe$中包含了很多通用的功能层，包含了$concat$,$slice$,$split$,$crop$,$flip$,$scale_layer$等,这些层在网络中经常被使用，本文也将对其中的常见layer进行说明与源码分析。<br><a id="more"></a></p><h4 id="1-常用-Layer"><a href="#1-常用-Layer" class="headerlink" title="1.常用$Layer$"></a>1.常用$Layer$</h4><h5 id="1-CropLayer"><a href="#1-CropLayer" class="headerlink" title="(1) $CropLayer$"></a>(1) $CropLayer$</h5><p>  CropLayer完成数据的裁剪，输入两个 $bottom,bottom[0]$ 为原始数据，$bottom[1]$ 为裁剪后<br>的输出尺寸，输出 $top[0]$ 为裁剪后的数据，尺寸与 $bottom[1]$ 相同，其中有$axis 控制裁剪<br>的起始轴,offset表示对应裁剪轴的起始位置。举例说明：<br>$$bottom[0]的shape:[32,64,512,512],bottom[1]的shape:[32,32,256,256] \\<br>axis = 1 \qquad offset = [:,16,128,128] \\<br>则:top[1]的为bottom[0][:,16+bottom[1].shape(1),128+bottom[1].shape(2),128+bottom[1].shape(3)))$$<br>下面会进行具体的代码解释说明</p><p>1.基本成员变量<br> 基本成员变量，记录开始的axis和每个shape的起始偏移<br> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;offsets_;</div><div class="line"><span class="keyword">int</span> axis;</div></pre></td></tr></table></figure></p><p>2.基本成员函数<br>  基本成员函数包括LayerSetup,Reshape,forward,Backward,crop_copy，具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LayerSetup 主要完成proto的参数提取过程</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CropLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">  <span class="keyword">const</span> CropParameter&amp; crop_param = <span class="keyword">this</span>-&gt;layer_param_.crop_param();</div><div class="line">  CHECK_EQ(bottom.size(),<span class="number">2</span>);<span class="comment">//必须是2个</span></div><div class="line">  <span class="keyword">int</span> input_dim = bottom[<span class="number">0</span>]-&gt;num_axes();<span class="comment">// 一般为4， 即shape_.size()</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> start_axis = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(crop_param.axis());</div><div class="line">  <span class="comment">// 这里的axis要判断是否小于Input_dim</span></div><div class="line">  <span class="keyword">if</span> (crop_param.offset_size() &gt; <span class="number">1</span>) &#123; <span class="comment">// offset_size() == offset.size()</span></div><div class="line">      CHECK_EQ(start_axis+crop_param.offset_size(),input_dim);</div><div class="line">      <span class="comment">//保证起始后的axis均有offset</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Reshape,确定offsets和crop_size的尺寸</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CropLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> CropParameter param = <span class="keyword">this</span>-&gt;layer_param_.crop_param();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dim = bottom[<span class="number">0</span>]-&gt;num_axes();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> start_axis = param.axis();</div><div class="line">    offsets_ = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;&lt;<span class="keyword">int</span>&gt;(input_dim,<span class="number">0</span>);</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; new_shape(bottom[<span class="number">0</span>]-&gt;shape());</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; dim; i++) &#123;</div><div class="line">        <span class="keyword">int</span> crop_offset = <span class="number">0</span>; <span class="comment">//偏移量</span></div><div class="line">        <span class="keyword">int</span> new_size = bottom[<span class="number">0</span>]-&gt;shape(i);<span class="comment">//每个shape的size</span></div><div class="line">        <span class="comment">// i &gt;= start_axis的时候才crop,否则不改变shape</span></div><div class="line">        <span class="keyword">if</span> ( i &gt;= start_axis) &#123;</div><div class="line">           new_size = bottom[<span class="number">1</span>].shape(i);</div><div class="line">           <span class="keyword">if</span> (param.offset_size() == <span class="number">1</span>) &#123;<span class="comment">//如果只给出一个offset默认都一样</span></div><div class="line">              crop_offset = param.offset(<span class="number">0</span>);</div><div class="line">           &#125;</div><div class="line">           <span class="keyword">else</span> <span class="keyword">if</span>(param.offset_size() &gt; <span class="number">1</span>)&#123;</div><div class="line">             crop_offset = param.offset(i-start_axis);</div><div class="line">           &#125;</div><div class="line">           CHECK_GE(bottom[<span class="number">0</span>]-&gt;shape(i),crop_offset+bottom[<span class="number">1</span>]-&gt;shape(i));</div><div class="line">        &#125;</div><div class="line">        new_shape[i] = new_size;</div><div class="line">        offsets_[i] = crop_offset;</div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;Reshape(new_shape);</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p><p>  $Forward$ 的前向过程设计到元素复制的问题，使用 $crop_copy$ 函数单独实现，<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CropLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;indices(top[<span class="number">0</span>].num_axes(),<span class="number">0</span>);</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();<span class="comment">//输入</span></div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();<span class="comment">//输出</span></div><div class="line">  crop_copy(botoom,top,offsets,indices,<span class="number">0</span>,botton_data,top_data,<span class="literal">true</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> CropLayer&lt;Dtype&gt;::crop_copy(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> vecotr&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; offsets,</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; indices,<span class="keyword">int</span> cur_dim,<span class="keyword">const</span> Dtype* src_data,</div><div class="line">  <span class="keyword">const</span> Dtype* dst_data,<span class="keyword">bool</span> is_forward)&#123;</div><div class="line"></div><div class="line">  <span class="comment">//循环赋值每个维度</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top[<span class="number">0</span>]-&gt;shape(cur_dim); i++) &#123;</div><div class="line"></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="2-AccracyLayer"><a href="#2-AccracyLayer" class="headerlink" title="(2) $AccracyLayer$"></a>(2) $AccracyLayer$</h5><p>  Accracy_layer用以统计训练过程中样本预测的准确率，根据label值与top_K的得分标签的对比，来计算准确率，因此可以在prototxt中设置top_k参数，观察训练状况。<br>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> label_axis_;<span class="comment">//实际上就是第一个channels是label</span></div><div class="line"><span class="keyword">int</span> outer_num_;<span class="comment">//BATCH_SIZE</span></div><div class="line"><span class="keyword">int</span> inner_num_;<span class="comment">//一般为 1 即H*W</span></div><div class="line"><span class="keyword">int</span> top_k;</div><div class="line"><span class="keyword">bool</span> has_ignore_label;</div><div class="line"><span class="keyword">int</span> ignore_label;</div><div class="line">Blob&lt;Dtype&gt;nums_buffer_;<span class="comment">//统计每个类别的样本数量</span></div></pre></td></tr></table></figure></p><p>2.基本成员函数<br>  基本成员函数包括$LayerSetup$,$Reshape$,$forward$,其中参数的设置和读取发生在$LayerSetup$和$Reshape$上,acuracy可以显示训练中的信息，稍加修改也可以显示$Recall,F1$值等信息,同时<br>  类别较少的时候，加入一个输出$top$，即可显示出每个类别的训练中的$accuracy$情况，具体实现如下:<br>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//layersetup 仅仅完成参数的读取</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> AccuracyLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; top)&#123;</div><div class="line">  <span class="keyword">const</span> AccuracyParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.accuracy_param();</div><div class="line">  top_k = param.top_k();</div><div class="line">  label_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());;</div><div class="line">  has_ignore_label = param.has_ignore_label();</div><div class="line">  <span class="keyword">if</span> (has_ignore_label) &#123;</div><div class="line">     ignore_label = param.ignore_label();</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Reshape</span></div><div class="line"><span class="comment">// 多个top的时候，第一个top为整体的AC，第二个top为每个类别的ac</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> AccuracyLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&gt;*&gt;&amp; top)&#123;</div><div class="line">  outer_num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,label_axis_);<span class="comment">//N</span></div><div class="line">  inner_num_ = bottom[<span class="number">0</span>]-&gt;count(label_axis_+<span class="number">1</span>);<span class="comment">//1*1 (H*W)</span></div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;top_shape(<span class="number">0</span>);</div><div class="line">  top[<span class="number">0</span>]-&gt;Reshape(top_shape);</div><div class="line">  <span class="keyword">if</span> (top.size() &gt; <span class="number">1</span>) &#123;</div><div class="line">     <span class="comment">//每个类别是一个向量，每个类别都需要统计单独的accuracy，而不是整体的</span></div><div class="line">     <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;top_shape_pre_class(<span class="number">1</span>);</div><div class="line">     top_shape_pre_class[<span class="number">0</span>] = bottom[<span class="number">0</span>]-&gt;shape(label_axis_);<span class="comment">//N个类别</span></div><div class="line">     top[<span class="number">1</span>]-&gt;Reshape(top_shape_pre_class);</div><div class="line">     nums_buffer_.Reshape(top_shape_pre_class);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">//Forward_cpu,top[1]为C*1 ,Top[0]为1*1*1*1</span></div><div class="line"><span class="comment">//前向过程，如果多个top则需要统计每个类别的accuracy保存到top[1]中</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> AccracyLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> dim = bottom[<span class="number">0</span>]-&gt;count()/outer_num_;<span class="comment">//类别数目</span></div><div class="line">  <span class="keyword">if</span> (top.size() &gt; <span class="number">1</span>) &#123;</div><div class="line">     caffe_set(nums_buffer_.count(),<span class="number">0</span>,nums_buffer_.mutable_cpu_data());</div><div class="line">     caffe_set(top[<span class="number">1</span>]-&gt;count(),<span class="number">0</span>,top[<span class="number">1</span>]-&gt;mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line">  Dtype accuracy = <span class="number">0</span>;</div><div class="line">  <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; outer_num_; i++) &#123; <span class="comment">//N</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123; <span class="comment">//1*1</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i*inner_num_+j]);</div><div class="line">      <span class="keyword">if</span> (has_ignore_label &amp;&amp; ignore_label == label_value) &#123;</div><div class="line">          <span class="keyword">continue</span>;<span class="comment">//当前label是忽略的label</span></div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (top-&gt;size() &gt; <span class="number">1</span>) &#123;</div><div class="line">        nums_buffer_.mutable_cpu_data()[label_value]++;<span class="comment">//类别数目+1</span></div><div class="line">      &#125;</div><div class="line">      <span class="comment">//看top_k的最大</span></div><div class="line">      <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::pair&lt;Dtype,<span class="keyword">int</span>&gt;&gt; bottom_data_vector;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> k = <span class="number">0</span>; k &lt; dim ; k++) &#123;</div><div class="line">          bottom_data_vector.push_back(</div><div class="line">            <span class="built_in">std</span>::make_pair(bottom_data[i*dim+k*inner_num_+j]));</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//最大堆排序</span></div><div class="line">      <span class="built_in">std</span>::partial_sort(</div><div class="line">      bottom_data_vector.begin(),bottom_data_vector.begin()+top_k,</div><div class="line">      bottom_data_vector.end(),<span class="built_in">std</span>::greater&lt;<span class="built_in">std</span>::pair&lt;Dtype, <span class="keyword">int</span>&gt;&gt;());</div><div class="line"></div><div class="line">      <span class="comment">//查找top_k有没有真实的label</span></div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top_k; i++) &#123;</div><div class="line">        <span class="keyword">if</span> (bottom_data_vector[i].second == label_value) &#123;</div><div class="line">          ++accuracy;</div><div class="line">          <span class="keyword">if</span> (top.size() &gt; <span class="number">1</span>) &#123;</div><div class="line">            <span class="comment">//每类样本预测正确的数目+1</span></div><div class="line">            top[<span class="number">1</span>]-&gt;mutable_cpu_data()[label_value]++;</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      count++;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//全部mini的样本循环完成后</span></div><div class="line">  top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = accuracy/count;</div><div class="line">  <span class="keyword">if</span> (top.size() &gt; <span class="number">1</span>) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; dim; i++) &#123;<span class="comment">//dim表示类别</span></div><div class="line">        top[<span class="number">1</span>]-&gt;mutable_cpu_data()[i] =</div><div class="line">        nums_buffer_.cpu_data()[i] == <span class="number">0</span> ? <span class="number">0</span>;</div><div class="line">        top[<span class="number">1</span>]-&gt;cpu_data()[i]/nums_buffer_.cpu_data()[i];</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="3-EltwiseLayer"><a href="#3-EltwiseLayer" class="headerlink" title="(3) $EltwiseLayer$"></a>(3) $EltwiseLayer$</h5><p>  $EltwiseLayer$在深度网络中运用非常广泛，常用与多$layer$的合并，在$ResidualNet$中用以连接$block$与$x$部分，其组合方式有$prod,sum,max$,最常见的为$sum$和$max$,由于组合的方式有多种，因此在进行前向和后向的分析的时候需要按照多种情况进行分析，详细的代码解析如下所示：<br>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">EltwiseParameter_EltwiseOp op_;<span class="comment">// sum,prod,max 实际是个enum数据</span></div><div class="line"><span class="built_in">vector</span>&lt;Dtype&gt; coeffs_; <span class="comment">// 代表操作参数 如果-1，代表a-b</span></div><div class="line">Blob&lt;<span class="keyword">int</span>&gt; max_idx;</div><div class="line"><span class="keyword">bool</span> stable_prod_grad_;<span class="comment">//只针对PROD，点乘模式</span></div></pre></td></tr></table></figure></p><p>2.基本成员函数<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">  <span class="comment">//LayerSetup 完成参数的读取</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> EltwiseLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> EltwiseParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.eltwise_param();</div><div class="line">    op_ = param.operation();</div><div class="line">    coeffs_ = <span class="built_in">vector</span>&lt;Dtype&gt;(bottom.size(),<span class="number">1</span>);</div><div class="line">    <span class="keyword">if</span> (param.coeff_size()) &#123;<span class="comment">//每个layer的前面的标量</span></div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; param.coeff_size(); i++) &#123;</div><div class="line">        coeffs_[i] = param.coeff(i); <span class="comment">//1 -1等参数</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    stable_prod_grad_ = param.stable_prod_grad();</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//Reshape过程，完成topshape的构造</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> EltwiseLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="comment">//bottom的shape要完全一样</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">1</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">        CHECK_EQ(bottom[i]-&gt;shape() == bottom[<span class="number">0</span>]-&gt;shape())</div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;ReshapeLike(*bottom[<span class="number">0</span>]);<span class="comment">//当然输出也要一样</span></div><div class="line">    <span class="keyword">if</span> (this_-&gt;layer_param_.eltwise_param().operation()==</div><div class="line">        EltwiseParameter_EltwiseOp_Max &amp;&amp; top.size() == <span class="number">1</span>) &#123;</div><div class="line">        max_idx_.Reshape(bottom[<span class="number">0</span>]-&gt;shape());<span class="comment">//记录每个的maxid</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">//Forward_cpu,完成layer的前向操作</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> EltwiseLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">  <span class="comment">//临时变量,用以MAX操作</span></div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = <span class="literal">NULL</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = top[<span class="number">0</span>]-&gt;count();<span class="comment">//</span></div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="keyword">switch</span> (op_) &#123;</div><div class="line">  <span class="keyword">case</span> EltwiseParameter_EltwiseOp_PROD:</div><div class="line">    caffe_mul(count,bottom[<span class="number">0</span>]-&gt;cpu_data(),bottom[<span class="number">1</span>]-&gt;cpu_data(),top_data);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">2</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">      caffe_mul(count,top_data,bottom[i]-&gt;cpu_data(),top_data);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span>  EltwiseParameter_EltwiseOp_SUM:</div><div class="line">    caffe_set(count,Dtype(<span class="number">0</span>),top_data);<span class="comment">//先初始输出为0</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">      caffe_axpy(count,coeffs_[i],bottom[i]-&gt;cpu_data(),top_data);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> EltwiseParameter_EltwiseOp_MAX:</div><div class="line">    caffe_set(count,<span class="number">-1</span>,max_idx_.mutable_cpu_data());</div><div class="line">    caffe_set(count,Dtype(-FLT_MIN),top_data);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">      bottom_data = bottom[i]-&gt;cpu_data();</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; count; j++) &#123; <span class="comment">//整体遍历</span></div><div class="line">         <span class="keyword">if</span> (bottom_data[j] &gt; top_data[j]) &#123;</div><div class="line">            top_data[j] = bottom_data[j];</div><div class="line">            max_idx_.mutable_cpu_data()[j] = i;</div><div class="line">         &#125;</div><div class="line">       &#125;</div><div class="line">    &#125;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    <span class="comment">//Not Support;</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Backward_cpu,完成layer的反向操作</span></div><div class="line"><span class="comment">// 当method == SUM的时候，bottom_diff[i] = coeffs_[i]* top_diff;</span></div><div class="line"><span class="comment">// 当method == product的时候,bottom_diff[i] = top_diff*top_data/bottom_data[i]</span></div><div class="line"><span class="comment">// 当method == Max 的时候，bottom_diff[i] = top_diff*(j==max_idx_?1:0)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> EltwiseLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> cont = top[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">      <span class="keyword">if</span> (propagate_down[i]) &#123; <span class="comment">//需要backward才考虑</span></div><div class="line">          <span class="keyword">const</span> Dtype* bottom_data = bottom[i]-&gt;cpu_data();</div><div class="line">          Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff();</div><div class="line">          <span class="keyword">switch</span> (op_) &#123;</div><div class="line">            <span class="comment">//bottom_diff[i] = top_diff*top_data/bottom_data[i]</span></div><div class="line">            <span class="keyword">case</span> EltwiseParameter_EltwiseOp_PROD:<span class="comment">//点成操作</span></div><div class="line">              <span class="keyword">if</span> (stable_prod_grad_) &#123; <span class="comment">//渐进梯度的实现top_data/bottom[i]</span></div><div class="line">                <span class="keyword">bool</span> initiaized = <span class="literal">false</span>;</div><div class="line">                <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; bottom.size(); j++) &#123;</div><div class="line">                  <span class="keyword">if</span>(i == j) <span class="keyword">continue</span>; <span class="comment">//top/bottom[i] == bottom[j]连乘</span></div><div class="line">                  <span class="keyword">if</span> (!initiaized) &#123; <span class="comment">//初始化</span></div><div class="line">                    <span class="comment">//用bottom[j]初始一下bottom_diff</span></div><div class="line">                    caffe_copy(count,bottom[j]-&gt;cpu_data(),bottom_diff);</div><div class="line">                    initiaized = <span class="literal">true</span>;</div><div class="line">                  &#125;</div><div class="line">                  <span class="keyword">else</span>&#123;</div><div class="line">                    caffe_mul(count,bottom[j]-&gt;cpu_dat(),bottom_diff,</div><div class="line">                            bottom_diff);</div><div class="line">                  &#125;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">              <span class="keyword">else</span>&#123;</div><div class="line">                caffe_div(count,top_data,bottom_data,bottom_diff);</div><div class="line">              &#125;</div><div class="line">              caffe_mul(count,bottom_diff,top_diff,bottom_diff);</div><div class="line">              <span class="keyword">break</span>;</div><div class="line"></div><div class="line">            <span class="comment">//bottom_diff[i] = coeffs_[i]* top_diff;</span></div><div class="line">            <span class="keyword">case</span>  EltwiseParameter_EltwiseOp_SUM:<span class="comment">//sum操作</span></div><div class="line">                <span class="keyword">if</span> (coeffs_[i] == Dtype(<span class="number">1</span>)) &#123;</div><div class="line">                  caffe_copy(count,top_diff,bottom_diff);</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span>&#123;</div><div class="line">                  caffe_scale(count,coeffs_[i],top_diff,bottom_diff);</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line"></div><div class="line">            <span class="comment">//bottom_diff[i] = top_diff*(j==max_idx_?1:0)</span></div><div class="line">            <span class="keyword">case</span> EltwiseParameter_EltwiseOp_MAX: <span class="comment">//max操作</span></div><div class="line">                <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; count; j++) &#123;</div><div class="line">                   <span class="keyword">if</span> (max_idx_.cpu_data()[j] == i) &#123;</div><div class="line">                     bottom_diff[j] = top_diff[j];</div><div class="line">                   &#125;</div><div class="line">                   <span class="keyword">else</span>&#123;</div><div class="line">                     bottom_diff[j] = Dtype(<span class="number">0</span>);</div><div class="line">                   &#125;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line"></div><div class="line">            <span class="keyword">default</span>:</div><div class="line">            <span class="comment">//  Not Support</span></div><div class="line"></div><div class="line">          &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>$Eltwise$的$backward$的$product$模式有两种实现方式:<br>(1) top_data/bottom[i];<br>(2) $\prod_{j=0,j!=i}^{n-1}bottom[i]$</p><h5 id="4-ConcatLayer"><a href="#4-ConcatLayer" class="headerlink" title="(4)$ConcatLayer$"></a>(4)$ConcatLayer$</h5><p>  同$Eltwise$类似，$Concat$在多特征图的融合方面使用也极为广泛(denseNet,Dpn),$concat$中有$axis$和$concat_dim$控制的特征拼接的准则:通常做通道间融合,例如:<br>  $A：[32,112,256,256];B:[32,32,256,256]$,$concat_dim$为1,则输出的尺寸为 $[32,144,256,256]$<br>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> num_concats_; <span class="comment">// 合并通道前的值，一般为mini——batch</span></div><div class="line"><span class="keyword">int</span> concat_input_size_; <span class="comment">//合并通道后的SIze一般为H*W</span></div><div class="line"><span class="keyword">int</span> concat_axis_; <span class="comment">// 合并的shape值，一般为1，即Channels合并</span></div></pre></td></tr></table></figure></p><p>2.基本成员函数<br>  基本成员函数包含$LayerSetup$,$Reshape$,$Forward$,$Backward$,具体实现如下:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// LayerSetup 主要包含参数的读取和判断是否合理</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ConcatLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> ConcatParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.concat_param();</div><div class="line">    <span class="comment">// axis和concat_dim必须要有一个</span></div><div class="line">    CHECK(!(concat_param.has_axis() &amp;&amp; concat_param.has_concat_dim()));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Reshape，根据prototxt的参数 ,决定top的shape</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ConcatLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_axes = bottom[<span class="number">0</span>]-&gt;num_axes();<span class="comment">// 基本认为4 NCHW</span></div><div class="line">    <span class="keyword">const</span> ConcatParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.concat_param();</div><div class="line">    <span class="keyword">if</span> (param.has_concat_dim()) &#123;</div><div class="line">        concat_axis_ = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(param.concat_dim());<span class="comment">// 拼接的Channel</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        concat_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());<span class="comment">//默认C融合</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;top_shape = bottom[<span class="number">0</span>]-&gt;shape();<span class="comment">//先初始一下bottom[0]--top_shape</span></div><div class="line">    num_concats_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,concat_axis_); <span class="comment">//N;</span></div><div class="line">    concat_input_size_ = bottom[<span class="number">0</span>]-&gt;count(concat_axis_+<span class="number">1</span>);<span class="comment">//H*W</span></div><div class="line">    <span class="keyword">int</span> bottom_count_sum = bottom[<span class="number">0</span>]-&gt;count();    <span class="comment">// 输出count</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">1</span>; i &lt; bottom.size(); i++) &#123; <span class="comment">// 决定输出的Size</span></div><div class="line">        CHECK_EQ(bottom[i]-&gt;num_axes(),num_axes);<span class="comment">//NCHW 四维</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; num_axes; j++) &#123;</div><div class="line">            <span class="keyword">if</span> (j == concat_axis_) &#123;</div><div class="line">                top_shape[j] += bottom[i]-&gt;shape(j);<span class="comment">//拼接</span></div><div class="line">            &#125;</div><div class="line">            <span class="comment">//除了合并的通道shape要求可以不同，其余的都要相同</span></div><div class="line">            CHECK_EQ(bottom[i].shape(j),top_shape[j]);</div><div class="line">        &#125;</div><div class="line">        bottom_count_sum += bottom[i]-&gt;count();</div><div class="line">    &#125;</div><div class="line">    top[<span class="number">0</span>]-&gt;Reshape(top_shape);</div><div class="line">    <span class="comment">//其实没必要，这不出错则之前就会报错</span></div><div class="line">    CHECK_EQ(bottom_count_sum,top[<span class="number">0</span>]-&gt;count());</div><div class="line">    <span class="keyword">if</span> (bottom.size() == <span class="number">1</span>) &#123;<span class="comment">//类似于一个layer的拷贝</span></div><div class="line">        top[<span class="number">0</span>]-&gt;ShareData(*bottom[<span class="number">0</span>]);</div><div class="line">        top[<span class="number">0</span>]-&gt;ShareDiff(*bottom[<span class="number">0</span>]);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Forward_cpu 前向过程，比较简单，</span></div><div class="line"><span class="comment">// for循环完成整个的copy过程</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ConcatLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">if</span> (bottom.size() == <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">return</span> ;<span class="comment">// Reshape 的时候完成了top的复制</span></div><div class="line">    &#125;</div><div class="line">    Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">    <span class="keyword">int</span> offset = <span class="number">0</span>; <span class="comment">// 合并channel的偏移</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> top_concat_axis = top[<span class="number">0</span>]-&gt;shape(concat_axis_);<span class="comment">//</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">        <span class="keyword">const</span> Dtype* bottom_data = bottom[i]-&gt;cpu_data();</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> bottom_concat_axis = bottom[i]-&gt;shape(concat_axis_);<span class="comment">//当前C</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; num_concats_; n++) &#123; <span class="comment">//样本的循环</span></div><div class="line">        <span class="comment">//加入N*C*H*W,N循环，每次copy C*(H*W) 到top的 bottom_concat_axis*(H*W)</span></div><div class="line">        <span class="comment">// bottom是n*C_bottom*(H*W) top是( n*C_top + c)*(H*W)</span></div><div class="line">            caffe_copy(bottom_concat_axis*concat_input_size_,<span class="comment">//C_bottom*(H*W)</span></div><div class="line">            bottom_data+n*bottom_concat_axis*concat_input_size_,</div><div class="line">            top_data+(n*top_concat_axis+offset)*concat_input_size_);</div><div class="line">        &#125;</div><div class="line">        offset += bottom_concat_axis;<span class="comment">//处理完一个bottom,offset+C_bottom</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// BackFord_cpu过程，由于使用的是concat,输出只是输入的拼接，因此</span></div><div class="line"><span class="comment">// 只需要将top.diff 拆分为多块，每一块的bottom.diff对应top.diff</span></div><div class="line"><span class="comment">// offset 每次加上bottom的C</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ConcatLayer&lt;Dtype&gt;::BackFord_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp;propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">if</span> (bottom.size() == <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">return</span> ;<span class="comment">// Reshape的室友ShareDiff已经copy</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = cpu_diff(); <span class="comment">// top层的loss</span></div><div class="line">    <span class="keyword">int</span> offset = <span class="number">0</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> top_concat_axis = top[<span class="number">0</span>]-&gt;shape(concat_axis_);<span class="comment">//C_top</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</div><div class="line">        <span class="keyword">if</span> (propagate_down[i]) &#123;</div><div class="line">            <span class="keyword">int</span> bottom_concat_axis = bottom[i]-&gt;shape(concat_axis_);<span class="comment">//C_bottom</span></div><div class="line">            Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff();</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; num_concats_; n++) &#123; <span class="comment">//样本遍历</span></div><div class="line">                caffe_copy(bottom_concat_axis*concat_input_size_,<span class="comment">//count</span></div><div class="line">                top_diff+(n*top_concat_axis+offset)*concat_input_size_,</div><div class="line">                bottom_diff+n*bottom_concat_axis*concat_input_size_);</div><div class="line">            &#125;</div><div class="line">            offset += bottom_concat_axis;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="5-SliceLayer"><a href="#5-SliceLayer" class="headerlink" title="$(5) \, SliceLayer$"></a>$(5) \, SliceLayer$</h5><p>  $SliceLayer$与$concat$是一个相反的过程，只不过$slice$是$bottom$分层，而$concat$是$bottom$的组合，通过$slice_point$来控制切片的格局,$axis$控制切片的通道.<br>1.基本数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> num_slice_; <span class="comment">//一般为N，即slice_axis的前面的乘积</span></div><div class="line"><span class="keyword">int</span> slice_size_; <span class="comment">//切成几片</span></div><div class="line"><span class="keyword">int</span> slice_axis_; <span class="comment">// NCHW哪一个开始切;</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;slice_point_;<span class="comment">//prototxt的slice_point是134表示0-1 1-3 3-4</span></div></pre></td></tr></table></figure></p><p>2.基本成员函数<br>  基本成员函数包含$LayerSetup$,$Reshape$,$Forward$,$Backward$,具体实现如下:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LayerSetup 读取prototxt的参数</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SliceLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> SliceParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.slice_param();</div><div class="line">    <span class="comment">//类似concat axis或者 slice_dim prototxt中选一个</span></div><div class="line">    CHECK(!(slice_param.has_axis() &amp;&amp; slice_param.has_slice_dim()));</div><div class="line">    slice_point_.clear();</div><div class="line">    <span class="comment">//其实就是把prototxt的slice_point参数push到slice_point_中</span></div><div class="line">    <span class="comment">//可以写成for i:slice_point_size(),slice_point_.push_back()</span></div><div class="line">    <span class="built_in">std</span>::copy(param.slice_point().begin(),</div><div class="line">            param.slice_point().end(),<span class="built_in">std</span>::back_inserter(slice_point_));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Reshape ,top的size对应 slice_point_.size()+1 slice_point记录index</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SliceLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_axes = bottom[<span class="number">0</span>]-&gt;num_axes();</div><div class="line">    <span class="keyword">const</span> SliceParameter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.slice_param();</div><div class="line"></div><div class="line">    <span class="comment">//这里判断slice_dim的原因是，axis有default = 1</span></div><div class="line">    <span class="keyword">if</span> (param.has_slice_dim()) &#123;</div><div class="line">        slice_axis_ = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(param.slice_dim());</div><div class="line">        <span class="comment">//slice_axis_满足0----num_axes</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        slice_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());<span class="comment">//一般为1</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//原始shape,后续只需要修改slice_axis_的shape即可</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;top_shape = bottom[<span class="number">0</span>]-&gt;shape();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bottom_slice_axis = bottom[<span class="number">0</span>]-&gt;shape(slice_axis_);<span class="comment">//切分的通道容量</span></div><div class="line">    num_slice_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,slice_axis_);<span class="comment">//一般为N</span></div><div class="line">    slice_size = bottom[<span class="number">0</span>]-&gt;count(slice_axis_+<span class="number">1</span>);<span class="comment">//一般为H*W</span></div><div class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">    <span class="keyword">if</span> (slice_point_.size() != <span class="number">0</span>) &#123;</div><div class="line">        CHECK_EQ(slice_point_.size(),top.size()<span class="number">-1</span>);</div><div class="line">        <span class="keyword">int</span> prev = <span class="number">0</span>;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; slices;<span class="comment">//存放每个slice的Channels的大小</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; slice_point_.size(); i++) &#123;</div><div class="line">            CHECK_GT(slice_point_[i],prev);<span class="comment">//slice_point_的值要递增</span></div><div class="line">            slices.push_back(slice_point_[i] - prev);</div><div class="line">            prev = slice_point[i];</div><div class="line">        &#125;</div><div class="line">        slices.push_back(bottom_slice_axis-prev);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top.size(); i++) &#123;</div><div class="line">            top_shape[slice_axis_] = slices[i];</div><div class="line">            top[i]-&gt;Reshape(top_shape);</div><div class="line">            count += top[i]-&gt;count();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// slice_point_ = 0,则根据top的size来进行均分</span></div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        CHECK_EQ(bottom_slice_axis % top.size(),<span class="number">0</span>);<span class="comment">//要整除</span></div><div class="line">        top_shape[slice_axis_] = bottom_slice_axis / top.size();<span class="comment">//每块的Channel</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top.size(); i++) &#123;</div><div class="line">            top[i]-&gt;Reshape(top_shape);</div><div class="line">            count += top[i].count();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    CHECK_EQ(count,bottom[<span class="number">0</span>]-&gt;count());<span class="comment">//累加的top和bottom[0] count相同</span></div><div class="line">    <span class="keyword">if</span> (top.size() == <span class="number">1</span>) &#123;</div><div class="line">        top[<span class="number">0</span>]-&gt;ShareData(*bottom[<span class="number">0</span>]);<span class="comment">// 类似于copy</span></div><div class="line">        top[<span class="number">0</span>]-&gt;ShareDiff(*bottom[<span class="number">0</span>]);<span class="comment">// 类似于copy</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// forward的过程，需要for top.size然后copy bottom的值</span></div><div class="line"><span class="comment">// forward过程和concat的backward过程一致，只是diff--&gt;data</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SliceLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">if</span> (top.size() == <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">return</span> ;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> offset = <span class="number">0</span>; <span class="comment">// 每次offset 加上 一个top的channels</span></div><div class="line">    Const Dtype* bottom_data = bottom-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bottom_slice_axis = bottom[<span class="number">0</span>]-&gt;shape(slice_axis_);<span class="comment">//总C_bottom</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top.size(); i++) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> top_slice_axis = top[i]-&gt;shape(slice_axis_);<span class="comment">//当前C_top</span></div><div class="line">        Dtype* top_data = top[i]-&gt;mutable_cpu_data();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; num_slice_; n++) &#123; <span class="comment">//其实就是样本N</span></div><div class="line">            caffe_copy(top_slice_axis*slice_size_, <span class="comment">// 每次的copy_size C_TOP*H*W</span></div><div class="line">            bottom+(n*bottom_slice_axis+offset)*slice_size_,<span class="comment">//bottom地址</span></div><div class="line">            top+n*top_slice_axis*slice_size_);<span class="comment">//top的第n个样本的起始地址</span></div><div class="line">        &#125;</div><div class="line">        offset += top_slice_axis;<span class="comment">// 每次bottom的channels偏移一个top的C_top</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// backward 反向的top的梯度对应bottom的一部分，因此反向类似于</span></div><div class="line"><span class="comment">// concat的前向，因此可以写出如下代码</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SliceLayer&lt;Dtype&gt;::BackFord_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">if</span> (top.size() == <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">return</span> ;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bottom_slice_axis = bottom[<span class="number">0</span>]-&gt;shape(slice_axis_);</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">int</span> offset = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; top.size(); i++) &#123;</div><div class="line">        <span class="keyword">if</span> (propagate_down[i]) &#123;</div><div class="line">            <span class="keyword">const</span> <span class="keyword">int</span> top_slice_axis = top[i]-&gt;shape(slice_axis_);</div><div class="line">            <span class="keyword">const</span> Dtype* top_diff = top[i]-&gt;cpu_diff();</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; num_slice_; n++) &#123; <span class="comment">// 样本数目</span></div><div class="line">                copy(top_slice_axis* slice_size_, <span class="comment">// 每次copy的数据量</span></div><div class="line">                    top_diff+n*top_slice_axis*slice_size_,<span class="comment">//top的地址</span></div><div class="line">                bottom_diff+(n*bottom_slice_axis+offset)*slice_size_);</div><div class="line">            &#125;</div><div class="line">            offset += top_slice_axis;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="6-FlatternLayer"><a href="#6-FlatternLayer" class="headerlink" title="$(6)\,FlatternLayer$"></a>$(6)\,FlatternLayer$</h5><p>  $flatternLayer$实际完成输入维度的压缩，主要在$reshape$的操作上，<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Reshape 64*10*30*30 axis = 1，end_axis =2则 10*300*30</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> FlatternLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">  <span class="keyword">const</span> FlatternParameter&amp; param = <span class="keyword">this</span>&gt;layer_param_.flattern_param();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> start_axis = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.axis());</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> end_axis = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(param.end_axis());</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;top_shape;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; start_axis; i++) &#123;</div><div class="line">    top_shape.push_back(bottom[<span class="number">0</span>]-&gt;shape(i));<span class="comment">//前面的shape保持不变</span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> flattern_dim = bottom[<span class="number">0</span>]-&gt;count(start_axis,end_axis+<span class="number">1</span>);</div><div class="line">  top_shape.push_back(flattern_dim);</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = end_axis+<span class="number">1</span>; i &lt; bottom[<span class="number">0</span>].num_axes(); i++) &#123;</div><div class="line">    top_shape.push_back(bottom[<span class="number">0</span>]-&gt;shape(i));</div><div class="line">  &#125;</div><div class="line">  top[<span class="number">0</span>]-&gt;Reshape(top_shape);</div><div class="line">&#125;</div><div class="line"><span class="comment">//forward 和 backward同正常的feedforwad相同 HCHW展开式相同的</span></div><div class="line">top[<span class="number">0</span>]-&gt;ShareData(*bottom[<span class="number">0</span>]);</div><div class="line">bottom[<span class="number">0</span>]-&gt;ShareDiff(*top[<span class="number">0</span>]);</div></pre></td></tr></table></figure></p><h5 id="7-DropoutLayer"><a href="#7-DropoutLayer" class="headerlink" title="$(7)\, DropoutLayer$"></a>$(7)\, DropoutLayer$</h5><p>  $DropoutLayer$在深度学习的网络结构中对网络过拟合起到很大的作用，通过设置$drop_ratio$来控制网络的结点开闭，从而产生网络的异构多样性，降低网络的过拟合。<br>1.基本数据成员<br>  在训练阶段，结点是随机开闭的，但是在预测阶段，结点均开，但是输出会乘以概率p<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; rand_vec_;<span class="comment">//存放bottom对应位置随机出来的值</span></div><div class="line">Dtype threshold_;<span class="comment">// drop的阈值</span></div><div class="line">Dtype scale_ ;<span class="comment">// scale因子,由于结点闭合的原因，开放的结点需要乘以的因子</span></div><div class="line"><span class="comment">// 这里判断是否参数训练的时候乘以了scale,结点少了每个结点权重提高</span></div><div class="line"><span class="keyword">bool</span> scale_train_;</div></pre></td></tr></table></figure></p><p>2.基本成员函数<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">  <span class="comment">//LayerSetup,类似于activation，读取参数</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123;</div><div class="line">    NeuronLayer&lt;Dtype&gt;::LayerSetUp(bottom, top);</div><div class="line">    <span class="keyword">const</span> DropoutParamter&amp; param = <span class="keyword">this</span>-&gt;layer_param_.dropout_param();</div><div class="line">    threshold_ = param.dropout_ratio();</div><div class="line">    scale_ = <span class="number">1.</span>/(<span class="number">1</span>-threshold_);<span class="comment">// 测试的时候开放的scale</span></div><div class="line">    scale_train_ = param.scale_train();</div><div class="line">    unit_thres_ = <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;(UINT_MAX*threshold_);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Reshape</span></div><div class="line">  <span class="comment">// 类似于激励函数，只是有的toplayer会闭合置零</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123;</div><div class="line">    NeuronLayer&lt;Dtype&gt;::Reshape(botom,top);</div><div class="line">    rand_vec_.Reshape(bottom[<span class="number">0</span>]-&gt;shape());</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    Dtype top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;phase_ == <span class="string">"TRAIN"</span>) &#123; <span class="comment">// 如果是训练阶段</span></div><div class="line">       cafe_rng_bernoulli(count,<span class="number">1.</span>-threshold_,rand_vec_.mutable_cpu_data());</div><div class="line">       <span class="keyword">if</span> (scale_train_) &#123; <span class="comment">//训练时是否每个结点都提高权重</span></div><div class="line">         <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">           <span class="comment">//rand_vec_为1表示保留，为0表示闭合drop</span></div><div class="line">           top_data[i] = bottom_data[i]*rand_vec_.cpu_data()[i]*scale_;</div><div class="line">         &#125;</div><div class="line">       &#125;</div><div class="line">       <span class="keyword">else</span>&#123;</div><div class="line">         <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">           top_data[i] = bottom_data[i]*rand_vec_.cpu_data()[i];</div><div class="line">         &#125;</div><div class="line">       &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//测试阶段全开，如果训练提高权重则不处理，反之则测试的时候除以权重</span></div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">        caffe_copy(count,bottom_data,top_data);</div><div class="line">        <span class="keyword">if</span> (!scale_train_) &#123;</div><div class="line">           caffe_scal&lt;Dtype&gt;(count,<span class="number">1.</span>/scale,top_data);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// backward_cpu过程,开的才会有偏导，根据rand_vec_的值来确定</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span>* mask = rand_vec_-&gt;cpu_data();</div><div class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;phase_ == <span class="string">"TRAIN"</span>) &#123;</div><div class="line">          <span class="keyword">if</span> (scale_train_) &#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">                bottom_diff[i] = top_diff[i]*mask[i]*scale_;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">else</span>&#123;<span class="comment">//训练的时候没有乘以扩增因子</span></div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">                bottom_diff[i] = top_diff[i]*mask[i];</div><div class="line">            &#125;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">    <span class="comment">// 测试的时候</span></div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">      caffe_copy(count,top_diff,bottom_diff);</div><div class="line">      <span class="keyword">if</span> (!scale_train_) &#123;</div><div class="line">        caffe_scal&lt;Dtype&gt;(count,<span class="number">1.</span>/scale_,bottom_diff);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-CommonLayer分析&quot;&gt;&lt;a href=&quot;#Caffe-CommonLayer分析&quot; class=&quot;headerlink&quot; title=&quot;Caffe CommonLayer分析&quot;&gt;&lt;/a&gt;Caffe CommonLayer分析&lt;/h3&gt;&lt;p&gt;  $Caffe$中包含了很多通用的功能层，包含了$concat$,$slice$,$split$,$crop$,$flip$,$scale_layer$等,这些层在网络中经常被使用，本文也将对其中的常见layer进行说明与源码分析。&lt;br&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe NeuronLayer分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0(Activation)%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe 激励函数(Activation)分析/</id>
    <published>2017-10-20T03:16:01.000Z</published>
    <updated>2017-10-24T14:28:06.536Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-NeuronLayer"><a href="#Caffe-NeuronLayer" class="headerlink" title="Caffe_NeuronLayer"></a>Caffe_NeuronLayer</h3><p>  一般来说，激励层的输入输出尺寸一致，为非线性函数，完成非线性映射，从而能够拟合更为复杂的函数表达式激励层都派生于NeuronLayer: class XXXlayer : public NeuronLayer<dtype></dtype></p><h4 id="1-基本函数"><a href="#1-基本函数" class="headerlink" title="1.基本函数"></a>1.基本函数</h4><p>  激励层的基本函数较为简单，主要包含构造函数和前向、后向函数<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">XXXLayer</span><span class="params">(<span class="keyword">const</span> LayerParameter&amp; param)</span></span></div><div class="line">        :NeuronLayer&lt;Dtype&gt;(param)&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">"layerNane"</span>; &#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure></p><a id="more"></a><h4 id="2-常用-Neuron-层"><a href="#2-常用-Neuron-层" class="headerlink" title="2.常用$Neuron$层"></a>2.常用$Neuron$层</h4><h5 id="1-Relu-PRelu-Rectufied-Linear-Units"><a href="#1-Relu-PRelu-Rectufied-Linear-Units" class="headerlink" title="(1) Relu/PRelu Rectufied Linear Units"></a>(1) Relu/PRelu Rectufied Linear Units</h5><p>   ReLU的函数表达式为 $f(x) = x*(x&gt;0) + negative_slope*x*(x &lt;= 0)$ 具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">//forward_cpu</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; <span class="comment">// 根据bottom求解top</span></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();<span class="comment">//const 不可修饰</span></div><div class="line">      Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();<span class="comment">//可修饰</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//因为count_一致，也可用top</span></div><div class="line">      Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         top_data[i] = bottom_data[i]*(bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">                    + negative_slope*bottom_data[i]*(bottom_data[i] &lt;= <span class="number">0</span>);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="comment">//Backward_cpu</span></div><div class="line">  <span class="comment">// 导数形式 f'(x) = 1 x&gt;0 ; negative_slope*x x&lt;0</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">      <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>].cpu_diff();<span class="comment">//top diff</span></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>].cpu_data();<span class="comment">//用以判断x是否大于0</span></div><div class="line">      Dtype* bottom_diff = bottom[<span class="number">0</span>].cpu_diff();<span class="comment">//bottom diff</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>].count();</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         bottom_diff[i] = top_diff[i]*(bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">                    +negative_slope*(bottom_data[i] &lt;= <span class="number">0</span>);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">// Relu 函数形式简单，导函数简单，能有效的解决梯度弥散问题，但是当x小于0时，易碎</span></div><div class="line"><span class="comment">// 但是网络多为多神经元，所以实际应用中不会影响到网络的正常训练。</span></div></pre></td></tr></table></figure></p><h5 id="2-Sigmoid-S曲线"><a href="#2-Sigmoid-S曲线" class="headerlink" title="(2) Sigmoid (S曲线)"></a>(2) Sigmoid (S曲线)</h5><p>   Sigmoid函数表达式为$f(x) = 1./(1+exp(-x))$;值域0-1，常作为BP神经网络的激活函数<br>由于输出为0-1，也作为logistic回归分析的概率输出函数。具体实现如下;<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">    <span class="comment">//定义一个sigmoid函数方便计算</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">sigmoid</span><span class="params">(Dtype x)</span></span>&#123;</div><div class="line">       <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1.</span>+<span class="built_in">exp</span>(-x));</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//前向 直接带入sigmoid函数即可</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">        <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">        <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">        Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();<span class="comment">//需要计算</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//N*C*H*W;</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">           top_data[i] = sigmoid(bottom_data[i]);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//Backward_cpu 由于f'(x) = f(x)*(1-f(x))，所以需要top_data</span></div><div class="line">    <span class="comment">// bottom_diff = top_diff*f'(bottom_data) = top_diff*top_data*(1-top_data)</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">        <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">        <span class="keyword">const</span> Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">        Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff(); <span class="comment">//需要计算</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">            <span class="comment">//top_data[i] == sigmoid(bottom_data[i]);</span></div><div class="line">            bottom_diff[i] = top_diff[i]*top_data[i]*(<span class="number">1.</span>-top_data[i]);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="comment">// Sigmoid函数可以作为二分类的概率输出，也可以作为激活函数完成非线性映射，但是网络</span></div><div class="line"><span class="comment">// 增加时，容易出现梯度弥散问题，目前在CNN中基本不使用</span></div></pre></td></tr></table></figure></p><h5 id="3-TanH-双正切函数"><a href="#3-TanH-双正切函数" class="headerlink" title="(3)TanH,双正切函数"></a>(3)TanH,双正切函数</h5><p>  TanH函数的表达式为 $f(x) =\frac{(1.-exp(-2x))}{(1.+exp(-2x))}$;值域0-1,与sigmoid函数有相同的问题,<br>但是TanH在RNN中使用较为广泛,<a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">理由参考</a>，具体实现如下所示。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//定义一个tanH的函数表达式,实际已经封装</span></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">TanH</span><span class="params">(Dtype x)</span></span>&#123;</div><div class="line">   <span class="keyword">return</span> (<span class="number">1.</span>-<span class="built_in">exp</span>(<span class="number">-2</span>*x))/(<span class="number">1.</span>+<span class="built_in">exp</span>(<span class="number">-2</span>*x));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Forward_cpu</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> TanHLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">        top[i] = TanH(bottom_data[i]);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Backward_cpu f'(x) = 1-f(x)*f(x);</span></div><div class="line"><span class="comment">// bottom_diff = top_diff(1-top_data*top_data);</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> TanHLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff(); <span class="comment">//需要计算</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">        <span class="comment">//top_data[i] == TanH(bottom_data[i]);</span></div><div class="line">        bottom_diff[i] = top_diff[i]*(<span class="number">1.</span>-top_data[i]*top_data[i]);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><pre><code>其他的激励函数就不在枚举，可以查看具体的caffe源码，实现大致相同</code></pre><h3 id="3-说明"><a href="#3-说明" class="headerlink" title="3.说明"></a>3.说明</h3><h4 id="1-梯度弥散和梯度爆炸"><a href="#1-梯度弥散和梯度爆炸" class="headerlink" title="(1) 梯度弥散和梯度爆炸"></a>(1) 梯度弥散和梯度爆炸</h4><p>  网络方向传播时，loss经过激励函数会有$loss*\partial{f(x)}$,而如sigmoid的函数，<br>max($\partial{f(x)}$)只有1/4因此深层网络传播时loss越来越小，则出现前层网络未完整学习而后层网络学习饱和的现象</p><h4 id="2-Caffe激励层的构建"><a href="#2-Caffe激励层的构建" class="headerlink" title="(2) Caffe激励层的构建"></a>(2) Caffe激励层的构建</h4><p>  如上述的代码所示，激励层主要完成forward和Bacward的函数实现即可，由构建的函数表达式推导出它的导函数形式，弄懂bottom_data,top_data,bottom_diff,top_diff即可</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0(Activation" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a>%E5%88%86%E6%9E%90/)<br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-NeuronLayer&quot;&gt;&lt;a href=&quot;#Caffe-NeuronLayer&quot; class=&quot;headerlink&quot; title=&quot;Caffe_NeuronLayer&quot;&gt;&lt;/a&gt;Caffe_NeuronLayer&lt;/h3&gt;&lt;p&gt;  一般来说，激励层的输入输出尺寸一致，为非线性函数，完成非线性映射，从而能够拟合更为复杂的函数表达式激励层都派生于NeuronLayer: class XXXlayer : public NeuronLayer&lt;dtype&gt;&lt;/dtype&gt;&lt;/p&gt;
&lt;h4 id=&quot;1-基本函数&quot;&gt;&lt;a href=&quot;#1-基本函数&quot; class=&quot;headerlink&quot; title=&quot;1.基本函数&quot;&gt;&lt;/a&gt;1.基本函数&lt;/h4&gt;&lt;p&gt;  激励层的基本函数较为简单，主要包含构造函数和前向、后向函数&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;explicit&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;XXXLayer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; LayerParameter&amp;amp; param)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        :NeuronLayer&amp;lt;Dtype&amp;gt;(param)&amp;#123;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* &lt;span class=&quot;title&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;/span&gt;&amp;#123; &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;layerNane&quot;&lt;/span&gt;; &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Net分析</title>
    <link href="http://www.enjoyai.site/2017/10/19/Caffe_Net/"/>
    <id>http://www.enjoyai.site/2017/10/19/Caffe_Net/</id>
    <published>2017-10-19T06:42:00.000Z</published>
    <updated>2017-10-20T17:24:20.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-Net"><a href="#Caffe-Net" class="headerlink" title="Caffe_Net"></a>Caffe_Net</h1><h3 id="1-基本数据"><a href="#1-基本数据" class="headerlink" title="1.基本数据"></a>1.基本数据</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_; <span class="comment">// 记录每一层的layer参数</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; bottom_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; &gt; bottom_need_backward_;</div><div class="line"><span class="comment">/// top_vecs stores the vectors containing the output for each layer</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; top_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; param_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; layer_names_;</div><div class="line"><span class="comment">//learnable_params_[learnable_param_ids_[i]] == params_[i].get()</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;<span class="comment">//层间权重与bias</span></div></pre></td></tr></table></figure><a id="more"></a><h3 id="2-常用的函数"><a href="#2-常用的函数" class="headerlink" title="2. 常用的函数"></a>2. 常用的函数</h3><pre><code>介绍了Caffe内的Net的常用函数:</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"> <span class="function"><span class="keyword">const</span> <span class="built_in">string</span>&amp; <span class="title">name</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> name_;&#125;<span class="comment">//网络的名称</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; layer_names&#123;<span class="keyword">return</span> layer_names_;&#125;<span class="comment">// net每层的layer名称</span></div><div class="line"> <span class="comment">// net内每层的layer的Blob名称</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; blob_names()&#123;<span class="keyword">return</span> blob_names_;&#125;</div><div class="line"> <span class="comment">//net内层次间的权值与bias</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt;&amp; blobs()&#123;<span class="keyword">return</span> blob_;&#125;;</div><div class="line"> <span class="comment">//net内的layers</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt;&gt;&gt;&amp; layers()&#123;<span class="keyword">return</span> layers_;&#125;;</div><div class="line">  <span class="comment">//net-&gt;bottom_vecs() 返回该layer的输入，输出向量，</span></div><div class="line">  <span class="comment">//以及具体的 top_id_vecs[layer_id][top_id];</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; bottom_vecs()&#123; <span class="keyword">return</span> bottom_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; top_vecs() &#123; <span class="keyword">return</span> top_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;&amp; bottom_id_vecs()&#123; <span class="keyword">return</span> bottom_id_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;&amp; top_id_vecs() &#123; <span class="keyword">return</span> top_id_vecs_;&#125;</div><div class="line"> <span class="function"><span class="keyword">void</span> <span class="title">CopyTrainedLayersFrom</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span> trained_filename)</span></span>;<span class="comment">//加载权重</span></div><div class="line"> <span class="comment">//网络的输入输出</span></div><div class="line"> <span class="comment">//感觉等效于bottom_vecs_[0]</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; input_blobs()&#123;<span class="keyword">return</span> net_input_blobs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; output_blobs()</div><div class="line"> &#123;<span class="keyword">return</span> net_output_blobs;&#125;<span class="comment">//top_vecs[top_vecs.size()-1];</span></div><div class="line"></div><div class="line"> <span class="function"><span class="keyword">const</span> <span class="keyword">int</span> <span class="title">num_input</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> net_input_blobs_.size()&#125;;<span class="comment">//输入blob的size</span></div><div class="line"> <span class="comment">//has_blob()然后find return</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;blob_by_name(<span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name);</div><div class="line"></div><div class="line"><span class="comment">// 前向计算loss和网络的输出</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; forward(Dtype* loss = <span class="literal">NULL</span>);</div><div class="line"><span class="comment">// --- *loss = ForwardFromTo(0.layers_.size()-1);</span></div><div class="line"><span class="comment">// --- 此处调用 Dtype* Net&lt;Dtype&gt;::ForwardFrom(int start,int end)</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = start; i &lt; end; i++)&#123;</div><div class="line">    <span class="comment">//多态，调用具体的Layer的Forward函数,并返回该层次的loss</span></div><div class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i],top_vecs_[i]);</div><div class="line">    loss += layer_loss;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> loss;</div><div class="line"></div><div class="line"><span class="comment">// backward反向，更新权值</span></div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Backward()&#123; <span class="comment">//</span></div><div class="line">    BackwardFromTo(layers_size()<span class="number">-1</span>,<span class="number">0</span>); <span class="comment">// 具体函数实现如第三部分</span></div><div class="line">    <span class="keyword">if</span> (debug_info_) &#123;</div><div class="line">        <span class="comment">/*层次的参数*/</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3-具体函数实现"><a href="#3-具体函数实现" class="headerlink" title="3.具体函数实现"></a>3.具体函数实现</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> Net&lt;Dtype&gt;::AppendBottom(<span class="keyword">const</span> NetParamter&amp; param, <span class="keyword">int</span> layer_id,</div><div class="line"><span class="keyword">int</span> bottom_id,<span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* availabel_blobs,<span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;* blob_name_to_idx)&#123;</div><div class="line">    <span class="keyword">const</span> LayerParammeter&amp; layer_param = param.layer(layer_id);</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = layer_param.bottom(bottom_id);</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> blob_id = (*blob_name_to_idx)[blob_name];</div><div class="line">    <span class="comment">//layer输入的shape等</span></div><div class="line">    bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());</div><div class="line">    bottom_id_vecs_[layer_id].push_back(blob_id);</div><div class="line">    <span class="comment">//LOG CONV&lt;--data 等,只要是丢入输入</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// learnable_params_</span></div><div class="line">  <span class="comment">//conv的shape一般为num_output*input_channels*kernel_width*kernel_height</span></div><div class="line">  <span class="comment">//bias的shape一般为Num_output</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Net&lt;Dtype&gt;::AppendParam(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</div><div class="line">                           <span class="keyword">const</span> <span class="keyword">int</span> param_id) &#123;</div><div class="line">       <span class="keyword">const</span> <span class="keyword">int</span> learnable_param_id = learnable_params_.size();</div><div class="line">       learnable_params_.push_back(params_[net_param_id].get());</div><div class="line">       learnable_param_ids_.push_back(learnable_param_id);</div><div class="line">       has_params_lr_.push_back(param_spec-&gt;has_lr_mult());</div><div class="line">       has_params_decay_.push_back(param_spec-&gt;has_decay_mult());</div><div class="line">       params_lr_.push_back(param_spec-&gt;lr_mult());</div><div class="line">       params_weight_decay_.push_back(param_spec-&gt;decay_mult());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFromTo(<span class="keyword">int</span> start,<span class="keyword">int</span> end)&#123;</div><div class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = start;i &gt;= end;--i)&#123;</div><div class="line">         <span class="comment">//backward 调用各层次的backward更新权值和bias</span></div><div class="line">         layers_[i].Backward(top_vecs_[i],bottom_need_backward_[i],</div><div class="line">                            bottom_vecs_[i]);</div><div class="line">      &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h3 id="4-基本流程"><a href="#4-基本流程" class="headerlink" title="4.基本流程"></a>4.基本流程</h3><p>  基本流程：Net构造函数开始</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 递归更新变量</span></div><div class="line">vectot&lt;<span class="built_in">string</span>&gt;*stage ;</div><div class="line"><span class="keyword">int</span> level;</div><div class="line"></div><div class="line"><span class="comment">//起始调用</span></div><div class="line">net_.reset(<span class="keyword">new</span> Net&lt;<span class="keyword">float</span>&gt;(model_file, TEST));</div><div class="line"></div><div class="line"><span class="comment">//送入prototxt文件和Test OR Train</span></div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Net</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; param_file,Phase phase,<span class="keyword">const</span> <span class="keyword">int</span> level = <span class="number">0</span>,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;* stage = <span class="literal">NULL</span>,<span class="keyword">const</span> Net* root_net = <span class="literal">NULL</span>)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">// 解析保存在NetParamter param内,这里用到了</span></div><div class="line"><span class="comment">//protobuf::TextFormat::Parse(FileInputStream*,param)</span></div><div class="line">ReadNetParamsFromTextFileOrDie(param_file,&amp;param);</div><div class="line"></div><div class="line"><span class="comment">// 读取了NetParamter 后需要进行整个网络的初始化工作</span></div><div class="line">Init(param); <span class="comment">//初始化网络的接口，下续为具体实现</span></div><div class="line">FilterNet(param, &amp;filtered_param);<span class="comment">// 打印网络结构</span></div><div class="line"></div><div class="line"><span class="comment">/*内部会完成split added 如果有必要(残差结构),记录层与层之间的联系关系与层次的名称</span></div><div class="line"><span class="comment">等，是否有loss_weight，layer的size等*/</span></div><div class="line">InsertSplits(filtered_param,&amp;param);</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; param.layer_size(); i++) &#123; <span class="comment">//遍历setupLayer</span></div><div class="line">   <span class="keyword">const</span> LayerParammeter&amp; layer_param = param.layer(i);<span class="comment">//层次的参数</span></div><div class="line">   layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</div><div class="line">   <span class="comment">// CreateLayer会走layer_factory的CreateLayer的注册 ,比如input,conv,bn...</span></div><div class="line">   layer_names_.push_back(layer_param.name());</div><div class="line"></div><div class="line">   <span class="comment">//开始继续遍历每层输入的具体的细节,第i个layer的第botom_id个输入</span></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size();</div><div class="line">   bottom_id++) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> blob_id =</div><div class="line">      AppendBottom(param,i,bottom_id,&amp;availabel_blobs,&amp;blob_name_to_idx);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">//开始继续遍历每层输出的具体细节，第i个layer的第 top_id的输出</span></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; layer_param.top_size();</div><div class="line">   top_id++) &#123;</div><div class="line">      AppendTop(param,i,top_id,&amp;availabel_blobs,&amp;blob_name_to_idx);</div><div class="line">       <span class="keyword">if</span> (layer_param.type()== <span class="string">"Input"</span>) &#123;<span class="comment">//输入</span></div><div class="line">         <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size() - <span class="number">1</span>;</div><div class="line">         net_input_blob_indices_.push_back(blob_id);</div><div class="line">         net_input_blobs_.push_back(blobs_[blob_id].get());</div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line"> <span class="comment">//多态，具体调用具体的layer的Setup函数</span></div><div class="line">  layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</div><div class="line"></div><div class="line">  <span class="comment">//每个输出遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size();</div><div class="line">  top_id++) &#123;</div><div class="line">     <span class="comment">/*完成层次的blob_loss_weights,并统计memory_used_*/</span>;</div><div class="line">     memory_used_ += top_vecs_[layer_id][top_id]-&gt;count();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//总的memory_used_: memory_used_*sizeof(Dtype);</span></div><div class="line"></div><div class="line">  <span class="comment">//如果层次间有学习权值和偏置，则需要再次设置，比如conv</span></div><div class="line">  <span class="comment">//num_param_blobs weights And bias</span></div><div class="line">  <span class="comment">// relu pooling等层无中间权值参数，则num_param_blobs = 0</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</div><div class="line">      AppendParam(param, layer_id, param_id);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*接下来需要研究网络的backwards问题，决定哪些层次对loss有贡献，并且检查哪些层次</span></div><div class="line"><span class="comment">不需要back_propagate_down操作，遍历是反向的操作一个layer是否需要回溯计算，主要</span></div><div class="line"><span class="comment">依据两个方面：(1)该layer的top blob 是否参与loss的计算；(2):该layer的bottom</span></div><div class="line"><span class="comment">blob 是否需要回溯计算，比如Data层一般就不需要backward computation */</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> layer_id = layers_.size()<span class="number">-1</span>; layer_id &gt;= <span class="number">0</span>; --layer_id)&#123;</div><div class="line">   <span class="keyword">bool</span> layer_contributes_loss = <span class="literal">false</span>;<span class="comment">//默认是无贡献的</span></div><div class="line">   <span class="keyword">bool</span> layer_skip_propagate_down = <span class="literal">true</span>;<span class="comment">// 默认不参与backwards的loss贡献</span></div><div class="line"></div><div class="line">  <span class="comment">//Layer内的输出遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size();</div><div class="line">  top_id++) &#123;</div><div class="line">      <span class="comment">//blob_name_[index]名字</span></div><div class="line">     <span class="built_in">string</span>&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];</div><div class="line">     <span class="keyword">if</span> (layer_[layer_id]-&gt;loss(top_id)||</div><div class="line">     blobs_under_loss.find(blob_name) != blobs_under_loss.end()) &#123;</div><div class="line">         <span class="comment">//该层次的layerloss不为0或者loss_weight = 1;</span></div><div class="line">        layer_contributes_loss = <span class="literal">true</span>;</div><div class="line">     &#125;</div><div class="line">     <span class="keyword">if</span> (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) &#123;</div><div class="line">        layer_skip_propagate_down = <span class="literal">false</span>;</div><div class="line">     &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//同理 Layer内的输入遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> bottom_id = <span class="number">0</span>; bottom_id &lt; bottom_vecs_[layer_id].size();</div><div class="line">  bottom_id++) &#123;</div><div class="line">    <span class="keyword">if</span> (layer_contributes_loss) &#123;</div><div class="line">      <span class="built_in">string</span>* blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">      blobs_under_loss.insert(blob_name);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">      bottom_need_backward_[layer_id][bottom_id] = <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!bottom_need_backward_[layer_id][bottom_id]) &#123;</div><div class="line">      <span class="built_in">string</span>&amp;blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">      blok_skip_backp.insert(blob_name);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/*code*/</span></div><div class="line"></div><div class="line">&#125;<span class="comment">//init函数尾</span></div></pre></td></tr></table></figure><h3 id="5-说明"><a href="#5-说明" class="headerlink" title="5.说明"></a>5.说明</h3><p>   blob_name_to_idx是一个局部变量，其实它是在当前layer的top blob 和下一层的bottom blob间起着一个桥梁作用。 blob_name_to_idx中元素的pair是从网络最开始一层一层搭建的过程中压入map的，其中的name和id都是不重复的。name是关键字，不重复是map数据结构的必然要求，id也是不重复的，—0,1,2…blob_name_to_idx和blobs_一样，在”Normal output”的情形下，每次遍历到一个top blob的时候都会更新。</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/19/Caffe_Net/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/19/Caffe_Net/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-Net&quot;&gt;&lt;a href=&quot;#Caffe-Net&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Net&quot;&gt;&lt;/a&gt;Caffe_Net&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据&quot;&gt;&lt;a href=&quot;#1-基本数据&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据&quot;&gt;&lt;/a&gt;1.基本数据&lt;/h3&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;Layer&amp;lt;Dtype&amp;gt; &amp;gt; &amp;gt; layers_; &lt;span class=&quot;comment&quot;&gt;// 记录每一层的layer参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; &amp;gt; bottom_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; bottom_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt; &amp;gt; bottom_need_backward_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;/// top_vecs stores the vectors containing the output for each layer&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; &amp;gt; top_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; top_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; param_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;string&lt;/span&gt;&amp;gt; layer_names_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//learnable_params_[learnable_param_ids_[i]] == params_[i].get()&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; learnable_params_;&lt;span class=&quot;comment&quot;&gt;//层间权重与bias&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Layer分析</title>
    <link href="http://www.enjoyai.site/2017/10/19/Caffe_layer/"/>
    <id>http://www.enjoyai.site/2017/10/19/Caffe_layer/</id>
    <published>2017-10-19T03:31:00.000Z</published>
    <updated>2017-10-20T17:25:51.547Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://images2017.cnblogs.com/blog/888534/201710/888534-20171019224727334-359802148.png" alt=""></p><h1 id="Caffe-Layers"><a href="#Caffe-Layers" class="headerlink" title="Caffe_Layers"></a>Caffe_Layers</h1><h3 id="1-基本数据结构"><a href="#1-基本数据结构" class="headerlink" title="1.基本数据结构"></a>1.基本数据结构</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Layer层主要的的参数</span></div><div class="line">LayerParamter layer_param_; <span class="comment">// protobuf内的layer参数</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;*&gt;&gt;blobs_;<span class="comment">//存储layer的参数，</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;param_propagate_down_;<span class="comment">//表示是否计算各个blobs反向误差。</span></div></pre></td></tr></table></figure><a id="more"></a><h3 id="2-主要函数接口"><a href="#2-主要函数接口" class="headerlink" title="2.主要函数接口"></a>2.主要函数接口</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</span></span></div><div class="line"><span class="function"><span class="params">                    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function">Dtype <span class="title">Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</span></span></div><div class="line"><span class="function"><span class="params">                    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">Backward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;top,</span></span></div><div class="line"><span class="function"><span class="params"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;param_propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure><h3 id="3-具体的Layer分析"><a href="#3-具体的Layer分析" class="headerlink" title="3.具体的Layer分析"></a>3.具体的Layer分析</h3><pre><code>具体的常用Layer分析</code></pre><h4 id="1-数据层-DataLayer"><a href="#1-数据层-DataLayer" class="headerlink" title="(1) 数据层(DataLayer)"></a>(1) 数据层(DataLayer)</h4><p>数据通过数据层进入Layer,可以来自于数据库(LevelDB或者LMDB),也可以来自内存，HDF5等<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Database：类型 Database</span></div><div class="line"><span class="comment">//必须参数 source,batch_size</span></div><div class="line"><span class="comment">//可选参数：rand_skip,mirror,backend[default LEVELDB]</span></div><div class="line"></div><div class="line"><span class="comment">// In-Memory：类型 MemoryData</span></div><div class="line"><span class="comment">// 必选参数：batch_size，channels,height,width</span></div><div class="line"></div><div class="line"><span class="comment">//HDF5 Input:类型 HDF5Data</span></div><div class="line"><span class="comment">//必选参数: source,batch_size</span></div><div class="line"></div><div class="line"><span class="comment">//Images : 类型 ImageData</span></div><div class="line"><span class="comment">//必要参数：source(文件名label),batch_size</span></div><div class="line"><span class="comment">//可选参数：rand_skip,shuffle,new_width,new_height;</span></div></pre></td></tr></table></figure></p><h4 id="2-激励层-neuron-layers"><a href="#2-激励层-neuron-layers" class="headerlink" title="(2) 激励层(neuron_layers)"></a>(2) 激励层(neuron_layers)</h4><pre><code>一般来说，激励层是element-wise，输入输出大小相同，一般非线性函数输入：n\*c\*h\*w输出：n\*c\*h\*w</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ReLU/PReLU</span></div><div class="line"><span class="comment">//可选参数 negative_slope 指定输入值小于零时的输出。</span></div><div class="line"><span class="comment">// f(x) = x*(x&gt;0)+negative_slope*(x&lt;=0)</span></div><div class="line"><span class="comment">//ReLU目前使用最为广泛，收敛快，解决梯度弥散问题</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"relu"</span></div><div class="line">    type:<span class="string">"ReLU"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Sigmoid</span></div><div class="line"><span class="comment">//f(x) = 1./(1+exp(-x)); 负无穷--正无穷映射到-1---1</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"sigmoid-test"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">    type:<span class="string">"Sigmoid"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="3-视觉层-vision-layer"><a href="#3-视觉层-vision-layer" class="headerlink" title="(3) 视觉层(vision_layer)"></a>(3) 视觉层(vision_layer)</h4><pre><code>常用layer操作</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">//卷积层(Convolution):类型Convolution</span></div><div class="line"> <span class="comment">//包含学习率，输出卷积核，卷积核size，初始方式，权值衰减</span></div><div class="line"> <span class="comment">//假使输入n*ci*hi*wi,则输出</span></div><div class="line"> <span class="comment">// new_h = ((hi-kernel_h)+2*pad_h)/stride+1;</span></div><div class="line"> <span class="comment">// new_w = ((wi-kernel_w)+2*pad_w)/stride+1;</span></div><div class="line"> <span class="comment">//输出n*num_output*new_h*new_w;</span></div><div class="line"> layer&#123;</div><div class="line">     name: <span class="string">"conv1"</span></div><div class="line">     type: <span class="string">"CONVOLUTION"</span></div><div class="line">     bottom: <span class="string">"data"</span></div><div class="line">     top: <span class="string">"conv1"</span></div><div class="line">     blobs_lr: <span class="number">1</span></div><div class="line">     blobs_lr: <span class="number">2</span></div><div class="line">     weight_decay: <span class="number">1</span></div><div class="line">     weight_decay: <span class="number">0</span></div><div class="line">     convolution_param &#123;</div><div class="line">         num_output: <span class="number">96</span></div><div class="line">         kernel_size: <span class="number">11</span></div><div class="line">         stride: <span class="number">4</span></div><div class="line">         weight_filler &#123;</div><div class="line">             type: <span class="string">"gaussian"</span></div><div class="line">            <span class="built_in">std</span>: <span class="number">0.01</span></div><div class="line">         &#125;</div><div class="line">         bias_filler &#123;</div><div class="line">           type: <span class="string">"constant"</span></div><div class="line">           value: <span class="number">0</span></div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="comment">//池化层(Pooling) 类型 Pooling</span></div><div class="line"><span class="comment">// (hi-kernel_h)/2+1;</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"pool1"</span></div><div class="line">    type:<span class="string">"POOLING"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">    pooling_param&#123;</div><div class="line">        pool:MAX <span class="comment">//AVE,STOCHASTIC</span></div><div class="line">        kernel_size:<span class="number">3</span></div><div class="line">        stride:<span class="number">2</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//BatchNormalization</span></div><div class="line"><span class="comment">// x' = (x-u)/δ ;y = α*x'+β;</span></div></pre></td></tr></table></figure><h4 id="4-损失层-Loss-layer"><a href="#4-损失层-Loss-layer" class="headerlink" title="(4) 损失层(Loss_layer)"></a>(4) 损失层(Loss_layer)</h4><pre><code>最小化输出于目标的LOSS来驱动学习更新</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Softmax</span></div></pre></td></tr></table></figure><h3 id="4-说明"><a href="#4-说明" class="headerlink" title="4.说明"></a>4.说明</h3><p>SetUp函数需要根据实际的参数设置进行实现，对各种类型的参数初始化；Forward和Backward对应前向计算和反向更新，输入统一都是bottom，输出为top，其中Backward里面有个propagate_down参数，用来表示该Layer是否反向传播参数。<br>在Forward和Backward的具体实现里，会根据Caffe::mode()进行对应的操作，即使用cpu或者gpu进行计算，两个都实现了对应的接口Forward_cpu、Forward_gpu和Backward_cpu、Backward_gpu，这些接口都是virtual，具体还是要根据layer的类型进行对应的计算（注意：有些layer并没有GPU计算的实现，所以封装时加入了CPU的计算作为后备）。另外，还实现了ToProto的接口，将Layer的参数写入到protocol buffer文件中。</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/19/Caffe_layer/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/19/Caffe_layer/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/888534/201710/888534-20171019224727334-359802148.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Caffe-Layers&quot;&gt;&lt;a href=&quot;#Caffe-Layers&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Layers&quot;&gt;&lt;/a&gt;Caffe_Layers&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据结构&quot;&gt;&lt;a href=&quot;#1-基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据结构&quot;&gt;&lt;/a&gt;1.基本数据结构&lt;/h3&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//Layer层主要的的参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;LayerParamter layer_param_; &lt;span class=&quot;comment&quot;&gt;// protobuf内的layer参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;gt;blobs_;&lt;span class=&quot;comment&quot;&gt;//存储layer的参数，&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;param_propagate_down_;&lt;span class=&quot;comment&quot;&gt;//表示是否计算各个blobs反向误差。&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Blob分析</title>
    <link href="http://www.enjoyai.site/2017/10/18/Caffe_blob/"/>
    <id>http://www.enjoyai.site/2017/10/18/Caffe_blob/</id>
    <published>2017-10-17T16:03:10.000Z</published>
    <updated>2017-10-20T17:24:37.013Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-blob"><a href="#Caffe-blob" class="headerlink" title="Caffe_blob"></a>Caffe_blob</h1><h3 id="1-基本数据结构"><a href="#1-基本数据结构" class="headerlink" title="1.基本数据结构"></a>1.基本数据结构</h3><p>  Blob为模板类，可以理解为四维数组，n * c * h * w的结构,Layer内为blob输入data和diff，Layer间的blob为学习的参数.内部封装了SyncedMemory类,该类负责存储分配和主机与设备的同步<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_; <span class="comment">// data指针</span></div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_; <span class="comment">// diff指针</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_; <span class="comment">// blob形状</span></div><div class="line">    <span class="keyword">int</span> count_; <span class="comment">// blob的nchw</span></div><div class="line">    <span class="comment">// 当前的Blob容量，当Blob reshape后count&gt; capacity_时，capacity_ = count_;</span></div><div class="line">    <span class="comment">// 重新new 然后 reset data和 diff</span></div><div class="line">    <span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure></p><a id="more"></a><h3 id="2-常用函数"><a href="#2-常用函数" class="headerlink" title="2.常用函数"></a>2.常用函数</h3><pre><code>Blob类中常用的函数如下所示</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">Blob&lt;<span class="keyword">float</span>&gt;test;</div><div class="line"><span class="comment">//explicit关键字的作用是禁止单参数构造函数的隐式转换</span></div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="keyword">int</span> width)</span></span>;</div><div class="line">test.shape_string();<span class="comment">//初始为空 0 0 0 0</span></div><div class="line"><span class="comment">//Reshape函数将num,channels,height,width传递给vector shape_</span></div><div class="line">test.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);<span class="comment">// shape_string() 1,2,3,4</span></div><div class="line"></div><div class="line">test.shape(i);<span class="comment">// NCHW</span></div><div class="line">test.count(<span class="keyword">int</span> start_axis,<span class="keyword">int</span> end_axis); <span class="comment">// start_axis---end_axis .x* shape[i]</span></div><div class="line">test.count();<span class="comment">// nchw  count(1) chw count(2) hw.....</span></div><div class="line"><span class="comment">//shared_ptr&lt;SyncedMemory&gt; data_-&gt;cpu_data();</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* data = test.cpu_data();</div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* diff = test.cpu_diff();</div><div class="line"><span class="keyword">float</span>* data_1 = test.mutable_cpu_data();<span class="comment">//mutable修饰的表示可以修改内部值</span></div><div class="line"><span class="keyword">float</span>* diff_1 = test.mutable_cpu_diff();</div><div class="line">test.asum_data();<span class="comment">//求和 L1范数</span></div><div class="line">test.sumsq_data();<span class="comment">//平方和 L2范数</span></div><div class="line">test.Update();<span class="comment">//data = data-diff;</span></div><div class="line">a.ToProto(BlobProto&amp; bp,<span class="literal">true</span>/<span class="literal">false</span>);<span class="comment">//(FromProto)</span></div><div class="line"><span class="comment">// if &lt; 0 ,return num_axis()+axis_index;//索引序列</span></div><div class="line"><span class="keyword">int</span> index = a.CanonicalAxisIndex(<span class="keyword">int</span> axis_index);</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">offset</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//((n*channels()+c)*height()+h)*width()+w</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">data_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_data()[offset(n,c,h,w)];</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">diff_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_diff()[offset(n,c,h,w)];</span></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span>&#123;<span class="keyword">return</span> _data&#125;;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;<span class="comment">// data乘以一个标量。同理 scale_diff();</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">bool</span> reshape = <span class="literal">false</span>)</span></span>;  <span class="comment">// copy_diff是否复制diff</span></div></pre></td></tr></table></figure><h3 id="3-写入磁盘操作"><a href="#3-写入磁盘操作" class="headerlink" title="3.写入磁盘操作"></a>3.写入磁盘操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Blob内部值写入到磁盘</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;a;</div><div class="line">a.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> count = a.count();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">    a[i] = i;<span class="comment">//init the test Blob</span></div><div class="line">&#125;</div><div class="line">BlobProto bp,bp2;</div><div class="line">a.ToProto(&amp;bp,<span class="literal">true</span>);<span class="comment">//写入data和diff到bp中</span></div><div class="line">WriteProtoToBinaryFile(bp,<span class="string">"a.blob"</span>);<span class="comment">//写入磁盘</span></div><div class="line">ReadProtoFromBinaryFile(<span class="string">"a.blob"</span>,&amp;bp2);<span class="comment">//从磁盘读取blob</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;b;</div><div class="line">b.FromProto(bp2,<span class="literal">true</span>);<span class="comment">//序列化对象bp2中克隆b，完整克隆</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; b.num(); n++) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> c = <span class="number">0</span>; c &lt; b.channels(); c++) &#123;</div><div class="line">       <span class="keyword">for</span> (<span class="keyword">size_t</span> h = <span class="number">0</span>; h &lt; b.height(); h++) &#123;</div><div class="line">           <span class="keyword">for</span> (<span class="keyword">size_t</span> w = <span class="number">0</span>; w &lt; b.width(); w++) &#123;</div><div class="line">               <span class="built_in">cout</span>&lt;&lt;<span class="string">"b["</span>&lt;&lt;n&lt;&lt;<span class="string">"]["</span>&lt;&lt;c&lt;&lt;<span class="string">"]["</span>&lt;&lt;h&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]="</span>&lt;&lt;</div><div class="line">               b[(((n*b.channels()+c)*b.height)+h)*b.width()+w]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">               <span class="comment">//(((n*c+ci)*h+hi)*w+wi)</span></div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="4-部分函数的具体实现"><a href="#4-部分函数的具体实现" class="headerlink" title="4.部分函数的具体实现"></a>4.部分函数的具体实现</h3><pre><code>本部分的实现未考虑参数是否合理。一般操作blob需要分CPU和GPU,采用math_functions具体计算</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)&#123;<span class="comment">//reshape操作</span></div><div class="line">    count_ = <span class="number">1</span>;<span class="comment">//初始count_ NCHW;</span></div><div class="line">    shape_.resize(shape.size());</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; shape.size(); i++) &#123;</div><div class="line">      count_ *= shape[i];</div><div class="line">      shape_[i] = shape[i];</div><div class="line">      <span class="keyword">if</span> (count_ &gt; capacity_) &#123; <span class="comment">//reshape的size大于了目前的最大容量</span></div><div class="line">         capacity_ = count_;</div><div class="line">         data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_*<span class="keyword">sizeof</span>(Dtype)));</div><div class="line">         diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_*<span class="keyword">sizeof</span>(Dtype)));</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">int</span> n,<span class="keyword">int</span> c,<span class="keyword">int</span> h ,<span class="keyword">int</span> w)&#123;<span class="comment">//reshape操作</span></div><div class="line">   <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;shape(<span class="number">4</span>);</div><div class="line">   shape[<span class="number">0</span>] = n;</div><div class="line">   shape[<span class="number">1</span>] = c;</div><div class="line">   shape[<span class="number">2</span>] = h;</div><div class="line">   shape[<span class="number">3</span>] = w;</div><div class="line">   Reshape(shape);</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data()&#123;</div><div class="line">   <span class="comment">//实际调用的shared_ptr&lt;SyncedMemory&gt;data_-&gt;cpu_data();,同理cpu_diff();</span></div><div class="line">   CHECK(data_);</div><div class="line">   <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Updata()&#123; <span class="comment">//data = data-diff;需要判断cpu OR gpu</span></div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">      caffe_axpy&lt;Dtype&gt;(count_,Dtype(<span class="number">-1</span>),</div><div class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span>&lt;Dtype*&gt;(diff_-&gt;cpu_data()),</div><div class="line">      <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">  &#125;</div><div class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:<span class="comment">//在gpu或者CPU/GPU已经同步</span></div><div class="line">    <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line">    <span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">      caffe_gpu_axpy&lt;Dtype&gt;(count_.Dtype(<span class="number">-1</span>),</div><div class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span>&lt;Dtype*&gt;(diff_-&gt;gpu_data()),</div><div class="line">      <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt; <span class="comment">//从source 拷贝数据,copy_diff控制是拷贝diff还是data</span></div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, <span class="keyword">bool</span> reshape) &#123;</div><div class="line">  <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</div><div class="line">    <span class="keyword">if</span> (reshape) &#123;</div><div class="line">      ReshapeLike(source);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;  <span class="comment">//copy diff</span></div><div class="line">      caffe_copy(count_, source.gpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.gpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.cpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.cpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ToProto(BlobProto* proto,<span class="keyword">bool</span> write_diff)&#123;</div><div class="line">    proto-&gt;clear_shape();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; shaoe_.size(); i++) &#123;</div><div class="line">        proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);</div><div class="line">    &#125;</div><div class="line">    proto-&gt;clear_data();</div><div class="line">    proto-&gt;clear_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* data_vec = cpu_data();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count_; i++) &#123;</div><div class="line">        proto-&gt;add_data(data_vec[i]);<span class="comment">//data写入proto</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (write_diff) &#123;</div><div class="line">        <span class="keyword">const</span> Dtype* diff_vec = cpu_diff();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count_; i++) &#123;</div><div class="line">            proto-&gt;add_diff(diff_vec[i]);<span class="comment">//diff写入proto</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="5-说明"><a href="#5-说明" class="headerlink" title="5.说明"></a>5.说明</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*Blob作为一个最基础的类，其中构造函数开辟一个内存空间来存储数据，Reshape</span></div><div class="line"><span class="comment">函数在Layer中的reshape或者forward操作中来调整top的输出维度。同时在改变Blob</span></div><div class="line"><span class="comment">大小时， 内存将会被重新分配如果内存大小不够了，并且额外的内存将不会被释放。</span></div><div class="line"><span class="comment">对input的blob进行reshape, 若立马调用Net::Backward是会出错的，因为reshape</span></div><div class="line"><span class="comment">之后，要么Net::forward或者Net::Reshape就会被调用来将新的input shape传播</span></div><div class="line"><span class="comment">到高层 */</span></div></pre></td></tr></table></figure><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/18/Caffe_blob/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/18/Caffe_blob/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-blob&quot;&gt;&lt;a href=&quot;#Caffe-blob&quot; class=&quot;headerlink&quot; title=&quot;Caffe_blob&quot;&gt;&lt;/a&gt;Caffe_blob&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据结构&quot;&gt;&lt;a href=&quot;#1-基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据结构&quot;&gt;&lt;/a&gt;1.基本数据结构&lt;/h3&gt;&lt;p&gt;  Blob为模板类，可以理解为四维数组，n * c * h * w的结构,Layer内为blob输入data和diff，Layer间的blob为学习的参数.内部封装了SyncedMemory类,该类负责存储分配和主机与设备的同步&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;protected&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; data_; &lt;span class=&quot;comment&quot;&gt;// data指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; diff_; &lt;span class=&quot;comment&quot;&gt;// diff指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; shape_; &lt;span class=&quot;comment&quot;&gt;// blob形状&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; count_; &lt;span class=&quot;comment&quot;&gt;// blob的nchw&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 当前的Blob容量，当Blob reshape后count&amp;gt; capacity_时，capacity_ = count_;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 重新new 然后 reset data和 diff&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; capacity_;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法 1-统计学习算法概述</title>
    <link href="http://www.enjoyai.site/2017/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://www.enjoyai.site/2017/09/14/统计学习方法1-统计学习算法概述/</id>
    <published>2017-09-14T06:24:11.000Z</published>
    <updated>2017-10-20T17:31:06.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计学习的主要特点"><a href="#统计学习的主要特点" class="headerlink" title="统计学习的主要特点"></a>统计学习的主要特点</h2><pre><code>统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。</code></pre><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><pre><code>监督学习(supervised learning)无监督学习(unsupervised learning)半监督学习(self-supervised learning)增强式学习(reinfoucement learning)</code></pre><a id="more"></a><h3 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h3><p>输入实际x的特征向量记做$x = (x^{(1)},x^{(2)},x^{(3)}, \cdots ,x^{(n)})^T$<br>训练集：$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),\cdots (x_n,y_n)}$<br>输入变量与输出变量均为连续变量的预测问题为回归问题；<br>输出变量为有限个离散变量的预测问题为分类问题;</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/14</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;统计学习的主要特点&quot;&gt;&lt;a href=&quot;#统计学习的主要特点&quot; class=&quot;headerlink&quot; title=&quot;统计学习的主要特点&quot;&gt;&lt;/a&gt;统计学习的主要特点&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;监督学习(supervised learning)
无监督学习(unsupervised learning)
半监督学习(self-supervised learning)
增强式学习(reinfoucement learning)
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计学习方法" scheme="http://www.enjoyai.site/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="http://www.enjoyai.site/2017/09/10/Linear-Regression/"/>
    <id>http://www.enjoyai.site/2017/09/10/Linear-Regression/</id>
    <published>2017-09-10T02:23:08.000Z</published>
    <updated>2017-10-20T17:26:47.210Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Model-and-Cost-Function-模型和损失函数"><a href="#Model-and-Cost-Function-模型和损失函数" class="headerlink" title="Model and Cost Function(模型和损失函数)"></a>Model and Cost Function(模型和损失函数)</h2><p>对于model，给出如下定义 $y = \theta x$<br>损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$<br><a id="more"></a><br>Gradient descent algorithm<br>repeat until convergence{<br>    $\quad \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$<br>}</p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>寻找两类样本正中间的划分超平面，因为该超平面对训练样本的布局扰动的容忍度最好，是最鲁棒的<br>划分超平面方程:<br>$$wx+b = 0$$<br>我们假使<br>$$<br>\begin{cases}<br>wx_i+b &gt;= 1 \qquad\quad y_i = +1 \\\<br>\\<br>wx_i+b &lt;=-1 \qquad\, y_i = -1<br>\end{cases}<br>$$<br>则距离超平面最近的几个点使得下列式子成立<br>$$\max\limits_{w,b}(\frac{2}{||w||}) \rightarrow \min_{w,b}\frac{1}{2}||w||^2$$<br>$$s.t. y_i(wx_i+b)\ge 1 i = 1,2,…,m.$$<br>通用表达式:<br>    $f(x)=w\psi(x)+b = \sum_{i=1}^{m}a_iy_i\psi(x_i)^T\psi(x)+b=\sum_{i=1}^{m}a_iy_i\kappa(x,x_i)+b$<br>$\kappa 为核函数.$</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/10/Linear-Regression/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/10/Linear-Regression/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Model-and-Cost-Function-模型和损失函数&quot;&gt;&lt;a href=&quot;#Model-and-Cost-Function-模型和损失函数&quot; class=&quot;headerlink&quot; title=&quot;Model and Cost Function(模型和损失函数)&quot;&gt;&lt;/a&gt;Model and Cost Function(模型和损失函数)&lt;/h2&gt;&lt;p&gt;对于model，给出如下定义 $y = \theta x$&lt;br&gt;损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>C++随笔</title>
    <link href="http://www.enjoyai.site/2017/09/08/C++%E9%9A%8F%E7%AC%94/"/>
    <id>http://www.enjoyai.site/2017/09/08/C++随笔/</id>
    <published>2017-09-08T05:44:33.000Z</published>
    <updated>2017-10-20T17:26:49.317Z</updated>
    
    <content type="html"><![CDATA[<h3 id="重写，重定义、重载的区别"><a href="#重写，重定义、重载的区别" class="headerlink" title="重写，重定义、重载的区别"></a>重写，重定义、重载的区别</h3><a id="more"></a><h4 id="重写"><a href="#重写" class="headerlink" title="重写"></a>重写</h4><p>$\qquad 子类(派生类)重新定义基类的虚函数方法，要求函数名，函数参数，返回类型完全相同.并\\$<br>$且基于必须是虚函数，不能有static关键字,重写函数的访问修饰符可以与基类的不同。\\$<br>$\qquad 基类指针指向派生类，若实现了重写，则调用派生类，若没，则调用基类,即实现多态$</p><h4 id="重定义"><a href="#重定义" class="headerlink" title="重定义"></a>重定义</h4><p>$\qquad 子类(派生类)重新申明和定义基类的函数，要求函数名相同，但是返回值可以不同，参数\\$<br>$不同，无论有无virtual，基类的都将被隐藏，参数相同，基类如果没有virtual，则基类的函被\\$<br>$隐藏$</p><h4 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h4><p>$\qquad函数名相同，但是他们的参数列表个数或者顺序，类型不同，且不能仅有返回类型不同，要\\$<br>$求再同一个作用于.$</p><h3 id="多态的实现方式"><a href="#多态的实现方式" class="headerlink" title="多态的实现方式"></a>多态的实现方式</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>$\qquad 多态: 即程序运行中，系统根据对象指针所指向的类别对相同的消息进行不同的方法处理$</p><h4 id="动态多态"><a href="#动态多态" class="headerlink" title="动态多态"></a>动态多态</h4><p>$\qquad 通过类的继承和虚函数机制，在程序运行期实现多态,虚函数表$</p><h4 id="静态多态"><a href="#静态多态" class="headerlink" title="静态多态"></a>静态多态</h4><p>$\qquad 函数重载；运算符重载$</p><h3 id="常用排序算法"><a href="#常用排序算法" class="headerlink" title="常用排序算法"></a>常用排序算法</h3><h4 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h4><p>$\qquad 快速排序的实现:$<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> l ,<span class="keyword">int</span> r)</span></span>&#123;<span class="comment">//或者vector</span></div><div class="line">    <span class="keyword">if</span>(l &lt; r)&#123;</div><div class="line">        <span class="keyword">int</span> i = l ,j = r ;</div><div class="line">        <span class="keyword">int</span> sed = a[i];<span class="comment">//种子点</span></div><div class="line">        <span class="keyword">while</span>(i &lt; j )&#123;</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[j] &gt; sed )</div><div class="line">                --j;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[i++] = a[j];</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[i] &lt; sed )</div><div class="line">                ++i;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[j--] = a[i];</div><div class="line">        &#125;</div><div class="line">        a[i] = sed;</div><div class="line">        quickSort(a,l,i<span class="number">-1</span>);</div><div class="line">        qucikSort(a,i+<span class="number">1</span>,r);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/08/C++%E9%9A%8F%E7%AC%94/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/08/C++%E9%9A%8F%E7%AC%94/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;重写，重定义、重载的区别&quot;&gt;&lt;a href=&quot;#重写，重定义、重载的区别&quot; class=&quot;headerlink&quot; title=&quot;重写，重定义、重载的区别&quot;&gt;&lt;/a&gt;重写，重定义、重载的区别&lt;/h3&gt;
    
    </summary>
    
      <category term="C++" scheme="http://www.enjoyai.site/categories/C/"/>
    
    
      <category term="C++" scheme="http://www.enjoyai.site/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归分析</title>
    <link href="http://www.enjoyai.site/2017/09/07/logistic/"/>
    <id>http://www.enjoyai.site/2017/09/07/logistic/</id>
    <published>2017-09-07T12:03:49.000Z</published>
    <updated>2017-10-20T17:26:23.816Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic回归分析"><a href="#Logistic回归分析" class="headerlink" title="Logistic回归分析"></a>Logistic回归分析</h3><p>$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$</p><p>$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$<br><a id="more"></a><br>$$f(x) = \frac{1}{1+exp(-z)}$$</p><p>$\quad \quad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$</p><p>$$trainingData \quad\, x^1 \quad\, x^2 \quad\, x^3 \quad\,\cdots \quad\, x^n $$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad labels \qquad   \quad  C_{1} \quad C_{1} \quad C_{2} \quad \cdots \quad C_{1} \\$<br>$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^n)$来寻找最佳的$w$和$b$<br>$$<br>w^{*},b^{*} = arg\max\limits_{w,b}(L(w,b))\Longrightarrow w^{*},b^{*} = arg\min\limits_{w,b}(-ln{L(w,b)})<br>$$</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><a href="https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="external">随机梯度下降法</a></h3><p>$\qquad 我们需要优化的函数:-ln{L(w,b)} = -{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)}\quad \\$<br>$$<br>\qquad 假设：<br>\begin{cases}<br>\hat{y} = 1 \qquad x\in1 \\\<br>\\<br>\hat{y} = 0 \qquad  x\in0<br>\end{cases}<br>\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n  w_{i}x_{i} 则<br>$$<br>$\qquad 我们需要优化的函数简化为：ln{L(w,b)} =\sum_{j=1}^{n}{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))} \\$</p><p>$\qquad 当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x) \\$<br>$\qquad 当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x)) \qquad \\$<br>$\qquad 即均满足上式 , 因此:$</p><p>$\qquad \qquad \quad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \quad \quad 而 \, \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=(1-f_{w,b}(x))*x_i \\$</p><p>$\quad \quad 同理 \quad   \frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i \qquad 则化简后:\\$<br>$\qquad \quad\,\, \qquad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \quad \qquad = \sum_{j=1}^{n}{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i} \\$</p><p>$\qquad \qquad \quad\qquad \qquad = \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i \\$</p><p>$\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i \\$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201709/888534-20170908103015851-1635753052.png" alt=""></p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><h4 id="1-为什么选用-crossEntropy-损失函数，而不用L2损失函数"><a href="#1-为什么选用-crossEntropy-损失函数，而不用L2损失函数" class="headerlink" title="1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数"></a>1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数</h4><p>$答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，\\$<br>$由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：\\$<br>$$\frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i $$<br>$则当\,\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad  \\$<br>$但是当\,\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对 \\$</p><h4 id="2-logistic-regression-的分类概率为什么选取了-sigmoid-函数"><a href="#2-logistic-regression-的分类概率为什么选取了-sigmoid-函数" class="headerlink" title="2. $logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数"></a>2. <a href="https://www.zhihu.com/question/54707359" target="_blank" rel="external">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</a></h4><p>$答: 我们假设样本的分布服从二次高斯分布，即\\$</p><p>$f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)},其中\mu为均值，\Sigma为协方差矩阵 \\$</p><p>$输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma, \\$<br>$因此需要求取最佳的高斯分布来满足样本的分布 \\$</p><p>$$Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots f_{\mu,\Sigma}(x^{N})$$<br>$$\mu^{*}，\Sigma^{*} = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma)$$<br>$$\mu^{*} = \frac{1}{N}\sum_{i=0}^{N}{x^i}$$<br>$$\Sigma^{*} = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^{*})(x^i-\mu^{*})^T}$$</p><p>$对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma \\$<br>$$\mu^1 = \sum_{i=1}^{n_1} x_i\qquad (x_i \in C_1) \quad ;\quad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) $$<br>$$\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ;\quad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T ;\quad \Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2 $$</p><p>$对于样本x，如果属于C_1则有：\\$</p><p>$\qquad \qquad\qquad \qquad P(C_{1}|x) \,\,= \frac{P(C_{1},x)}{P(x)} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+\frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+exp(-\alpha)} \\$</p><p>$其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})$</p><p>$将P(x|C_i)带入高斯分布的公式:\\$<br>$$P(C_1)=\frac{n_1}{n_1+n_2}\quad , \quad P(C_2)=\frac{n_2}{n_1+n_2} $$<br>$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)} $$<br>$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)} $$<br>$\alpha= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\$<br>$\quad =-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\$<br>$\quad =-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = (u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = wx+b\\$<br>$\quad w = (u^1-u^2)^T(\Sigma)^{-1} \quad ; \quad b=-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的 \\$</p><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><pre><code>基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候，</code></pre><p>都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss</p><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><pre><code>基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其</code></pre><p>效果一般比生成模型好(基于事实 not 基于猜测)</p><h3 id="小扩展"><a href="#小扩展" class="headerlink" title="小扩展"></a>小扩展</h3><h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><pre><code>基于先验概率得出的每个类别的后验概率为softmax函数，即：</code></pre><p>$\\$<br>$\qquad \qquad \qquad \qquad \, P(C_i|x) = \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad = \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}\\$</p><h4 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h4><p>未完待续</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/07/logistic/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/07/logistic/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic回归分析&quot;&gt;&lt;a href=&quot;#Logistic回归分析&quot; class=&quot;headerlink&quot; title=&quot;Logistic回归分析&quot;&gt;&lt;/a&gt;Logistic回归分析&lt;/h3&gt;&lt;p&gt;$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$&lt;/p&gt;
&lt;p&gt;$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计学习方法" scheme="http://www.enjoyai.site/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
</feed>
