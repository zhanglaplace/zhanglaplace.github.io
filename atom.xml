<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZhangFeng&#39;s Blog</title>
  
  <subtitle>HuaZhong University Of Science And Technology</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.enjoyai.site/"/>
  <updated>2017-10-21T15:18:48.820Z</updated>
  <id>http://www.enjoyai.site/</id>
  
  <author>
    <name>ZhangLaplace</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Caffe Loss分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe Loss分析/</id>
    <published>2017-10-20T03:16:01.000Z</published>
    <updated>2017-10-21T15:18:48.820Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-Loss"><a href="#Caffe-Loss" class="headerlink" title="Caffe_Loss"></a>Caffe_Loss</h3><p>  损失函数为深度学习中重要的一个组成部分，各种优化算法均是基于Loss来的，损失函数的设计好坏很大程度下能够影响最终网络学习的好坏。派生于 $LossLayer$,根据不同的Loss层有不同的参数;</p><h4 id="1-基本函数"><a href="#1-基本函数" class="headerlink" title="1.基本函数"></a>1.基本函数</h4><pre><code>主要包含构造函数，前向、后向以及Reshape，部分有SetUp的函数，每层都有Loss参数</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">explicit XXXLossLayer(const LayerParameter&amp; param):</div><div class="line">LossLayer&lt;Dtype&gt;(param),diff_() &#123;&#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure><a id="more"></a><h4 id="2-常用损失函数"><a href="#2-常用损失函数" class="headerlink" title="2.常用损失函数"></a>2.常用损失函数</h4><pre><code>由于训练中，采用mini_batch的模式</code></pre><h5 id="1-EuclideanLoss-欧式损失函数，L2损失"><a href="#1-EuclideanLoss-欧式损失函数，L2损失" class="headerlink" title="(1) EuclideanLoss (欧式损失函数，L2损失)"></a>(1) EuclideanLoss (欧式损失函数，L2损失)</h5><p>  $EuclideanLoss$的公式表达为 $loss = \frac{1}{2n}\sum_{i=1}^n{(y_{i}-\hat{y}_{i})^2}$<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">//reshape函数，完成层次的reshape,diff_与输入的N*C维度相同</span></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">     <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">     LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//先调用基类的Reshape函数</span></div><div class="line">     CHECK_EQ(bottom[<span class="number">0</span>]-&gt;count(<span class="number">1</span>),bottom[<span class="number">1</span>]-&gt;count(<span class="number">1</span>));<span class="comment">//label类别</span></div><div class="line">     diff_.Reshape(*bottom[<span class="number">0</span>]);<span class="comment">//一般是N*C*1*1</span></div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">// Forward_cpu 前向 主要计算loss</span></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">       <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    caffe_sub(count,</div><div class="line">              bottom[<span class="number">0</span>]-&gt;cpu_data(),<span class="comment">//网络的输出 N*C</span></div><div class="line">              bottom[<span class="number">1</span>]-&gt;cpu_data(),<span class="comment">//对应label N*C</span></div><div class="line">              diff_.mutable_cpu_data()<span class="comment">//对应的loss差分</span></div><div class="line">          );<span class="comment">//完成 y_&#123;predicy&#125;-y_&#123;label&#125; //bottom[0]-bottom[1]</span></div><div class="line">    Dtype dot = caffe_cpu_dot(count,diff_.cpu_data(),diff_.cpu_data());</div><div class="line">    <span class="comment">//bottom[0]-&gt;num()== bottom[0].shape(0);</span></div><div class="line">    Dtype loss = dot/bottom[<span class="number">0</span>]-&gt;num()/Dtype(<span class="number">2</span>);<span class="comment">//loss/(2*n)</span></div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss;</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="comment">//Backward_cpu f'(x) = 1/n*(y_&#123;predict&#125;-y_&#123;label&#125;)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> EuclideanLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">   <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp;propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</div><div class="line">       <span class="keyword">if</span> (propagate_down[i]) &#123;<span class="comment">//需要backward</span></div><div class="line">           <span class="comment">//对应predict-label 如果label为bottom[0]就需要乘以-1</span></div><div class="line">           <span class="keyword">const</span> Dtype sign = (i==<span class="number">0</span>) ? <span class="number">1</span> : <span class="number">-1</span>;</div><div class="line">           <span class="comment">//top[0]-&gt;cpu_diff()返回float* length = 1;下式为loss/n;</span></div><div class="line">           <span class="keyword">const</span> Dtype alpha = sign*top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>]/bottom[<span class="number">0</span>]-&gt;num();</div><div class="line">           <span class="comment">//y = ax+by ;</span></div><div class="line">           caffe_cpu_axpby(bottom[<span class="number">0</span>]-&gt;count(),<span class="comment">//count</span></div><div class="line">                           alpha,<span class="comment">// loss/n</span></div><div class="line">                           diff_.cpu_data(),<span class="comment">//y_&#123;predict&#125;-y_&#123;label&#125;</span></div><div class="line">                           Dtype(<span class="number">0</span>),</div><div class="line">                           bottom[i]-&gt;mutable_cpu_diff()</div><div class="line">                       );<span class="comment">//1/n*loss*(y_&#123;predict&#125;-y_&#123;label&#125;)</span></div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line">   <span class="comment">//欧式损失函数形式简单，常用于做回归分析，做分类需要统一量纲。</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="2-SoftmaxWithLoss-Softmax损失函数"><a href="#2-SoftmaxWithLoss-Softmax损失函数" class="headerlink" title="(2)SoftmaxWithLoss Softmax损失函数"></a>(2)SoftmaxWithLoss Softmax损失函数</h5><p>$\qquad softmax函数将输出的各个类别的概率值进行归一化，生成各个类别的prob$<br>$\qquad 常用的分类损失函数，Softmax输出与Multinomial Logistic Loss的结合。公式如下:$<br>$$ y_i = softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}}$$<br>$$loss = -log(y_k) ,k为实际的样本label$$<br>$\qquad 损失函数的推导:\frac{\partial Loss}{\partial x_i}=\sum_{j=1}^{n}{\frac{\partial loss}{\partial y_j}*\frac{\partial y_j}{\partial x_i}}=-\frac{1}{y_k}*\frac{\partial y_k}{\partial x_i} \quad k为实际的label,其他的\frac{\partial loss}{\partial y_j} =0 \\$<br>$$<br>\qquad \frac{\partial y_k}{\partial x_i} = \frac{\partial softmax(x_k)}{\partial x_i}=<br>\begin{cases}<br>  y_k*(1-y_k) \qquad k == i \\\<br>\\<br> -y_k*y_i \qquad \qquad k \,\,!=\,i<br>\end{cases}<br>$$<br>$$<br>整理后可以发现\frac{\partial loss}{\partial x_i}=<br>\begin{cases}<br>  y_k-1 \qquad k \,== \,i ，即i为实际label\\\<br>\\<br>  y_i \qquad \qquad k \,\,!=\,i,即i不是实际label<br>\end{cases}<br>$$<br>    具体代码的实现如下所示:<br>1.SoftmaxWithLossLayer的输入:bottom<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// bottom[0]为前层的特征输出，一般维度为N*C*1*1</span></div><div class="line"><span class="comment">// bottom[1]为来自data层的样本标签，一般维度为N*1*1*1;</span></div><div class="line"><span class="comment">// 申明</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom;</div><div class="line"><span class="comment">//backward部分代码</span></div><div class="line">Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line"><span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">caffe_copy(prob_.count(), prob_data, bottom_diff);</div><div class="line"><span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//label</span></div></pre></td></tr></table></figure></p><p>2.SoftmaxWithLossLayer层的输出:top<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// SoftmaxWithLossLayer的输出其实就是1*1*1*1的最终loss</span></div><div class="line"><span class="comment">// 如果有多个的话实际就是也会保存softmax的输出，但是需要注意的是内部包含了</span></div><div class="line"><span class="comment">//Softmax的FORWAR过程，产生的概率值保存在prob_内</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top;</div><div class="line"><span class="comment">//forward部分代码 ,</span></div><div class="line">top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / get_normalizer(normalization_, count);</div><div class="line"><span class="keyword">if</span> (top.size() == <span class="number">2</span>) &#123;</div><div class="line">    top[<span class="number">1</span>]-&gt;ShareData(prob_);<span class="comment">//top[1]保存softmax的前向概率</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>3.SoftmaxWithLossLayer的关键变量: $softmax_top_vec_,prob_$ 记录中间值<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;</div><div class="line"><span class="comment">/// prob stores the output probability predictions from the SoftmaxLayer.</span></div><div class="line">Blob&lt;Dtype&gt; prob_;</div><div class="line"><span class="comment">/// bottom vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;</div><div class="line"><span class="comment">/// top vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;</div><div class="line"><span class="comment">/// Whether to ignore instances with a certain label.</span></div><div class="line"><span class="keyword">bool</span> has_ignore_label_;</div><div class="line"><span class="comment">/// The label indicating that an instance should be ignored.</span></div><div class="line"><span class="keyword">int</span> ignore_label_;</div><div class="line"><span class="comment">/// How to normalize the output loss.</span></div><div class="line">LossParameter_NormalizationMode normalization_;</div><div class="line"></div><div class="line"><span class="keyword">int</span> softmax_axis_, outer_num_, inner_num_;<span class="comment">//softmax的输出与Loss的维度</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//先调用基类的reshape</span></div><div class="line">    softmax_layer_-&gt;Reshape(softmax_bottom_vec,softmax_top_vec_);</div><div class="line">    <span class="keyword">int</span> axis = <span class="keyword">this</span>-&gt;layer_param_.softmax_param().axis();<span class="comment">//softmaxproto参数(1)</span></div><div class="line">    softmax_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(axis);<span class="comment">//正不变负倒数</span></div><div class="line">    outer_num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>,softmax_axis_);<span class="comment">// N mini_batch_size</span></div><div class="line">    inner_num_ = bottom[<span class="number">0</span>]-&gt;count(softmax_axis_+<span class="number">1</span>);<span class="comment">// H*W 一般为1*1</span></div><div class="line">    <span class="comment">//保证outer_num_*inner_num_ = bottom[1]-&gt;count();//bottom[1]为label N</span></div><div class="line">    <span class="keyword">if</span> (top.size() &gt;= <span class="number">2</span>) &#123;<span class="comment">//多个top实际上是并列的，prob_值完全一致</span></div><div class="line">        top[<span class="number">1</span>]-&gt;Reshapelike(*bottom[<span class="number">0</span>]);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//forward是一个计算loss的过程，loss为-log(p_label)</span></div><div class="line"><span class="comment">//由于softmaxWithLoss包含了Softmax所以需要经过Softmax的前向，并得到每个类别概率值</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="comment">//调用Softmax的前向</span></div><div class="line">    softmax_layer_-&gt;Forward(softmax_bottom_vec_,softmax_top_vec_);</div><div class="line">    <span class="comment">//这里等同于softmax_top_vec_[0]-&gt;cpu_data();</span></div><div class="line">    <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//label 一般来自Data层</span></div><div class="line">    <span class="comment">// 一般是N*C(n个样本，每个C个预测概率)/ N == 类别数目</span></div><div class="line">    <span class="keyword">int</span> dim = prob_.count()/out_num_;</div><div class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;<span class="comment">//统计实际参与loss的样本个数</span></div><div class="line">    Dtype loss = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; outer_num_; i++) &#123;<span class="comment">//每个样本遍历</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123; <span class="comment">//可以认为j == 0 绝大多数成立</span></div><div class="line">            <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i*inner_num_+j]);</div><div class="line">            <span class="keyword">if</span>(has_ignore_label_ &amp;&amp; label_value == ignore_label_)&#123;</div><div class="line">                <span class="comment">// softmaxLayer的参数，可以选择不参与loss的类别</span></div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;<span class="comment">//实际需要判断label_value &gt; 0 ,&lt; prob_.shape(1)</span></div><div class="line">                <span class="comment">// -= 因为loss = -log(p_label),prob_data 是n*c的</span></div><div class="line">                loss -= <span class="built_in">log</span>(<span class="built_in">std</span>::max(prob_data[i*dim+label_value*inner_num_+j)],</div><div class="line">                                Dtype(FLT_MIN)));<span class="comment">//防止溢出或prob出现NAN</span></div><div class="line">                ++count;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//全部样本遍历完成后，可以进行归一，其实也挺简单，</span></div><div class="line">    <span class="comment">// top[0]-&gt;mutable_cpu_data[0] = loss/归一化</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Backward_cpu,这里的Backward实际需要更新的是softmax的输入接口的数据，</span></div><div class="line"><span class="comment">// 中间有个y的转化，具体公式上面已经写出</span></div><div class="line"><span class="comment">// bottom_diff = top_diff * softmaxWithloss' = top_diff * &#123;p -1 或者 p&#125;</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="comment">//fc输出与label的位置固定了，因此不需要如同欧式loss去判断label和fc的输入位置</span></div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</div><div class="line">        <span class="comment">//label不需要backpropagate</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;<span class="comment">//输入，需要更新</span></div><div class="line">        Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();<span class="comment">//需要修改的</span></div><div class="line">        <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();<span class="comment">//N*C</span></div><div class="line">        <span class="comment">//这里把diff先确定为softmax输出的y值，即bottom_diff[t] = y_t ;</span></div><div class="line">        caffe_copy(prob_.count(),prob_data,bottom_diff);</div><div class="line">        <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">        <span class="comment">// 也可以替换为bottom[1]-&gt;count(),实际就是类别C</span></div><div class="line">        <span class="keyword">int</span> dim = prob_.count()/ outer_num_;<span class="comment">//NC/C == N</span></div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; outer_num_; i++) &#123; <span class="comment">//n个样本</span></div><div class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123; <span class="comment">// 实际j == 0</span></div><div class="line">                <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i*inner_num_+j]);</div><div class="line">                <span class="keyword">if</span> (has_ignore_label_ &amp;&amp; label_value == ignore_label_) &#123;</div><div class="line">                    <span class="comment">//正好是忽略loss的类别</span></div><div class="line">                    bottom_diff[i*dim+label_vale*inner_num_+j] = <span class="number">0</span>;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span>&#123;</div><div class="line">                    <span class="comment">//这里需要考虑为什么，实际上之前所有的diff初始为y_t，</span></div><div class="line">                    <span class="comment">//根据softmax的偏导知道真实label是y_t -1;</span></div><div class="line">                    bottom_diff[i*dim+label_vale*inner_num_+j] -= <span class="number">1</span>;</div><div class="line">                    ++count;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//这里只完成了loss的一部分，还差top_diff即Loss</span></div><div class="line">        <span class="comment">//如果归一化，就进行归一，同cpu_forward</span></div><div class="line">        <span class="comment">//cpu_diff可以认为是Loss</span></div><div class="line">        <span class="comment">// Dtype loss_weight = top[0]-&gt;cpu_diff()[0]/归一化</span></div><div class="line">        caffe_scal(prob_count(),loss_weight,bottom_diff);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="3-SmoothL1Loss-RCNN后提出的Loss"><a href="#3-SmoothL1Loss-RCNN后提出的Loss" class="headerlink" title="(3) SmoothL1Loss (RCNN后提出的Loss)"></a>(3) SmoothL1Loss (RCNN后提出的Loss)</h5><pre><code>SmoothL1Loss为欧式均方误差的修改版，为分段函数，对离散点不敏感,具体的公式如下:</code></pre><p>$$<br>SmoothL1Loss(x) =<br>\begin{cases}<br>  0.5*(sigma*x)^2 \qquad 其他<br>\\<br>  \left|x\right|-0.5/sigma^2 \qquad \left|x\right| &lt; 1./sigma^2<br>\end{cases}<br>$$<br>整体的公式为:$x_{new} = x_{input}*w_{in},output = w_{out}*SmoothL1loss(x_{new});$<br>1.基本的数据类型和意义:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Blob&lt;Dtype&gt; diff_;<span class="comment">// y_</span></div><div class="line">Blob&lt;Dtype&gt; error_;<span class="comment">//loss</span></div><div class="line">Blob&lt;Dtype&gt; ones_;</div><div class="line"><span class="keyword">bool</span> has_weights_; <span class="comment">// weight权值</span></div><div class="line">Dtype sigma2_ ;<span class="comment">// sigma 默认为1，此处sigma2_ = sigma*simga;</span></div></pre></td></tr></table></figure></p><p>2.基本的功能函数<br>    基本包含了LayerSetup Reshape Forward 和 Backward四个函数,具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//构建layer层次,SmoothL1LossLayer的参数有sigma，默认为1</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::LayerSetup(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    SmoothL1LossParameter loss_param = <span class="keyword">this</span>-&gt;layer_param_.smooth_l1_loss_param();</div><div class="line">    sigma2_ = loss_param.sigma()*loss_param.sigma();</div><div class="line">    has_weights_ = (bottom.size() &gt;= <span class="number">3</span>);<span class="comment">//bottom[3]---为weights</span></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">        <span class="comment">//bottom[3] == out_weight;//w_out</span></div><div class="line">        <span class="comment">//bottom[2] == in_weight;// w_in</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Reshape 根据输入输出调节结构，计算过程进行了拆分</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;</div><div class="line">    bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//基函数</span></div><div class="line">    <span class="comment">//这里判断参数维度,</span></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">        CHECK_EQ(bottom[<span class="number">0</span>]-&gt;count(<span class="number">1</span>) == bottom[<span class="number">2</span>]-&gt;count(<span class="number">1</span>) ==</div><div class="line">        bottom[<span class="number">3</span>].count(<span class="number">1</span>))  ;<span class="comment">//w_in和w_out的权值</span></div><div class="line">    &#125;</div><div class="line">    diff_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// diff_ = w_in*(bottom[0]-bottom[1]);</span></div><div class="line">    error_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// error_ = w_out*smoothL1(w_in*diff_);</span></div><div class="line">    ones_.Reshape(bottom[<span class="number">0</span>].shape());<span class="comment">// one_ = error_*w_out;</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; ones_-&gt;count(); i++) &#123;</div><div class="line">        one_s.mutable_cpu_data()[i] = Dtype(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Forward过程，一步一步操作</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="comment">//bottom[0]和bottom[1]不确定标签和特征的顺序</span></div><div class="line">    caffe_gpu_sub( <span class="comment">// 计算diff_ = bottom[0]-bottom[1];</span></div><div class="line">        count,</div><div class="line">        bottom[<span class="number">0</span>]-&gt;gpu_data(),</div><div class="line">        bottom[<span class="number">1</span>]-&gt;gpu_data(),</div><div class="line">        diff_.mutable_cpu_data()</div><div class="line">    );</div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123; x_new = x_input*in_weight,xinput==diff_</div><div class="line">        caffp_gpu_mul(</div><div class="line">            count,</div><div class="line">            bottom[<span class="number">2</span>]-&gt;gpu_data(),</div><div class="line">            diff_.gpu_data(),</div><div class="line">            diff_.mutable_gpu_data();</div><div class="line">        );</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//此处为SmoothL1的函数前向过程GPU实现</span></div><div class="line">    SmoothL1Forward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count),</div><div class="line">    CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</div><div class="line">        count, diff_.gpu_data(), errors_.mutable_gpu_data(), sigma2_);</div><div class="line">    CUDA_POST_KERNEL_CHECK;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (has_weights_) &#123; <span class="comment">//x_out= SmoothL1(w_in*x_input) * w_out</span></div><div class="line">        caffe_gpu_mul(</div><div class="line">            count,</div><div class="line">            bottom[<span class="number">3</span>]-&gt;gpu_data(),</div><div class="line">            error_.gpu_data(),</div><div class="line">            error_.mutable_gpu_data();</div><div class="line">        ); <span class="comment">// error _ = w_out* error_</span></div><div class="line">    &#125;</div><div class="line">    Dtype loss;</div><div class="line">    caffe_gpu_dot(count,ones_.gpu_data().error_gpu_data(),&amp;loss);<span class="comment">//类似于asum</span></div><div class="line">    top[<span class="number">0</span>]-&gt;mutable_gpu_data()[<span class="number">0</span>] = loss/bottom[<span class="number">0</span>]-&gt;num();<span class="comment">// mini_batch</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// GPU的实现SmoothL1loss,根据公式实现即可</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SmoothL1Forward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></div><div class="line"><span class="function"><span class="params">Dtype sigma2)</span> </span>&#123;</div><div class="line"><span class="comment">// f(x) = 0.5 * (sigma * x)^2          if |x| &lt; 1 / sigma / sigma</span></div><div class="line"><span class="comment">//        |x| - 0.5 / sigma / sigma    otherwise</span></div><div class="line">    CUDA_KERNEL_LOOP(index, n) &#123; <span class="comment">//for loop</span></div><div class="line">        Dtype val = in[index];</div><div class="line">        Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">        <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</div><div class="line">            out[index] = <span class="number">0.5</span> * val * val * sigma2;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            out[index] = abs_val - <span class="number">0.5</span> / sigma2;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><pre><code>反向过程中根据求导公式可以得到如下式子，Backward的过程也如下所示</code></pre><p>$$\frac{\partial Loss}{\partial x} = w_{in}*w_{out}*\frac{\partial SmoothL1(x)}{\partial x}$$</p><p>//cpu版本可以自己实现，只需要把GPU_data_diff换成cpu,<br>//以及gpu的smoothL1写成CPU的即可。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">    <span class="comment">//backward过程，根据导函数</span></div><div class="line">    <span class="comment">// f'()</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Backward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">        <span class="keyword">int</span> count = diff_.count();</div><div class="line"></div><div class="line">        <span class="comment">// 反向即公式的smoothL1的偏导</span></div><div class="line">        SmoothL1Backward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count),</div><div class="line">          CAFFE_CUDA_NUM_THREADS &gt;&gt;&gt;(</div><div class="line">            count, diff_.gpu_data(), diff_.mutable_gpu_data(), sigma2_);</div><div class="line">        CUDA_POST_KERNEL_CHECK;</div><div class="line"></div><div class="line">        <span class="comment">//此处的循环loop如同欧式损失函数，因为无法确认bottom[0]和bottom[1]，fc和label</span></div><div class="line">        <span class="comment">//的顺序，forward默认是0-1，因此如果0为label，则sign = -1;</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</div><div class="line">            <span class="keyword">if</span> (propagate_down[i]) &#123;</div><div class="line">                <span class="keyword">const</span> Dtype sign = (i == <span class="number">0</span>) ? <span class="number">1</span>:<span class="number">-1</span>;<span class="comment">//代码默许了label为bottom[1]</span></div><div class="line">                <span class="comment">//sign* loss/n;</span></div><div class="line">                <span class="keyword">const</span> Dtype alpha = sign*top_diff-&gt;gpu_diff()[<span class="number">0</span>]/bottom[i]-&gt;num();</div><div class="line">                <span class="comment">//smoothL1输入的是diff_.gpu_data()</span></div><div class="line">                caffe_cpu_axpby(</div><div class="line">                    count,</div><div class="line">                    alpha,</div><div class="line">                    diff_.gpu_data(),<span class="comment">//此处的data已经是SmoothL1返回的导数了</span></div><div class="line">                    Dtype(<span class="number">0</span>),</div><div class="line">                    bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                );</div><div class="line">                <span class="keyword">if</span> (has_weights_) &#123;</div><div class="line">                    caffe_gpu_mul(</div><div class="line">                        count,</div><div class="line">                        bottom[<span class="number">2</span>]-&gt;gpu_data(),</div><div class="line">                        bottom[i]-&gt;gpu_diff(),</div><div class="line">                        bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                    ); 乘以了内层的weight</div><div class="line">                    caffe_gpu_mul(</div><div class="line">                        count,</div><div class="line">                        bottom[<span class="number">3</span>]-&gt;gpu_data(),</div><div class="line">                        bottom[i]-&gt;gpu_diff(),</div><div class="line">                        bottom[i]-&gt;mutable_gpu_diff()</div><div class="line">                    ); 乘以了外层的weight</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SmoothL1Backward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></div><div class="line"><span class="function"><span class="params">    Dtype sigma2)</span> </span>&#123;</div><div class="line">  <span class="comment">// f'(x) = sigma * sigma * x         if |x| &lt; 1 / sigma / sigma</span></div><div class="line">  <span class="comment">//       = sign(x)                   otherwise</span></div><div class="line">        CUDA_KERNEL_LOOP(index, n) &#123;</div><div class="line">            Dtype val = in[index];</div><div class="line">            Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">            <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</div><div class="line">              out[index] = sigma2 * val;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">              out[index] = (Dtype(<span class="number">0</span>) &lt; val) - (val &lt; Dtype(<span class="number">0</span>));<span class="comment">//1或者-1</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p><pre><code>cpu版本的SmoothL1前向和后向实现如下,cpu版本速度过慢，不建议使用</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">    <span class="comment">//前向 替换前向GPU中一部分</span></div><div class="line">    <span class="keyword">const</span> Dtype* in = diff_.cpu_data();</div><div class="line">    Dtype* out = errors_.mutable_cpu_data();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; diff_.count(); i++) &#123;</div><div class="line">       Dtype val = in[index];</div><div class="line">       Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">       <span class="keyword">if</span>(abs_val &lt; <span class="number">1.0</span> / sigma2_)&#123;</div><div class="line">           out[index] = <span class="number">0.5</span> * val * val * sigma2_;</div><div class="line">       &#125;</div><div class="line">       <span class="keyword">else</span>&#123;</div><div class="line">           out[index] = abs_val - <span class="number">0.5</span> / sigma2_;</div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">//反向，替换反向GPU的一部分</span></div><div class="line">   <span class="keyword">const</span> Dtype* in = diff_.cpu_data();</div><div class="line">   Dtype* out = diff_.mutable_cpu_data();</div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; diff_.count(); i++) &#123;</div><div class="line">      Dtype val = in[index];</div><div class="line">      Dtype abs_val = <span class="built_in">abs</span>(val);</div><div class="line">      <span class="keyword">if</span>(abs_val &lt; <span class="number">1.0</span> / sigma2_)&#123;</div><div class="line">          out[index] = sigma2_ *  val;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span>&#123;</div><div class="line">          out[index] = (Dtype(<span class="number">0</span>) &lt; val) - (val &lt; Dtype(<span class="number">0</span>));</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line"><span class="comment">// smoothL1在目标检测的时候效果良好，由于多损失函数以及回归点的变换，bottom[2]和</span></div><div class="line"><span class="comment">// bottom[3]基本都存在，由于其函数特性，对偏远的点不敏感，因此可以替换L2loss</span></div></pre></td></tr></table></figure><h5 id="4-SigmoidCrossEntropyLoss-交叉熵"><a href="#4-SigmoidCrossEntropyLoss-交叉熵" class="headerlink" title="(4) SigmoidCrossEntropyLoss (交叉熵)"></a>(4) SigmoidCrossEntropyLoss (交叉熵)</h5><p>  交叉熵应用广泛，常作为二分类的损失函数，在$logistic$中使用，由于$sigmoid$的函数的输出特性，能够很好的以输出值代表类别概率。具体的公式如下所示:</p><p>  $$loss =  -\frac{1}{n}\sum_{1}^{n}(\hat{p_i}*log(p_i)+(1-\hat{p_i})*log(1-p_i)))$$<br>$$p_i = \frac{1}{1.+exp(-x_i)}$$<br>$$ \frac{\partial loss}{\partial x_i} = \frac{1}{n}*\sum_{i=1}^{n}((\hat{p_i}*\frac{1}{p_i}*p_i*(1-p_i)-(1-\hat{p_i})*\frac{1}{1-p_i}*(1-p_i)*p_i)) $$<br>$$= \frac{1}{n}\sum_{i=1}^{n}(\hat{p_i}-p_i)$$<br>1.基本的数据成员<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">shared_ptr</span>&lt;SigmoidLayer&lt;Dtype&gt;&gt;sigmoid_layer_;<span class="comment">//layer参数</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; sigmoid_output_; <span class="comment">// sigmoid输出的值N*C C一般==2</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_bottom_vec_;<span class="comment">// sigmoid函数的输入x</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_top_vec_;<span class="comment">// sigmoid函数的输出</span></div></pre></td></tr></table></figure></p><p>2.基本的成员函数<br>    基本的成员函数为LayerSetup，Reshape ,Forward和Backward,实现如下:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//构建layer 中间有sigmoid函数过度，所以如同softmaxLoss类似过程</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::LayerSetup(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::LayerSetup(bottom,top);</div><div class="line">    sigmoid_bottom_vec_.clear();</div><div class="line">    sigmoid_bottom_vec_.push_back(bottom[<span class="number">0</span>]);</div><div class="line">    sigmoid_top_vec_.clear();</div><div class="line">    sigmoid_top_vec_.push_back(sigmoid_output_.get());<span class="comment">//sigmoid的输出</span></div><div class="line">    sigmoid_layer_-&gt;Setup(sigmoid_bottom_vec_,sigmoid_top_vec_);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Reshape函数 比较简单</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Reshape(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    LossLayer&lt;Dtype&gt;::Reshape(bottom,top);<span class="comment">//步骤1</span></div><div class="line">    sigmoid_layer_-&gt;Reshape(sigmoid_bottom_vec_,sigmoid_top_vec_);<span class="comment">//步骤2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>这里Caffe实现的前向计算代码与公式有差异，具体原因如下<br>$\qquad  \hat{p}*log(p)+(1-\hat{p})*log(1-p) \\<br>\qquad \,= \hat{p}*log(\frac{1}{1+exp(-x)})+(1-\hat{p})*log(\frac{exp(-x)}{1+exp(-x)}) \\<br>\qquad =\hat{p}*log(\frac{1}{1+exp(-x)})-\hat{p}*log(\frac{exp(-x)}{1+exp(-x)})+log(\frac{exp(-x)}{1+exp(-x)}) \\<br>\qquad =\hat{p}*x+log(\frac{exp(-x)}{1+exp(-x)})$</p><p>当$exp(-x)很大时, \frac{exp(-x)}{1+exp(-x)}$ 计算不准确，因此采用下种计算方式,当 $x&lt;0$时,分子分母同时乘以$exp(x)$,有:</p><p>$$<br>\frac{exp(-x)}{1+exp(-x)}=<br>\begin{cases}<br>  \frac{exp(-x)}{1+exp(-x)} \qquad x\ge0<br>\\<br>  \frac{1}{1+exp(x)} \qquad x&lt;0<br>\end{cases}<br>$$</p><p>从而得到:<br>$$<br>\hat{p}*x+log(\frac{exp(-x)}{1+exp(-x)})=<br>\begin{cases}<br> \hat{p}*x+log(\frac{exp(-x)}{1+exp(-x)}) = (p-1)<br>*x-log(1+exp(-x)) \quad x\ge0<br>\\<br> \hat{p}*x+log(\frac{exp(-x)}{1+exp(-x)})=p*x-log(1+exp(-x)) \quad \qquad x&lt;0<br>\end{cases}<br>$$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Forward_cpu 前向函数，分布保存临时值，得到loss</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Forward_cpu(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    sigmoid_bottom_vec_[<span class="number">0</span>] = bottom[<span class="number">0</span>];<span class="comment">//这一步多余，setup时已经保持一致了</span></div><div class="line">    sigmoid_layer_-&gt;Forward_cpu(sigmoid_bottom_vec_,sigmoid_top_vec_);<span class="comment">//Sigmoid</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;num();</div><div class="line">    <span class="keyword">const</span> Dtype* input_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();<span class="comment">//真实label</span></div><div class="line">    Dtype loss = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;<span class="comment">//遍历mini_batch</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20Loss%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a><br>版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-Loss&quot;&gt;&lt;a href=&quot;#Caffe-Loss&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Loss&quot;&gt;&lt;/a&gt;Caffe_Loss&lt;/h3&gt;&lt;p&gt;  损失函数为深度学习中重要的一个组成部分，各种优化算法均是基于Loss来的，损失函数的设计好坏很大程度下能够影响最终网络学习的好坏。派生于 $LossLayer$,根据不同的Loss层有不同的参数;&lt;/p&gt;
&lt;h4 id=&quot;1-基本函数&quot;&gt;&lt;a href=&quot;#1-基本函数&quot; class=&quot;headerlink&quot; title=&quot;1.基本函数&quot;&gt;&lt;/a&gt;1.基本函数&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;主要包含构造函数，前向、后向以及Reshape，部分有SetUp的函数，每层都有Loss参数
&lt;/code&gt;&lt;/pre&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;explicit XXXLossLayer(const LayerParameter&amp;amp; param):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;LossLayer&amp;lt;Dtype&amp;gt;(param),diff_() &amp;#123;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Reshape&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe 激励层(Activation)分析</title>
    <link href="http://www.enjoyai.site/2017/10/20/Caffe%20%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0(Activation)%E5%88%86%E6%9E%90/"/>
    <id>http://www.enjoyai.site/2017/10/20/Caffe 激励函数(Activation)分析/</id>
    <published>2017-10-20T03:16:01.000Z</published>
    <updated>2017-10-20T17:33:59.072Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Caffe-Activation"><a href="#Caffe-Activation" class="headerlink" title="Caffe_Activation"></a>Caffe_Activation</h3><p>  一般来说，激励层的输入输出尺寸一致，为非线性函数，完成非线性映射，从而能够拟合更为复杂的函数表达式激励层都派生于NeuronLayer: class XXXlayer : public NeuronLayer<dtype></dtype></p><h4 id="1-基本函数"><a href="#1-基本函数" class="headerlink" title="1.基本函数"></a>1.基本函数</h4><p>  激励层的基本函数较为简单，主要包含构造函数和前向、后向函数<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">XXXLayer</span><span class="params">(<span class="keyword">const</span> LayerParameter&amp; param)</span></span></div><div class="line">        :NeuronLayer&lt;Dtype&gt;(param)&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">"layerNane"</span>; &#125;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure></p><a id="more"></a><h4 id="2-常用激励函数"><a href="#2-常用激励函数" class="headerlink" title="2.常用激励函数"></a>2.常用激励函数</h4><h5 id="1-Relu-PRelu-Rectufied-Linear-Units"><a href="#1-Relu-PRelu-Rectufied-Linear-Units" class="headerlink" title="(1) Relu/PRelu Rectufied Linear Units"></a>(1) Relu/PRelu Rectufied Linear Units</h5><p>   ReLU的函数表达式为 $f(x) = x*(x&gt;0) + negative_slope<em>\x\</em>(x &lt;= 0)$ 具体实现如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">//forward_cpu</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; <span class="comment">// 根据bottom求解top</span></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();<span class="comment">//const 不可修饰</span></div><div class="line">      Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();<span class="comment">//可修饰</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//因为count_一致，也可用top</span></div><div class="line">      Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         top_data[i] = bottom_data[i]*(bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">                    + negative_slope*bottom_data[i]*(bottom_data[i] &lt;= <span class="number">0</span>);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="comment">//Backward_cpu</span></div><div class="line">  <span class="comment">// 导数形式 f'(x) = 1 x&gt;0 ; negative_slope*x x&lt;0</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">      <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>].cpu_diff();<span class="comment">//top diff</span></div><div class="line">      <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>].cpu_data();<span class="comment">//用以判断x是否大于0</span></div><div class="line">      Dtype* bottom_diff = bottom[<span class="number">0</span>].cpu_diff();<span class="comment">//bottom diff</span></div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>].count();</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         bottom_diff[i] = top_diff[i]*(bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">                    +negative_slope*(bottom_data[i] &lt;= <span class="number">0</span>);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">// Relu 函数形式简单，导函数简单，能有效的解决梯度弥散问题，但是当x小于0时，易碎</span></div><div class="line"><span class="comment">// 但是网络多为多神经元，所以实际应用中不会影响到网络的正常训练。</span></div></pre></td></tr></table></figure></p><h5 id="2-Sigmoid-S曲线"><a href="#2-Sigmoid-S曲线" class="headerlink" title="(2) Sigmoid (S曲线)"></a>(2) Sigmoid (S曲线)</h5><p>   Sigmoid函数表达式为$f(x) = 1./(1+exp(-x))$;值域0-1，常作为BP神经网络的激活函数<br>由于输出为0-1，也作为logistic回归分析的概率输出函数。具体实现如下;<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">    <span class="comment">//定义一个sigmoid函数方便计算</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">sigmoid</span><span class="params">(Dtype x)</span></span>&#123;</div><div class="line">       <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1.</span>+<span class="built_in">exp</span>(-x));</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//前向 直接带入sigmoid函数即可</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">        <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">        <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">        Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();<span class="comment">//需要计算</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();<span class="comment">//N*C*H*W;</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">           top_data[i] = sigmoid(bottom_data[i]);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//Backward_cpu 由于f'(x) = f(x)*(1-f(x))，所以需要top_data</span></div><div class="line">    <span class="comment">// bottom_diff = top_diff*f'(bottom_data) = top_diff*top_data*(1-top_data)</span></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">    <span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">        <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">        <span class="keyword">const</span> Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">        Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff(); <span class="comment">//需要计算</span></div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">            <span class="comment">//top_data[i] == sigmoid(bottom_data[i]);</span></div><div class="line">            bottom_diff[i] = top_diff[i]*top_data[i]*(<span class="number">1.</span>-top_data[i]);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="comment">// Sigmoid函数可以作为二分类的概率输出，也可以作为激活函数完成非线性映射，但是网络</span></div><div class="line"><span class="comment">// 增加时，容易出现梯度弥散问题，目前在CNN中基本不使用</span></div></pre></td></tr></table></figure></p><h5 id="3-TanH-双正切函数"><a href="#3-TanH-双正切函数" class="headerlink" title="(3)TanH,双正切函数"></a>(3)TanH,双正切函数</h5><p>  TanH函数的表达式为 $f(x) =\frac{(1.-exp(-2x))}{(1.+exp(-2x))}$;值域0-1,与sigmoid函数有相同的问题,<br>但是TanH在RNN中使用较为广泛,<a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">理由参考</a>，具体实现如下所示。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//定义一个tanH的函数表达式,实际已经封装</span></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">TanH</span><span class="params">(Dtype x)</span></span>&#123;</div><div class="line">   <span class="keyword">return</span> (<span class="number">1.</span>-<span class="built_in">exp</span>(<span class="number">-2</span>*x))/(<span class="number">1.</span>+<span class="built_in">exp</span>(<span class="number">-2</span>*x));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Forward_cpu</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> TanHLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">        top[i] = TanH(bottom_data[i]);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Backward_cpu f'(x) = 1-f(x)*f(x);</span></div><div class="line"><span class="comment">// bottom_diff = top_diff(1-top_data*top_data);</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> TanHLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123;</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff(); <span class="comment">//需要计算</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">        <span class="comment">//top_data[i] == TanH(bottom_data[i]);</span></div><div class="line">        bottom_diff[i] = top_diff[i]*(<span class="number">1.</span>-top_data[i]*top_data[i]);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><pre><code>其他的激励函数就不在枚举，可以查看具体的caffe源码，实现大致相同</code></pre><h3 id="3-说明"><a href="#3-说明" class="headerlink" title="3.说明"></a>3.说明</h3><h4 id="1-梯度弥散和梯度爆炸"><a href="#1-梯度弥散和梯度爆炸" class="headerlink" title="(1) 梯度弥散和梯度爆炸"></a>(1) 梯度弥散和梯度爆炸</h4><p>  网络方向传播时，loss经过激励函数会有$loss*\partial{f(x)}$,而如sigmoid的函数，<br>max($\partial{f(x)}$)只有1/4因此深层网络传播时loss越来越小，则出现前层网络未完整学习而后层网络学习饱和的现象</p><h4 id="2-Caffe激励层的构建"><a href="#2-Caffe激励层的构建" class="headerlink" title="(2) Caffe激励层的构建"></a>(2) Caffe激励层的构建</h4><p>  如上述的代码所示，激励层主要完成forward和Bacward的函数实现即可，由构建的函数表达式推导出它的导函数形式，弄懂bottom_data,top_data,bottom_diff,top_diff即可</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/10/20/Caffe%20%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0(Activation" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/20</a>%E5%88%86%E6%9E%90/)<br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Caffe-Activation&quot;&gt;&lt;a href=&quot;#Caffe-Activation&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Activation&quot;&gt;&lt;/a&gt;Caffe_Activation&lt;/h3&gt;&lt;p&gt;  一般来说，激励层的输入输出尺寸一致，为非线性函数，完成非线性映射，从而能够拟合更为复杂的函数表达式激励层都派生于NeuronLayer: class XXXlayer : public NeuronLayer&lt;dtype&gt;&lt;/dtype&gt;&lt;/p&gt;
&lt;h4 id=&quot;1-基本函数&quot;&gt;&lt;a href=&quot;#1-基本函数&quot; class=&quot;headerlink&quot; title=&quot;1.基本函数&quot;&gt;&lt;/a&gt;1.基本函数&lt;/h4&gt;&lt;p&gt;  激励层的基本函数较为简单，主要包含构造函数和前向、后向函数&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;explicit&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;XXXLayer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; LayerParameter&amp;amp; param)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        :NeuronLayer&amp;lt;Dtype&amp;gt;(param)&amp;#123;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt;* &lt;span class=&quot;title&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;/span&gt;&amp;#123; &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;layerNane&quot;&lt;/span&gt;; &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Forward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_cpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Backward_gpu&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;&amp;amp; propagate_down, &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom)&lt;/span&gt;&lt;/span&gt;;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Net分析</title>
    <link href="http://www.enjoyai.site/2017/10/19/Caffe_Net/"/>
    <id>http://www.enjoyai.site/2017/10/19/Caffe_Net/</id>
    <published>2017-10-19T06:42:00.000Z</published>
    <updated>2017-10-20T17:24:20.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-Net"><a href="#Caffe-Net" class="headerlink" title="Caffe_Net"></a>Caffe_Net</h1><h3 id="1-基本数据"><a href="#1-基本数据" class="headerlink" title="1.基本数据"></a>1.基本数据</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_; <span class="comment">// 记录每一层的layer参数</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; bottom_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; &gt; bottom_need_backward_;</div><div class="line"><span class="comment">/// top_vecs stores the vectors containing the output for each layer</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; top_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; param_id_vecs_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; layer_names_;</div><div class="line"><span class="comment">//learnable_params_[learnable_param_ids_[i]] == params_[i].get()</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;<span class="comment">//层间权重与bias</span></div></pre></td></tr></table></figure><a id="more"></a><h3 id="2-常用的函数"><a href="#2-常用的函数" class="headerlink" title="2. 常用的函数"></a>2. 常用的函数</h3><pre><code>介绍了Caffe内的Net的常用函数:</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"> <span class="function"><span class="keyword">const</span> <span class="built_in">string</span>&amp; <span class="title">name</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> name_;&#125;<span class="comment">//网络的名称</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; layer_names&#123;<span class="keyword">return</span> layer_names_;&#125;<span class="comment">// net每层的layer名称</span></div><div class="line"> <span class="comment">// net内每层的layer的Blob名称</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; blob_names()&#123;<span class="keyword">return</span> blob_names_;&#125;</div><div class="line"> <span class="comment">//net内层次间的权值与bias</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt;&amp; blobs()&#123;<span class="keyword">return</span> blob_;&#125;;</div><div class="line"> <span class="comment">//net内的layers</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt;&gt;&gt;&amp; layers()&#123;<span class="keyword">return</span> layers_;&#125;;</div><div class="line">  <span class="comment">//net-&gt;bottom_vecs() 返回该layer的输入，输出向量，</span></div><div class="line">  <span class="comment">//以及具体的 top_id_vecs[layer_id][top_id];</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; bottom_vecs()&#123; <span class="keyword">return</span> bottom_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; top_vecs() &#123; <span class="keyword">return</span> top_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;&amp; bottom_id_vecs()&#123; <span class="keyword">return</span> bottom_id_vecs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;&amp; top_id_vecs() &#123; <span class="keyword">return</span> top_id_vecs_;&#125;</div><div class="line"> <span class="function"><span class="keyword">void</span> <span class="title">CopyTrainedLayersFrom</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span> trained_filename)</span></span>;<span class="comment">//加载权重</span></div><div class="line"> <span class="comment">//网络的输入输出</span></div><div class="line"> <span class="comment">//感觉等效于bottom_vecs_[0]</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; input_blobs()&#123;<span class="keyword">return</span> net_input_blobs_;&#125;</div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; output_blobs()</div><div class="line"> &#123;<span class="keyword">return</span> net_output_blobs;&#125;<span class="comment">//top_vecs[top_vecs.size()-1];</span></div><div class="line"></div><div class="line"> <span class="function"><span class="keyword">const</span> <span class="keyword">int</span> <span class="title">num_input</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> net_input_blobs_.size()&#125;;<span class="comment">//输入blob的size</span></div><div class="line"> <span class="comment">//has_blob()然后find return</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;blob_by_name(<span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name);</div><div class="line"></div><div class="line"><span class="comment">// 前向计算loss和网络的输出</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; forward(Dtype* loss = <span class="literal">NULL</span>);</div><div class="line"><span class="comment">// --- *loss = ForwardFromTo(0.layers_.size()-1);</span></div><div class="line"><span class="comment">// --- 此处调用 Dtype* Net&lt;Dtype&gt;::ForwardFrom(int start,int end)</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = start; i &lt; end; i++)&#123;</div><div class="line">    <span class="comment">//多态，调用具体的Layer的Forward函数,并返回该层次的loss</span></div><div class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i],top_vecs_[i]);</div><div class="line">    loss += layer_loss;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> loss;</div><div class="line"></div><div class="line"><span class="comment">// backward反向，更新权值</span></div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Backward()&#123; <span class="comment">//</span></div><div class="line">    BackwardFromTo(layers_size()<span class="number">-1</span>,<span class="number">0</span>); <span class="comment">// 具体函数实现如第三部分</span></div><div class="line">    <span class="keyword">if</span> (debug_info_) &#123;</div><div class="line">        <span class="comment">/*层次的参数*/</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3-具体函数实现"><a href="#3-具体函数实现" class="headerlink" title="3.具体函数实现"></a>3.具体函数实现</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> Net&lt;Dtype&gt;::AppendBottom(<span class="keyword">const</span> NetParamter&amp; param, <span class="keyword">int</span> layer_id,</div><div class="line"><span class="keyword">int</span> bottom_id,<span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* availabel_blobs,<span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;* blob_name_to_idx)&#123;</div><div class="line">    <span class="keyword">const</span> LayerParammeter&amp; layer_param = param.layer(layer_id);</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = layer_param.bottom(bottom_id);</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> blob_id = (*blob_name_to_idx)[blob_name];</div><div class="line">    <span class="comment">//layer输入的shape等</span></div><div class="line">    bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());</div><div class="line">    bottom_id_vecs_[layer_id].push_back(blob_id);</div><div class="line">    <span class="comment">//LOG CONV&lt;--data 等,只要是丢入输入</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// learnable_params_</span></div><div class="line">  <span class="comment">//conv的shape一般为num_output*input_channels*kernel_width*kernel_height</span></div><div class="line">  <span class="comment">//bias的shape一般为Num_output</span></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Net&lt;Dtype&gt;::AppendParam(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</div><div class="line">                           <span class="keyword">const</span> <span class="keyword">int</span> param_id) &#123;</div><div class="line">       <span class="keyword">const</span> <span class="keyword">int</span> learnable_param_id = learnable_params_.size();</div><div class="line">       learnable_params_.push_back(params_[net_param_id].get());</div><div class="line">       learnable_param_ids_.push_back(learnable_param_id);</div><div class="line">       has_params_lr_.push_back(param_spec-&gt;has_lr_mult());</div><div class="line">       has_params_decay_.push_back(param_spec-&gt;has_decay_mult());</div><div class="line">       params_lr_.push_back(param_spec-&gt;lr_mult());</div><div class="line">       params_weight_decay_.push_back(param_spec-&gt;decay_mult());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFromTo(<span class="keyword">int</span> start,<span class="keyword">int</span> end)&#123;</div><div class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = start;i &gt;= end;--i)&#123;</div><div class="line">         <span class="comment">//backward 调用各层次的backward更新权值和bias</span></div><div class="line">         layers_[i].Backward(top_vecs_[i],bottom_need_backward_[i],</div><div class="line">                            bottom_vecs_[i]);</div><div class="line">      &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h3 id="4-基本流程"><a href="#4-基本流程" class="headerlink" title="4.基本流程"></a>4.基本流程</h3><p>  基本流程：Net构造函数开始</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 递归更新变量</span></div><div class="line">vectot&lt;<span class="built_in">string</span>&gt;*stage ;</div><div class="line"><span class="keyword">int</span> level;</div><div class="line"></div><div class="line"><span class="comment">//起始调用</span></div><div class="line">net_.reset(<span class="keyword">new</span> Net&lt;<span class="keyword">float</span>&gt;(model_file, TEST));</div><div class="line"></div><div class="line"><span class="comment">//送入prototxt文件和Test OR Train</span></div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Net</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; param_file,Phase phase,<span class="keyword">const</span> <span class="keyword">int</span> level = <span class="number">0</span>,</span></span></div><div class="line"><span class="function"><span class="params">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;* stage = <span class="literal">NULL</span>,<span class="keyword">const</span> Net* root_net = <span class="literal">NULL</span>)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">// 解析保存在NetParamter param内,这里用到了</span></div><div class="line"><span class="comment">//protobuf::TextFormat::Parse(FileInputStream*,param)</span></div><div class="line">ReadNetParamsFromTextFileOrDie(param_file,&amp;param);</div><div class="line"></div><div class="line"><span class="comment">// 读取了NetParamter 后需要进行整个网络的初始化工作</span></div><div class="line">Init(param); <span class="comment">//初始化网络的接口，下续为具体实现</span></div><div class="line">FilterNet(param, &amp;filtered_param);<span class="comment">// 打印网络结构</span></div><div class="line"></div><div class="line"><span class="comment">/*内部会完成split added 如果有必要(残差结构),记录层与层之间的联系关系与层次的名称</span></div><div class="line"><span class="comment">等，是否有loss_weight，layer的size等*/</span></div><div class="line">InsertSplits(filtered_param,&amp;param);</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; param.layer_size(); i++) &#123; <span class="comment">//遍历setupLayer</span></div><div class="line">   <span class="keyword">const</span> LayerParammeter&amp; layer_param = param.layer(i);<span class="comment">//层次的参数</span></div><div class="line">   layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</div><div class="line">   <span class="comment">// CreateLayer会走layer_factory的CreateLayer的注册 ,比如input,conv,bn...</span></div><div class="line">   layer_names_.push_back(layer_param.name());</div><div class="line"></div><div class="line">   <span class="comment">//开始继续遍历每层输入的具体的细节,第i个layer的第botom_id个输入</span></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size();</div><div class="line">   bottom_id++) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> blob_id =</div><div class="line">      AppendBottom(param,i,bottom_id,&amp;availabel_blobs,&amp;blob_name_to_idx);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">//开始继续遍历每层输出的具体细节，第i个layer的第 top_id的输出</span></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; layer_param.top_size();</div><div class="line">   top_id++) &#123;</div><div class="line">      AppendTop(param,i,top_id,&amp;availabel_blobs,&amp;blob_name_to_idx);</div><div class="line">       <span class="keyword">if</span> (layer_param.type()== <span class="string">"Input"</span>) &#123;<span class="comment">//输入</span></div><div class="line">         <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size() - <span class="number">1</span>;</div><div class="line">         net_input_blob_indices_.push_back(blob_id);</div><div class="line">         net_input_blobs_.push_back(blobs_[blob_id].get());</div><div class="line">       &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line"> <span class="comment">//多态，具体调用具体的layer的Setup函数</span></div><div class="line">  layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</div><div class="line"></div><div class="line">  <span class="comment">//每个输出遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size();</div><div class="line">  top_id++) &#123;</div><div class="line">     <span class="comment">/*完成层次的blob_loss_weights,并统计memory_used_*/</span>;</div><div class="line">     memory_used_ += top_vecs_[layer_id][top_id]-&gt;count();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//总的memory_used_: memory_used_*sizeof(Dtype);</span></div><div class="line"></div><div class="line">  <span class="comment">//如果层次间有学习权值和偏置，则需要再次设置，比如conv</span></div><div class="line">  <span class="comment">//num_param_blobs weights And bias</span></div><div class="line">  <span class="comment">// relu pooling等层无中间权值参数，则num_param_blobs = 0</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</div><div class="line">      AppendParam(param, layer_id, param_id);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*接下来需要研究网络的backwards问题，决定哪些层次对loss有贡献，并且检查哪些层次</span></div><div class="line"><span class="comment">不需要back_propagate_down操作，遍历是反向的操作一个layer是否需要回溯计算，主要</span></div><div class="line"><span class="comment">依据两个方面：(1)该layer的top blob 是否参与loss的计算；(2):该layer的bottom</span></div><div class="line"><span class="comment">blob 是否需要回溯计算，比如Data层一般就不需要backward computation */</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> layer_id = layers_.size()<span class="number">-1</span>; layer_id &gt;= <span class="number">0</span>; --layer_id)&#123;</div><div class="line">   <span class="keyword">bool</span> layer_contributes_loss = <span class="literal">false</span>;<span class="comment">//默认是无贡献的</span></div><div class="line">   <span class="keyword">bool</span> layer_skip_propagate_down = <span class="literal">true</span>;<span class="comment">// 默认不参与backwards的loss贡献</span></div><div class="line"></div><div class="line">  <span class="comment">//Layer内的输出遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size();</div><div class="line">  top_id++) &#123;</div><div class="line">      <span class="comment">//blob_name_[index]名字</span></div><div class="line">     <span class="built_in">string</span>&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];</div><div class="line">     <span class="keyword">if</span> (layer_[layer_id]-&gt;loss(top_id)||</div><div class="line">     blobs_under_loss.find(blob_name) != blobs_under_loss.end()) &#123;</div><div class="line">         <span class="comment">//该层次的layerloss不为0或者loss_weight = 1;</span></div><div class="line">        layer_contributes_loss = <span class="literal">true</span>;</div><div class="line">     &#125;</div><div class="line">     <span class="keyword">if</span> (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) &#123;</div><div class="line">        layer_skip_propagate_down = <span class="literal">false</span>;</div><div class="line">     &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//同理 Layer内的输入遍历</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> bottom_id = <span class="number">0</span>; bottom_id &lt; bottom_vecs_[layer_id].size();</div><div class="line">  bottom_id++) &#123;</div><div class="line">    <span class="keyword">if</span> (layer_contributes_loss) &#123;</div><div class="line">      <span class="built_in">string</span>* blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">      blobs_under_loss.insert(blob_name);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span>&#123;</div><div class="line">      bottom_need_backward_[layer_id][bottom_id] = <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!bottom_need_backward_[layer_id][bottom_id]) &#123;</div><div class="line">      <span class="built_in">string</span>&amp;blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">      blok_skip_backp.insert(blob_name);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/*code*/</span></div><div class="line"></div><div class="line">&#125;<span class="comment">//init函数尾</span></div></pre></td></tr></table></figure><h3 id="5-说明"><a href="#5-说明" class="headerlink" title="5.说明"></a>5.说明</h3><p>   blob_name_to_idx是一个局部变量，其实它是在当前layer的top blob 和下一层的bottom blob间起着一个桥梁作用。 blob_name_to_idx中元素的pair是从网络最开始一层一层搭建的过程中压入map的，其中的name和id都是不重复的。name是关键字，不重复是map数据结构的必然要求，id也是不重复的，—0,1,2…blob_name_to_idx和blobs_一样，在”Normal output”的情形下，每次遍历到一个top blob的时候都会更新。</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/19/Caffe_Net/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/19/Caffe_Net/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-Net&quot;&gt;&lt;a href=&quot;#Caffe-Net&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Net&quot;&gt;&lt;/a&gt;Caffe_Net&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据&quot;&gt;&lt;a href=&quot;#1-基本数据&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据&quot;&gt;&lt;/a&gt;1.基本数据&lt;/h3&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;Layer&amp;lt;Dtype&amp;gt; &amp;gt; &amp;gt; layers_; &lt;span class=&quot;comment&quot;&gt;// 记录每一层的layer参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; &amp;gt; bottom_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; bottom_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt; &amp;gt; bottom_need_backward_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;/// top_vecs stores the vectors containing the output for each layer&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; &amp;gt; top_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; top_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; &amp;gt; param_id_vecs_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;string&lt;/span&gt;&amp;gt; layer_names_;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//learnable_params_[learnable_param_ids_[i]] == params_[i].get()&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt; learnable_params_;&lt;span class=&quot;comment&quot;&gt;//层间权重与bias&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Layer分析</title>
    <link href="http://www.enjoyai.site/2017/10/19/Caffe_layer/"/>
    <id>http://www.enjoyai.site/2017/10/19/Caffe_layer/</id>
    <published>2017-10-19T03:31:00.000Z</published>
    <updated>2017-10-20T17:25:51.547Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://images2017.cnblogs.com/blog/888534/201710/888534-20171019224727334-359802148.png" alt=""></p><h1 id="Caffe-Layers"><a href="#Caffe-Layers" class="headerlink" title="Caffe_Layers"></a>Caffe_Layers</h1><h3 id="1-基本数据结构"><a href="#1-基本数据结构" class="headerlink" title="1.基本数据结构"></a>1.基本数据结构</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Layer层主要的的参数</span></div><div class="line">LayerParamter layer_param_; <span class="comment">// protobuf内的layer参数</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;*&gt;&gt;blobs_;<span class="comment">//存储layer的参数，</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;param_propagate_down_;<span class="comment">//表示是否计算各个blobs反向误差。</span></div></pre></td></tr></table></figure><a id="more"></a><h3 id="2-主要函数接口"><a href="#2-主要函数接口" class="headerlink" title="2.主要函数接口"></a>2.主要函数接口</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</span></span></div><div class="line"><span class="function"><span class="params">                    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</div><div class="line"><span class="function">Dtype <span class="title">Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,</span></span></div><div class="line"><span class="function"><span class="params">                    <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;top)</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">Backward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp;top,</span></span></div><div class="line"><span class="function"><span class="params"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;param_propagate_down,<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</div></pre></td></tr></table></figure><h3 id="3-具体的Layer分析"><a href="#3-具体的Layer分析" class="headerlink" title="3.具体的Layer分析"></a>3.具体的Layer分析</h3><pre><code>具体的常用Layer分析</code></pre><h4 id="1-数据层-DataLayer"><a href="#1-数据层-DataLayer" class="headerlink" title="(1) 数据层(DataLayer)"></a>(1) 数据层(DataLayer)</h4><p>数据通过数据层进入Layer,可以来自于数据库(LevelDB或者LMDB),也可以来自内存，HDF5等<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Database：类型 Database</span></div><div class="line"><span class="comment">//必须参数 source,batch_size</span></div><div class="line"><span class="comment">//可选参数：rand_skip,mirror,backend[default LEVELDB]</span></div><div class="line"></div><div class="line"><span class="comment">// In-Memory：类型 MemoryData</span></div><div class="line"><span class="comment">// 必选参数：batch_size，channels,height,width</span></div><div class="line"></div><div class="line"><span class="comment">//HDF5 Input:类型 HDF5Data</span></div><div class="line"><span class="comment">//必选参数: source,batch_size</span></div><div class="line"></div><div class="line"><span class="comment">//Images : 类型 ImageData</span></div><div class="line"><span class="comment">//必要参数：source(文件名label),batch_size</span></div><div class="line"><span class="comment">//可选参数：rand_skip,shuffle,new_width,new_height;</span></div></pre></td></tr></table></figure></p><h4 id="2-激励层-neuron-layers"><a href="#2-激励层-neuron-layers" class="headerlink" title="(2) 激励层(neuron_layers)"></a>(2) 激励层(neuron_layers)</h4><pre><code>一般来说，激励层是element-wise，输入输出大小相同，一般非线性函数输入：n\*c\*h\*w输出：n\*c\*h\*w</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ReLU/PReLU</span></div><div class="line"><span class="comment">//可选参数 negative_slope 指定输入值小于零时的输出。</span></div><div class="line"><span class="comment">// f(x) = x*(x&gt;0)+negative_slope*(x&lt;=0)</span></div><div class="line"><span class="comment">//ReLU目前使用最为广泛，收敛快，解决梯度弥散问题</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"relu"</span></div><div class="line">    type:<span class="string">"ReLU"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//Sigmoid</span></div><div class="line"><span class="comment">//f(x) = 1./(1+exp(-x)); 负无穷--正无穷映射到-1---1</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"sigmoid-test"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">    type:<span class="string">"Sigmoid"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="3-视觉层-vision-layer"><a href="#3-视觉层-vision-layer" class="headerlink" title="(3) 视觉层(vision_layer)"></a>(3) 视觉层(vision_layer)</h4><pre><code>常用layer操作</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">//卷积层(Convolution):类型Convolution</span></div><div class="line"> <span class="comment">//包含学习率，输出卷积核，卷积核size，初始方式，权值衰减</span></div><div class="line"> <span class="comment">//假使输入n*ci*hi*wi,则输出</span></div><div class="line"> <span class="comment">// new_h = ((hi-kernel_h)+2*pad_h)/stride+1;</span></div><div class="line"> <span class="comment">// new_w = ((wi-kernel_w)+2*pad_w)/stride+1;</span></div><div class="line"> <span class="comment">//输出n*num_output*new_h*new_w;</span></div><div class="line"> layer&#123;</div><div class="line">     name: <span class="string">"conv1"</span></div><div class="line">     type: <span class="string">"CONVOLUTION"</span></div><div class="line">     bottom: <span class="string">"data"</span></div><div class="line">     top: <span class="string">"conv1"</span></div><div class="line">     blobs_lr: <span class="number">1</span></div><div class="line">     blobs_lr: <span class="number">2</span></div><div class="line">     weight_decay: <span class="number">1</span></div><div class="line">     weight_decay: <span class="number">0</span></div><div class="line">     convolution_param &#123;</div><div class="line">         num_output: <span class="number">96</span></div><div class="line">         kernel_size: <span class="number">11</span></div><div class="line">         stride: <span class="number">4</span></div><div class="line">         weight_filler &#123;</div><div class="line">             type: <span class="string">"gaussian"</span></div><div class="line">            <span class="built_in">std</span>: <span class="number">0.01</span></div><div class="line">         &#125;</div><div class="line">         bias_filler &#123;</div><div class="line">           type: <span class="string">"constant"</span></div><div class="line">           value: <span class="number">0</span></div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="comment">//池化层(Pooling) 类型 Pooling</span></div><div class="line"><span class="comment">// (hi-kernel_h)/2+1;</span></div><div class="line">layer&#123;</div><div class="line">    name:<span class="string">"pool1"</span></div><div class="line">    type:<span class="string">"POOLING"</span></div><div class="line">    bottom:<span class="string">"conv1"</span></div><div class="line">    top:<span class="string">"conv1"</span></div><div class="line">    pooling_param&#123;</div><div class="line">        pool:MAX <span class="comment">//AVE,STOCHASTIC</span></div><div class="line">        kernel_size:<span class="number">3</span></div><div class="line">        stride:<span class="number">2</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//BatchNormalization</span></div><div class="line"><span class="comment">// x' = (x-u)/δ ;y = α*x'+β;</span></div></pre></td></tr></table></figure><h4 id="4-损失层-Loss-layer"><a href="#4-损失层-Loss-layer" class="headerlink" title="(4) 损失层(Loss_layer)"></a>(4) 损失层(Loss_layer)</h4><pre><code>最小化输出于目标的LOSS来驱动学习更新</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Softmax</span></div></pre></td></tr></table></figure><h3 id="4-说明"><a href="#4-说明" class="headerlink" title="4.说明"></a>4.说明</h3><p>SetUp函数需要根据实际的参数设置进行实现，对各种类型的参数初始化；Forward和Backward对应前向计算和反向更新，输入统一都是bottom，输出为top，其中Backward里面有个propagate_down参数，用来表示该Layer是否反向传播参数。<br>在Forward和Backward的具体实现里，会根据Caffe::mode()进行对应的操作，即使用cpu或者gpu进行计算，两个都实现了对应的接口Forward_cpu、Forward_gpu和Backward_cpu、Backward_gpu，这些接口都是virtual，具体还是要根据layer的类型进行对应的计算（注意：有些layer并没有GPU计算的实现，所以封装时加入了CPU的计算作为后备）。另外，还实现了ToProto的接口，将Layer的参数写入到protocol buffer文件中。</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/19/Caffe_layer/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/19/Caffe_layer/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/888534/201710/888534-20171019224727334-359802148.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Caffe-Layers&quot;&gt;&lt;a href=&quot;#Caffe-Layers&quot; class=&quot;headerlink&quot; title=&quot;Caffe_Layers&quot;&gt;&lt;/a&gt;Caffe_Layers&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据结构&quot;&gt;&lt;a href=&quot;#1-基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据结构&quot;&gt;&lt;/a&gt;1.基本数据结构&lt;/h3&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//Layer层主要的的参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;LayerParamter layer_param_; &lt;span class=&quot;comment&quot;&gt;// protobuf内的layer参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;gt;blobs_;&lt;span class=&quot;comment&quot;&gt;//存储layer的参数，&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt;&amp;gt;param_propagate_down_;&lt;span class=&quot;comment&quot;&gt;//表示是否计算各个blobs反向误差。&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Blob分析</title>
    <link href="http://www.enjoyai.site/2017/10/18/Caffe_blob/"/>
    <id>http://www.enjoyai.site/2017/10/18/Caffe_blob/</id>
    <published>2017-10-17T16:03:10.000Z</published>
    <updated>2017-10-20T17:24:37.013Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-blob"><a href="#Caffe-blob" class="headerlink" title="Caffe_blob"></a>Caffe_blob</h1><h3 id="1-基本数据结构"><a href="#1-基本数据结构" class="headerlink" title="1.基本数据结构"></a>1.基本数据结构</h3><p>  Blob为模板类，可以理解为四维数组，n * c * h * w的结构,Layer内为blob输入data和diff，Layer间的blob为学习的参数.内部封装了SyncedMemory类,该类负责存储分配和主机与设备的同步<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_; <span class="comment">// data指针</span></div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_; <span class="comment">// diff指针</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_; <span class="comment">// blob形状</span></div><div class="line">    <span class="keyword">int</span> count_; <span class="comment">// blob的nchw</span></div><div class="line">    <span class="comment">// 当前的Blob容量，当Blob reshape后count&gt; capacity_时，capacity_ = count_;</span></div><div class="line">    <span class="comment">// 重新new 然后 reset data和 diff</span></div><div class="line">    <span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure></p><a id="more"></a><h3 id="2-常用函数"><a href="#2-常用函数" class="headerlink" title="2.常用函数"></a>2.常用函数</h3><pre><code>Blob类中常用的函数如下所示</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">Blob&lt;<span class="keyword">float</span>&gt;test;</div><div class="line"><span class="comment">//explicit关键字的作用是禁止单参数构造函数的隐式转换</span></div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="keyword">int</span> width)</span></span>;</div><div class="line">test.shape_string();<span class="comment">//初始为空 0 0 0 0</span></div><div class="line"><span class="comment">//Reshape函数将num,channels,height,width传递给vector shape_</span></div><div class="line">test.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);<span class="comment">// shape_string() 1,2,3,4</span></div><div class="line"></div><div class="line">test.shape(i);<span class="comment">// NCHW</span></div><div class="line">test.count(<span class="keyword">int</span> start_axis,<span class="keyword">int</span> end_axis); <span class="comment">// start_axis---end_axis .x* shape[i]</span></div><div class="line">test.count();<span class="comment">// nchw  count(1) chw count(2) hw.....</span></div><div class="line"><span class="comment">//shared_ptr&lt;SyncedMemory&gt; data_-&gt;cpu_data();</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* data = test.cpu_data();</div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* diff = test.cpu_diff();</div><div class="line"><span class="keyword">float</span>* data_1 = test.mutable_cpu_data();<span class="comment">//mutable修饰的表示可以修改内部值</span></div><div class="line"><span class="keyword">float</span>* diff_1 = test.mutable_cpu_diff();</div><div class="line">test.asum_data();<span class="comment">//求和 L1范数</span></div><div class="line">test.sumsq_data();<span class="comment">//平方和 L2范数</span></div><div class="line">test.Update();<span class="comment">//data = data-diff;</span></div><div class="line">a.ToProto(BlobProto&amp; bp,<span class="literal">true</span>/<span class="literal">false</span>);<span class="comment">//(FromProto)</span></div><div class="line"><span class="comment">// if &lt; 0 ,return num_axis()+axis_index;//索引序列</span></div><div class="line"><span class="keyword">int</span> index = a.CanonicalAxisIndex(<span class="keyword">int</span> axis_index);</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">offset</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//((n*channels()+c)*height()+h)*width()+w</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">data_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_data()[offset(n,c,h,w)];</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">diff_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_diff()[offset(n,c,h,w)];</span></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span>&#123;<span class="keyword">return</span> _data&#125;;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;<span class="comment">// data乘以一个标量。同理 scale_diff();</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></div><div class="line"><span class="function"><span class="params">  <span class="keyword">bool</span> reshape = <span class="literal">false</span>)</span></span>;  <span class="comment">// copy_diff是否复制diff</span></div></pre></td></tr></table></figure><h3 id="3-写入磁盘操作"><a href="#3-写入磁盘操作" class="headerlink" title="3.写入磁盘操作"></a>3.写入磁盘操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Blob内部值写入到磁盘</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;a;</div><div class="line">a.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> count = a.count();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">    a[i] = i;<span class="comment">//init the test Blob</span></div><div class="line">&#125;</div><div class="line">BlobProto bp,bp2;</div><div class="line">a.ToProto(&amp;bp,<span class="literal">true</span>);<span class="comment">//写入data和diff到bp中</span></div><div class="line">WriteProtoToBinaryFile(bp,<span class="string">"a.blob"</span>);<span class="comment">//写入磁盘</span></div><div class="line">ReadProtoFromBinaryFile(<span class="string">"a.blob"</span>,&amp;bp2);<span class="comment">//从磁盘读取blob</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;b;</div><div class="line">b.FromProto(bp2,<span class="literal">true</span>);<span class="comment">//序列化对象bp2中克隆b，完整克隆</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; b.num(); n++) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> c = <span class="number">0</span>; c &lt; b.channels(); c++) &#123;</div><div class="line">       <span class="keyword">for</span> (<span class="keyword">size_t</span> h = <span class="number">0</span>; h &lt; b.height(); h++) &#123;</div><div class="line">           <span class="keyword">for</span> (<span class="keyword">size_t</span> w = <span class="number">0</span>; w &lt; b.width(); w++) &#123;</div><div class="line">               <span class="built_in">cout</span>&lt;&lt;<span class="string">"b["</span>&lt;&lt;n&lt;&lt;<span class="string">"]["</span>&lt;&lt;c&lt;&lt;<span class="string">"]["</span>&lt;&lt;h&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]="</span>&lt;&lt;</div><div class="line">               b[(((n*b.channels()+c)*b.height)+h)*b.width()+w]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">               <span class="comment">//(((n*c+ci)*h+hi)*w+wi)</span></div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="4-部分函数的具体实现"><a href="#4-部分函数的具体实现" class="headerlink" title="4.部分函数的具体实现"></a>4.部分函数的具体实现</h3><pre><code>本部分的实现未考虑参数是否合理。一般操作blob需要分CPU和GPU,采用math_functions具体计算</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">  <span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)&#123;<span class="comment">//reshape操作</span></div><div class="line">    count_ = <span class="number">1</span>;<span class="comment">//初始count_ NCHW;</span></div><div class="line">    shape_.resize(shape.size());</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; shape.size(); i++) &#123;</div><div class="line">      count_ *= shape[i];</div><div class="line">      shape_[i] = shape[i];</div><div class="line">      <span class="keyword">if</span> (count_ &gt; capacity_) &#123; <span class="comment">//reshape的size大于了目前的最大容量</span></div><div class="line">         capacity_ = count_;</div><div class="line">         data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_*<span class="keyword">sizeof</span>(Dtype)));</div><div class="line">         diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_*<span class="keyword">sizeof</span>(Dtype)));</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">int</span> n,<span class="keyword">int</span> c,<span class="keyword">int</span> h ,<span class="keyword">int</span> w)&#123;<span class="comment">//reshape操作</span></div><div class="line">   <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;shape(<span class="number">4</span>);</div><div class="line">   shape[<span class="number">0</span>] = n;</div><div class="line">   shape[<span class="number">1</span>] = c;</div><div class="line">   shape[<span class="number">2</span>] = h;</div><div class="line">   shape[<span class="number">3</span>] = w;</div><div class="line">   Reshape(shape);</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"> <span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data()&#123;</div><div class="line">   <span class="comment">//实际调用的shared_ptr&lt;SyncedMemory&gt;data_-&gt;cpu_data();,同理cpu_diff();</span></div><div class="line">   CHECK(data_);</div><div class="line">   <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Updata()&#123; <span class="comment">//data = data-diff;需要判断cpu OR gpu</span></div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">      caffe_axpy&lt;Dtype&gt;(count_,Dtype(<span class="number">-1</span>),</div><div class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span>&lt;Dtype*&gt;(diff_-&gt;cpu_data()),</div><div class="line">      <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">  &#125;</div><div class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:<span class="comment">//在gpu或者CPU/GPU已经同步</span></div><div class="line">    <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line">    <span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">      caffe_gpu_axpy&lt;Dtype&gt;(count_.Dtype(<span class="number">-1</span>),</div><div class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span>&lt;Dtype*&gt;(diff_-&gt;gpu_data()),</div><div class="line">      <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt; <span class="comment">//从source 拷贝数据,copy_diff控制是拷贝diff还是data</span></div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, <span class="keyword">bool</span> reshape) &#123;</div><div class="line">  <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</div><div class="line">    <span class="keyword">if</span> (reshape) &#123;</div><div class="line">      ReshapeLike(source);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;  <span class="comment">//copy diff</span></div><div class="line">      caffe_copy(count_, source.gpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.gpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.cpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.cpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ToProto(BlobProto* proto,<span class="keyword">bool</span> write_diff)&#123;</div><div class="line">    proto-&gt;clear_shape();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; shaoe_.size(); i++) &#123;</div><div class="line">        proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);</div><div class="line">    &#125;</div><div class="line">    proto-&gt;clear_data();</div><div class="line">    proto-&gt;clear_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* data_vec = cpu_data();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count_; i++) &#123;</div><div class="line">        proto-&gt;add_data(data_vec[i]);<span class="comment">//data写入proto</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (write_diff) &#123;</div><div class="line">        <span class="keyword">const</span> Dtype* diff_vec = cpu_diff();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count_; i++) &#123;</div><div class="line">            proto-&gt;add_diff(diff_vec[i]);<span class="comment">//diff写入proto</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="5-说明"><a href="#5-说明" class="headerlink" title="5.说明"></a>5.说明</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*Blob作为一个最基础的类，其中构造函数开辟一个内存空间来存储数据，Reshape</span></div><div class="line"><span class="comment">函数在Layer中的reshape或者forward操作中来调整top的输出维度。同时在改变Blob</span></div><div class="line"><span class="comment">大小时， 内存将会被重新分配如果内存大小不够了，并且额外的内存将不会被释放。</span></div><div class="line"><span class="comment">对input的blob进行reshape, 若立马调用Net::Backward是会出错的，因为reshape</span></div><div class="line"><span class="comment">之后，要么Net::forward或者Net::Reshape就会被调用来将新的input shape传播</span></div><div class="line"><span class="comment">到高层 */</span></div></pre></td></tr></table></figure><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/10/18/Caffe_blob/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/10/18/Caffe_blob/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-blob&quot;&gt;&lt;a href=&quot;#Caffe-blob&quot; class=&quot;headerlink&quot; title=&quot;Caffe_blob&quot;&gt;&lt;/a&gt;Caffe_blob&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据结构&quot;&gt;&lt;a href=&quot;#1-基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据结构&quot;&gt;&lt;/a&gt;1.基本数据结构&lt;/h3&gt;&lt;p&gt;  Blob为模板类，可以理解为四维数组，n * c * h * w的结构,Layer内为blob输入data和diff，Layer间的blob为学习的参数.内部封装了SyncedMemory类,该类负责存储分配和主机与设备的同步&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;protected&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; data_; &lt;span class=&quot;comment&quot;&gt;// data指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; diff_; &lt;span class=&quot;comment&quot;&gt;// diff指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; shape_; &lt;span class=&quot;comment&quot;&gt;// blob形状&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; count_; &lt;span class=&quot;comment&quot;&gt;// blob的nchw&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 当前的Blob容量，当Blob reshape后count&amp;gt; capacity_时，capacity_ = count_;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 重新new 然后 reset data和 diff&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; capacity_;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://www.enjoyai.site/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://www.enjoyai.site/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://www.enjoyai.site/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法 1-统计学习算法概述</title>
    <link href="http://www.enjoyai.site/2017/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://www.enjoyai.site/2017/09/14/统计学习方法1-统计学习算法概述/</id>
    <published>2017-09-14T06:24:11.000Z</published>
    <updated>2017-10-20T17:31:06.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计学习的主要特点"><a href="#统计学习的主要特点" class="headerlink" title="统计学习的主要特点"></a>统计学习的主要特点</h2><pre><code>统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。</code></pre><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><pre><code>监督学习(supervised learning)无监督学习(unsupervised learning)半监督学习(self-supervised learning)增强式学习(reinfoucement learning)</code></pre><a id="more"></a><h3 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h3><p>输入实际x的特征向量记做$x = (x^{(1)},x^{(2)},x^{(3)}, \cdots ,x^{(n)})^T$<br>训练集：$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),\cdots (x_n,y_n)}$<br>输入变量与输出变量均为连续变量的预测问题为回归问题；<br>输出变量为有限个离散变量的预测问题为分类问题;</p><blockquote><p>本文作者： 张峰<br>本文链接：<a href="https://zhanglaplace.github.io/2017/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/14</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;统计学习的主要特点&quot;&gt;&lt;a href=&quot;#统计学习的主要特点&quot; class=&quot;headerlink&quot; title=&quot;统计学习的主要特点&quot;&gt;&lt;/a&gt;统计学习的主要特点&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;监督学习(supervised learning)
无监督学习(unsupervised learning)
半监督学习(self-supervised learning)
增强式学习(reinfoucement learning)
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计学习方法" scheme="http://www.enjoyai.site/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="http://www.enjoyai.site/2017/09/10/Linear-Regression/"/>
    <id>http://www.enjoyai.site/2017/09/10/Linear-Regression/</id>
    <published>2017-09-10T02:23:08.000Z</published>
    <updated>2017-10-20T17:26:47.210Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Model-and-Cost-Function-模型和损失函数"><a href="#Model-and-Cost-Function-模型和损失函数" class="headerlink" title="Model and Cost Function(模型和损失函数)"></a>Model and Cost Function(模型和损失函数)</h2><p>对于model，给出如下定义 $y = \theta x$<br>损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$<br><a id="more"></a><br>Gradient descent algorithm<br>repeat until convergence{<br>    $\quad \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$<br>}</p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>寻找两类样本正中间的划分超平面，因为该超平面对训练样本的布局扰动的容忍度最好，是最鲁棒的<br>划分超平面方程:<br>$$wx+b = 0$$<br>我们假使<br>$$<br>\begin{cases}<br>wx_i+b &gt;= 1 \qquad\quad y_i = +1 \\\<br>\\<br>wx_i+b &lt;=-1 \qquad\, y_i = -1<br>\end{cases}<br>$$<br>则距离超平面最近的几个点使得下列式子成立<br>$$\max\limits_{w,b}(\frac{2}{||w||}) \rightarrow \min_{w,b}\frac{1}{2}||w||^2$$<br>$$s.t. y_i(wx_i+b)\ge 1 i = 1,2,…,m.$$<br>通用表达式:<br>    $f(x)=w\psi(x)+b = \sum_{i=1}^{m}a_iy_i\psi(x_i)^T\psi(x)+b=\sum_{i=1}^{m}a_iy_i\kappa(x,x_i)+b$<br>$\kappa 为核函数.$</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/10/Linear-Regression/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/10/Linear-Regression/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Model-and-Cost-Function-模型和损失函数&quot;&gt;&lt;a href=&quot;#Model-and-Cost-Function-模型和损失函数&quot; class=&quot;headerlink&quot; title=&quot;Model and Cost Function(模型和损失函数)&quot;&gt;&lt;/a&gt;Model and Cost Function(模型和损失函数)&lt;/h2&gt;&lt;p&gt;对于model，给出如下定义 $y = \theta x$&lt;br&gt;损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>C++随笔</title>
    <link href="http://www.enjoyai.site/2017/09/08/C++%E9%9A%8F%E7%AC%94/"/>
    <id>http://www.enjoyai.site/2017/09/08/C++随笔/</id>
    <published>2017-09-08T05:44:33.000Z</published>
    <updated>2017-10-20T17:26:49.317Z</updated>
    
    <content type="html"><![CDATA[<h3 id="重写，重定义、重载的区别"><a href="#重写，重定义、重载的区别" class="headerlink" title="重写，重定义、重载的区别"></a>重写，重定义、重载的区别</h3><a id="more"></a><h4 id="重写"><a href="#重写" class="headerlink" title="重写"></a>重写</h4><p>$\qquad 子类(派生类)重新定义基类的虚函数方法，要求函数名，函数参数，返回类型完全相同.并\\$<br>$且基于必须是虚函数，不能有static关键字,重写函数的访问修饰符可以与基类的不同。\\$<br>$\qquad 基类指针指向派生类，若实现了重写，则调用派生类，若没，则调用基类,即实现多态$</p><h4 id="重定义"><a href="#重定义" class="headerlink" title="重定义"></a>重定义</h4><p>$\qquad 子类(派生类)重新申明和定义基类的函数，要求函数名相同，但是返回值可以不同，参数\\$<br>$不同，无论有无virtual，基类的都将被隐藏，参数相同，基类如果没有virtual，则基类的函被\\$<br>$隐藏$</p><h4 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h4><p>$\qquad函数名相同，但是他们的参数列表个数或者顺序，类型不同，且不能仅有返回类型不同，要\\$<br>$求再同一个作用于.$</p><h3 id="多态的实现方式"><a href="#多态的实现方式" class="headerlink" title="多态的实现方式"></a>多态的实现方式</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>$\qquad 多态: 即程序运行中，系统根据对象指针所指向的类别对相同的消息进行不同的方法处理$</p><h4 id="动态多态"><a href="#动态多态" class="headerlink" title="动态多态"></a>动态多态</h4><p>$\qquad 通过类的继承和虚函数机制，在程序运行期实现多态,虚函数表$</p><h4 id="静态多态"><a href="#静态多态" class="headerlink" title="静态多态"></a>静态多态</h4><p>$\qquad 函数重载；运算符重载$</p><h3 id="常用排序算法"><a href="#常用排序算法" class="headerlink" title="常用排序算法"></a>常用排序算法</h3><h4 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h4><p>$\qquad 快速排序的实现:$<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> l ,<span class="keyword">int</span> r)</span></span>&#123;<span class="comment">//或者vector</span></div><div class="line">    <span class="keyword">if</span>(l &lt; r)&#123;</div><div class="line">        <span class="keyword">int</span> i = l ,j = r ;</div><div class="line">        <span class="keyword">int</span> sed = a[i];<span class="comment">//种子点</span></div><div class="line">        <span class="keyword">while</span>(i &lt; j )&#123;</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[j] &gt; sed )</div><div class="line">                --j;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[i++] = a[j];</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[i] &lt; sed )</div><div class="line">                ++i;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[j--] = a[i];</div><div class="line">        &#125;</div><div class="line">        a[i] = sed;</div><div class="line">        quickSort(a,l,i<span class="number">-1</span>);</div><div class="line">        qucikSort(a,i+<span class="number">1</span>,r);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/08/C++%E9%9A%8F%E7%AC%94/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/08/C++%E9%9A%8F%E7%AC%94/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;重写，重定义、重载的区别&quot;&gt;&lt;a href=&quot;#重写，重定义、重载的区别&quot; class=&quot;headerlink&quot; title=&quot;重写，重定义、重载的区别&quot;&gt;&lt;/a&gt;重写，重定义、重载的区别&lt;/h3&gt;
    
    </summary>
    
      <category term="C++" scheme="http://www.enjoyai.site/categories/C/"/>
    
    
      <category term="C++" scheme="http://www.enjoyai.site/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归分析</title>
    <link href="http://www.enjoyai.site/2017/09/07/logistic/"/>
    <id>http://www.enjoyai.site/2017/09/07/logistic/</id>
    <published>2017-09-07T12:03:49.000Z</published>
    <updated>2017-10-20T17:26:23.816Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic回归分析"><a href="#Logistic回归分析" class="headerlink" title="Logistic回归分析"></a>Logistic回归分析</h3><p>$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$</p><p>$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$<br><a id="more"></a><br>$$f(x) = \frac{1}{1+exp(-z)}$$</p><p>$\quad \quad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$</p><p>$$trainingData \quad\, x^1 \quad\, x^2 \quad\, x^3 \quad\,\cdots \quad\, x^n $$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad labels \qquad   \quad  C_{1} \quad C_{1} \quad C_{2} \quad \cdots \quad C_{1} \\$<br>$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^n)$来寻找最佳的$w$和$b$<br>$$<br>w^{*},b^{*} = arg\max\limits_{w,b}(L(w,b))\Longrightarrow w^{*},b^{*} = arg\min\limits_{w,b}(-ln{L(w,b)})<br>$$</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><a href="https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="external">随机梯度下降法</a></h3><p>$\qquad 我们需要优化的函数:-ln{L(w,b)} = -{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)}\quad \\$<br>$$<br>\qquad 假设：<br>\begin{cases}<br>\hat{y} = 1 \qquad x\in1 \\\<br>\\<br>\hat{y} = 0 \qquad  x\in0<br>\end{cases}<br>\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n  w_{i}x_{i} 则<br>$$<br>$\qquad 我们需要优化的函数简化为：ln{L(w,b)} =\sum_{j=1}^{n}{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))} \\$</p><p>$\qquad 当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x) \\$<br>$\qquad 当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x)) \qquad \\$<br>$\qquad 即均满足上式 , 因此:$</p><p>$\qquad \qquad \quad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \quad \quad 而 \, \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=(1-f_{w,b}(x))*x_i \\$</p><p>$\quad \quad 同理 \quad   \frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i \qquad 则化简后:\\$<br>$\qquad \quad\,\, \qquad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \quad \qquad = \sum_{j=1}^{n}{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i} \\$</p><p>$\qquad \qquad \quad\qquad \qquad = \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i \\$</p><p>$\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i \\$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201709/888534-20170908103015851-1635753052.png" alt=""></p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><h4 id="1-为什么选用-crossEntropy-损失函数，而不用L2损失函数"><a href="#1-为什么选用-crossEntropy-损失函数，而不用L2损失函数" class="headerlink" title="1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数"></a>1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数</h4><p>$答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，\\$<br>$由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：\\$<br>$$\frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i $$<br>$则当\,\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad  \\$<br>$但是当\,\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对 \\$</p><h4 id="2-logistic-regression-的分类概率为什么选取了-sigmoid-函数"><a href="#2-logistic-regression-的分类概率为什么选取了-sigmoid-函数" class="headerlink" title="2. $logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数"></a>2. <a href="https://www.zhihu.com/question/54707359" target="_blank" rel="external">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</a></h4><p>$答: 我们假设样本的分布服从二次高斯分布，即\\$</p><p>$f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)},其中\mu为均值，\Sigma为协方差矩阵 \\$</p><p>$输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma, \\$<br>$因此需要求取最佳的高斯分布来满足样本的分布 \\$</p><p>$$Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots f_{\mu,\Sigma}(x^{N})$$<br>$$\mu^{*}，\Sigma^{*} = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma)$$<br>$$\mu^{*} = \frac{1}{N}\sum_{i=0}^{N}{x^i}$$<br>$$\Sigma^{*} = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^{*})(x^i-\mu^{*})^T}$$</p><p>$对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma \\$<br>$$\mu^1 = \sum_{i=1}^{n_1} x_i\qquad (x_i \in C_1) \quad ;\quad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) $$<br>$$\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ;\quad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T ;\quad \Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2 $$</p><p>$对于样本x，如果属于C_1则有：\\$</p><p>$\qquad \qquad\qquad \qquad P(C_{1}|x) \,\,= \frac{P(C_{1},x)}{P(x)} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+\frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+exp(-\alpha)} \\$</p><p>$其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})$</p><p>$将P(x|C_i)带入高斯分布的公式:\\$<br>$$P(C_1)=\frac{n_1}{n_1+n_2}\quad , \quad P(C_2)=\frac{n_2}{n_1+n_2} $$<br>$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)} $$<br>$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)} $$<br>$\alpha= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\$<br>$\quad =-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\$<br>$\quad =-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = (u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = wx+b\\$<br>$\quad w = (u^1-u^2)^T(\Sigma)^{-1} \quad ; \quad b=-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的 \\$</p><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><pre><code>基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候，</code></pre><p>都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss</p><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><pre><code>基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其</code></pre><p>效果一般比生成模型好(基于事实 not 基于猜测)</p><h3 id="小扩展"><a href="#小扩展" class="headerlink" title="小扩展"></a>小扩展</h3><h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><pre><code>基于先验概率得出的每个类别的后验概率为softmax函数，即：</code></pre><p>$\\$<br>$\qquad \qquad \qquad \qquad \, P(C_i|x) = \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad = \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}\\$</p><h4 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h4><p>未完待续</p><blockquote><p>本文作者： 张峰<br>本文链接： <a href="https://zhanglaplace.github.io/2017/09/07/logistic/" target="_blank" rel="external">https://zhanglaplace.github.io/2017/09/07/logistic/</a><br>版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic回归分析&quot;&gt;&lt;a href=&quot;#Logistic回归分析&quot; class=&quot;headerlink&quot; title=&quot;Logistic回归分析&quot;&gt;&lt;/a&gt;Logistic回归分析&lt;/h3&gt;&lt;p&gt;$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$&lt;/p&gt;
&lt;p&gt;$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://www.enjoyai.site/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.enjoyai.site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计学习方法" scheme="http://www.enjoyai.site/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
</feed>
