<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[目标检测常用衡量指标]]></title>
    <url>%2F2017%2F11%2F07%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[目标检测常用衡量指标 目标检测中，存在很多常见的模型评估与选择的度量方法，本文结合周志华老师的&lt;机器学习&gt;，以及自己的理解对常见的度量方法做一个总结。 基础介绍 常见的评估方法，我们在进行样本学习与测试的过程中，通常采用多种方式来进行样本集合的分类。 (1) 留出法 将样本按比例分为两个子集，一个为训练集，一个为验证集，通常保证训练集和验证集的样本类别服从同分布。多次划分后取平均的实验结果作为最终的结果。 (2) 交叉验证法 (最常用) 通过对数据集划分为k个大小基本相同，分布基本相似的子集，每次从中选取K-1次进行训练，1个进行测试，则可以得到K组结果，最终根据k组的结果进行统计，一般为5折或者10折。 (3) 自助法 数据集较小的时候，通过自身的bootstrapping方法，多次有放回的采样增加样本集合。 模型评估 通常我们定量一个模型的好坏，根据错误率和准确率来定量，但是在实际问题中，还有很多衡量的指标。 回归 (1) 常用均方误差来衡量MSE $$ E(f;D)= \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2$$ (2) 和方差 SSE $$E(f;D) = \sum_{i=1}^{m}w_i(f(x_i)-y_i)^2$$ (3) 均方根误差RMSE $$RMSE = \sqrt{MSE}= \sqrt{\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2}$$ 分类 (1) 错误率与准确率 $$E(f;D) = \frac{1}{m}\sum_{i=1}^{m} I(f(x_i) \neq y_i)$$ $$acc(f;D) = \frac{1}{m}\sum_{i=1}^{m} I(f(x_i) = y_i) = 1-E(f;D)$$ (2) 查准率(精确度)和查全率(召回率) $$表2.1 分类结果混淆矩阵$$| 真实\预测 | 正样本 | 负样本 || ——— | —— | —— || 正样本 | TP | FN || 负样本 | FP | TN |则，查准率与召回率公式如下$$ P = \frac{TP}{TP+FP}$$$$ R = \frac{TP}{TP+FN}$$一般来说查全率高，召回率往往低，召回率高，查全率就偏低，因此，常用F1Score来衡量:$$ F1 = \frac{2*P*R}{P+R}$$ 通常在做目标检测与分类时，会设定不同的阈值，目标会根据阈值划分到不同的类别，因此通过对分数阈值排序，可以得到多组的PR值，从而可以画出PR曲线，通常用y=x与PR曲线的交点来作为平衡点评估模型的好坏。 (3) ROC于AUC 在做识别任务中，通常产生一个分数值，通过与阈值的对比，从而判断样本属于正例还是负例，而ROC曲线，则用以衡量真正例率与假正例率的比例. $$ TPR = \frac{TP}{TP+FN} $$ $$ FPR = \frac{FP}{FP+TN} $$ 通过设置不同的阈值，可以得到不同的TPR和FPR，从而做出ROC曲线 而AUC用来衡量ROC曲线与坐标轴的面积，面积越大，则代表模型越好，通常: $$ AUC = \frac{1}{2}\sum_{i=1}^{m}(x_{i+1}-x_i)*(y_{i+1}-y_i) $$ (4) FAR 与 FRR FAR即(False Acceptance Rate),FRR即(False rejection Rate)，一般用来衡量二分类，例如人脸中，FAR代表不同的人识别为同一个人的概率，而FRR代表一个人识别为不同人的概率.如果300个人，每个人两张图片，则总共的比较次数为 $C_{600}^{2}$,其中应当识别成为同一个人的有300对，应当识别为不同的人的有 $C_{300}^{2}*C_{2}^{1}*C_{2}^{1}$,则通过计算300对中识别成不是一个人的个数与不同人识别为同一个人的概率来衡量模型的好坏。 本文作者： 张峰本文链接：http://www.enjoyai.site/2017/10/30版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Batch Normalization推导]]></title>
    <url>%2F2017%2F11%2F06%2FCaffe_BatchNormalization%2F</url>
    <content type="text"><![CDATA[Caffe BatchNormalization 推导 总所周知，BatchNormalization通过对数据分布进行归一化处理，从而使得网络的训练能够快速并简单，在一定程度上还能防止网络的过拟合，通过仔细看过Caffe的源码实现后发现，Caffe是通过BN层和Scale层来完整的实现整个过程的。 谈谈理论与公式推导 那么再开始前，先进行必要的公式说明：定义$L$为网络的损失函数，BN层的输出为$y$，根据反向传播目前已知 $\frac{\partial L}{\partial y_i}$,其中： $$y_i = \frac{x_i-\overline{x}}{\sqrt{\delta^2+\epsilon}},\quad\overline x = \frac{1}{m}\sum_{i=1}^{m}x_i,\quad \delta^2 = \frac{1}{m}\sum_{i=1}^{m}(x_i-\overline x)^2,\quad 求\frac{\partial L}{\partial x_i}$$ 推导的过程中应用了链式法则： $$ \frac{\partial L}{\partial x_i} = \sum_{j=1}^{m}{\frac{\partial L}{\partial y_j}*\frac{\partial y_j}{\partial x_i}} $$ 则只需要着重讨论公式 $\frac{\partial y_j}{\partial x_i}$ 分布探讨： (1) $\overline x$对$x_i$的导函数 $$\frac{\partial \overline x}{\partial x_i} = \frac{1}{m} $$ (2) $\delta^2$对$x_i$的导函数 $$\frac{\partial \delta^2}{\partial x_i} = \frac{1}{m}(\sum_{j=1}^{m}2*(x_j-\overline x)*(-\frac{1}{m}))+2(x_i-\overline x)$$ 由于 $\sum_{j=1}^{m}2*(x_j-\overline x) = 2* \sum_{i=1}^{m}x_i - n*\overline x = 0$ 所以： $\frac{\partial \delta^2}{\partial x_i} = \frac{2}{m}*(x_i-\overline x)$ 具体推导： $$\frac{\partial y_j}{\partial x_i} = \frac{\partial{\frac{x_j -\overline x}{\sqrt{\delta^2+\epsilon}}}}{\partial x_i} $$ 此处当$j$等于$i$成立时时，分子求导多一个 $x_i$的导数 $$\frac{\partial y_j}{\partial x_i} = -\frac{1}{m}(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j - \overline x)\quad\quad i \neq j $$ $$\frac{\partial y_j}{\partial x_i} = (1-\frac{1}{m})(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j - \overline x)\quad\quad i = j$$ 根据上式子，我们代入链式法则的式子 $$\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i}*(\delta^2+\epsilon)^{-1/2} + \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}*(-\frac{1}{m}(\delta^2+\epsilon)^{-1/2}-\frac{1}{m}(\delta^2+\epsilon)^{-3/2}(x_i-\overline x)(x_j-\overline x))$$ 我们提出 $(\delta^2+\epsilon)^{-1/2}:$ $$\frac{\partial L}{\partial x_i} = (\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}-\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}(\delta^2+\epsilon)^{-1}(x_i-\overline x)(x_j-\overline x)) \\ =(\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}-\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\frac{1}{m}y_jy_i \\ =(\delta^2+\epsilon)^{-1/2}(\frac{\partial L}{\partial y_i}- \frac{1}{m}\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}-\frac{1}{m}y_i\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}y_j)$$ 至此，我们可以对应到caffe的具体实现部分1234567 // if Y = (X-mean(X))/(sqrt(var(X)+eps)), then//// dE(Y)/dX =// (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)// ./ sqrt(var(X) + eps)//// where \cdot and ./ are hadamard product and elementwise division, 谈谈具体的源码实现 知道了BN层的公式与原理，接下来就是具体的源码解析，由于考虑到的情况比较多，所以$Caffe$中的BN的代码实际上不是那么的好理解，需要理解，BN的归一化是如何归一化的： HW的归一化，求出NC个均值与方差，然后N个均值与方差求出一个均值与方差的Vector，size为C，即相同通道的一个mini_batch的样本求出一个mean和variance 成员变量 BN层的成员变量比较多，由于在bn的实现中，需要记录mean_,variance_,归一化的值，同时根据训练和测试实现也有所差异。12345678910Blob&lt;Dtype&gt; mean_,variance_,temp_,x_norm; //temp_保存(x-mean_x)^2bool use_global_stats_;//标注训练与测试阶段Dtype moving_average_fraction_;int channels_;Dtype eps_; // 防止分母为0// 中间变量，理解了BN的具体过程即可明了为什么需要这些Blob&lt;Dtype&gt; batch_sum_multiplier_; // 长度为N*1，全为1，用以求和Blob&lt;Dtype&gt; num_by_chans_; // 临时保存H*W的结果，length为N*CBlob&lt;Dtype&gt; spatial_sum_multiplier_; // 统计HW的均值方差使用 成员函数 成员函数主要也是LayerSetUp,Reshape,Forward和Backward,下面是具体的实现： LayerSetUp,层次的建立，相应数据的读取123456789101112131415161718192021222324252627282930313233343536373839//LayerSetUp函数的具体实现template &lt;typename Dtype&gt;void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; // 参见proto中添加的BatchNormLayer BathcNormParameter param = this-&gt;layer_param_.batch_norm_param(); moving_average_fraction_ = param.moving_average_fraction();//默认0.99 //这里有点多余，好处是防止在测试的时候忘写了use_global_stats时默认true use_global_stats_ = this-&gt;phase_ == TEST; if (param.has_use_global_stat()) &#123; use_global_stats_ = param.use_global_stats(); &#125; if (bottom[0]-&gt;num_axes() == 1) &#123; //这里基本看不到为什么.....??? channels_ = 1; &#125; else&#123; // 基本走下面的通道，因为输入是NCHW channels_ = bottom[0]-&gt;shape(1); &#125; eps_ = param.eps(); // 默认1e-5 if (this-&gt;blobs_.size() &gt; 0) &#123; // 测试的时候有值了，保存了均值方差和系数 //保存mean,variance, &#125; else&#123; // BN层的内部参数的初始化 this-&gt;blobs_.resize(3); // 均值滑动，方差滑动，滑动系数 vector&lt;int&gt;sz; sz.push_back(channels_); this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(sz)); // C this-&gt;blobs_[1].reset(new Blob&lt;Dtype&gt;(sz)); // C sz[0] = 1; this-&gt;blobs_[2].reset(new Blob&lt;Dtype&gt;(sz)); // 1 for (size_t i = 0; i &lt; 3; i++) &#123; caffe_set(this-&gt;blobs_[i]-&gt;count(),Dtype(0), this-&gt;blobs_[i]-&gt;mutable_cpu_data()); &#125; &#125; &#125; Reshape,根据BN层在网络的位置，调整bottom和top的shapeReshape层主要是完成中间变量的值，由于是按照通道求取均值和方差，而CaffeBlob是NCHW,因此先求取了HW,后根据BatchN求最后的输出C,因此有了中间的batch_sum_multiplier_和spatial_sum_multiplier_以及num_by_chans_其中num_by_chans_与前两者不想同，前两者为方便计算，初始为1，而num_by_chans_为中间过渡123456789101112131415161718192021222324252627282930313233343536template &lt;typename Dtype&gt;void BatchNormLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; if (bottom[0]-&gt;num_axes() &gt;= 1) &#123; CHECK_EQ(bottom[0]-&gt;shape(1),channels_); &#125; top[0]-&gt;ReshapeLike(*bottom[0]); // Reshape(bottom[0]-&gt;shape()); vector&lt;int&gt;sz; sz.push_back(channels_); mean_.Reshape(sz); variance_.Reshape(sz); temp_.ReshapeLike(*bottom[0]); x_norm_.ReshapeLike(*bottom[0]); sz[0] = bottom[0]-&gt;shape(0); //N // 后续会初始化为1，为求Nbatch的均值和方差 batch_sum_multiplier_.Reshape(sz); caffe_set(batch_sum_multiplier_.count(),Dtype(1), batch_sum_multiplier_.mutable_cpu_data()); int spatial_dim = bottom[0]-&gt;count(2);//H*W if (spatial_sum_multiplier_.num_axes() == 0 || spatial_sum_multiplier_.shape(0) != spatial_dim) &#123; sz[0] = spatial_dim; spatial_sum_multiplier_.Reshape(sz); //初始化1，方便求和 caffe_set(spatial_sum_multiplier_.count(),Dtype(1) spatial_sum_multiplier_.mutable_cpu_data()); &#125; // N*C,保存H*W后的结果,会在计算中结合data与spatial_dim求出 int numbychans = channels_*bottom[0]-&gt;shape(0); if (num_by_chans_.num_axes() == 0 || num_by_chans_.shape(0) != numbychans) &#123; sz[0] = numbychans; num_by_chans_.Reshape(sz); &#125; &#125; Forward 前向计算前向计算，根据公式完成前计算，x_norm与top相同，均为归一化的值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102template &lt;typename Dtype&gt;void BatchNormLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; // 想要完成前向计算，必须计算相应的均值与方差，此处的均值与方差均为向量的形式c const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); int num = bottom[0]-&gt;shape(0);// N int spatial_dim = bottom[0]-&gt;count(2); //H*W if (bottom[0] != top[0]) &#123; caffe_copy(top[0]-&gt;count(),bottom_data,top_data);//先复制一下 &#125; if (use_global_stats_) &#123; // 测试阶段,使用全局的均值 const Dtype scale_factory = this_-&gt;blobs_[2]-&gt;cpu_data()[0] == 0? 0:1/this-&gt;blobs_[2]-&gt;cpu_data()[0]; // 直接载入训练的数据 alpha*x = y caffe_cpu_scale(mean_.count(),scale_factory, this_blobs_[0]-&gt;cpu_data(),mean_.mutable_cpu_data()); caffe_cpu_scale(variance_.count(),scale_factory, this_blobs_[1]-&gt;cpu_data(),variance_.mutable_cpu_data()); &#125; else&#123; //训练阶段 compute mean //1.计算均值,先计算HW的，在包含N // caffe_cpu_gemv 实现 y = alpha*A*x+beta*y; // 输出的是channels_*num, //每次处理的列是spatial_dim，由于spatial_sum_multiplier_初始为1，即NCHW中的 // H*W各自相加，得到N*C*average，此处多除以了num，下一步可以不除以 caffe_cpu_gemv&lt;Dtype&gt;(CBlasNoTrans,channels_*num,spatial_dim, 1./(spatial_dim*num),bottom_data,spatial_sum_multiplier_.cpu_data() ,0.,num_by_chans_.mutable_cpu_data()); //2.计算均值，计算N各的平均值. // 由于输出的是channels个均值，因此需要转置 // 上一步得到的N*C的均值，再按照num求均值，因为batch_sum全部为1, caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,1, num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(), 0,mean_.mutable_cpu_data()); &#125; // 此处的均值已经保存在mean_中了 // 进行 x - mean_x 操作，需要注意按照通道，即先确定x属于哪个通道. // 因此也是进行两种，先进行H*W的减少均值 // caffe_cpu_gemm 实现alpha * A*B + beta* C // 输入是num*1 * 1* channels_,输出是num*channels_ caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num,channels_,1,1, batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),0, num_by_chans_.mutable_cpu_data()); //同上，输入是num*channels_*1 * 1* spatial = NCHW // top_data = top_data - mean; caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num*channels_, spatial_dim,1,-1,num_by_chans_.cpu_data(), spatial_sum_multiplier_.cpu_data(),1, top_data()); // 解决完均值问题，接下来就是解决方差问题 if (use_global_stats_) &#123; // 测试的方差上述已经读取了 // compute variance using var(X) = E((X-EX)^2) // 此处的top已经为x-mean_x了 caffe_powx(top[0]-&gt;count(),top_data,Dtype(2), temp_.mutable_cpu_data());//temp_保存(x-mean_x)^2 // 同均值一样，此处先计算spatial_dim的值 caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,num*channels_,spatial_dim, 1./(num*spatial_dim),temp_.cpu_data(), spatial_sum_multiplier_.cpu_data(),0, num_by_chans_.mutable_cpu_data(); ) caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,1., num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(), 0,variance_.mutable_cpu_data());// E((X_EX)^2) //均值和方差计算完成后，需要更新batch的滑动系数 this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] *= moving_average_fraction_; this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] += 1; caffe_cpu_axpby(mean_.count(),Dtype(1),mean_.cpu_data(), moving_average_fraction_,this-&gt;blobs_[0]-&gt;mutable_cpu_data()); int m = bottom[0]-&gt;count()/channels_; Dtype bias_correction_factor = m &gt; 1? Dtype(m)/(m-1):1; caffe_cpu_axpby(variance_.count(),bias_correction_factor, variance_.cpu_data(),moving_average_fraction_, this-&gt;blobs_[1]-&gt;mutable_cpu_data()); &#125; // 方差求个根号,加上eps为防止分母为0 caffe_add_scalar(variance_.count(),eps_,variance_.mutable_cpu_data()); caffe_powx(variance_.count(),variance_.cpu_data(),Dtype(0.5), variance_.mutable_cpu_data()); // top_data = x-mean_x/sqrt(variance_),此处的top_data已经转化为x-mean_x了 // 同减均值，也要分C--N*C和 N*C --- N*C*H*W // N*1 * 1*C == N*C caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num,channels_,1,1, batch_sum_multiplier_.cpu_data(),variance_.cpu_data(),0, num_by_chans_.mutable_cpu_data()); // NC*1 * 1* spatial_dim = NCHW caffe_cpu_gemm&lt;Dtype&gt;(CBlasNoTrans,CBlasNoTrans,num*channels_,spatial_dim, 1, 1.,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(), 0, temp_.mutable_cpu_data()); // temp最终保存的是sqrt（方差+eps) caffe_cpu_div(top[0].count(),top_data,temp_.cpu_data(),top_data); &#125; 整个forward过程按照x-mean/variance的过程进行，包含了求mean和variance，他们都是C*1的向量，然后输入的是NCHW,因此通过了gemm操作做广播填充到整个featuremap然后完成减mean和除以方差的操作。同时需要注意caffe的inplace操作，所以用x_norm保存原始的top值，后续修改也不会影响它。 Backward过程，根据梯度，反向计算 Backward过程会根据前面所推导的公式进行计算，具体的实现如下面所示. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374template &lt;typename Dtype&gt;void BatchNormLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123; const Dtype* top_diff; if (bottom[0] != top[0]) &#123; // 判断是否同名 top_diff = top[0]-&gt;cpu_diff(); &#125; else&#123; caffe_copy(x_norm_.count(),top[0]-&gt;cpu_diff(),x_norm_.mutable_cpu_diff()); top_diff = x_norm_.cpu_diff(); &#125; Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); if (use_global_stats_) &#123; // 测试阶段 caffe_div(temp_.count(),top_diff,temp_.cpu_data(),bottom_diff); return ; // 测试阶段不需要计算梯度。 &#125; const Dtype* top_data = x_norm_.cpu_data(); int num = bottom[0]-&gt;shape(0); //n int spatial_dim = bottom[0]-&gt;count(2); // H*W // 根据推导的公式开始具体计算。 // dE(Y)/dX = // (top_diff- mean(top_diff) - mean(top_diff \cdot Y) \cdot Y) // ./ sqrt(var(X) + eps) // sum(top_diff \cdot Y) ,y为x_norm_ NCHW,求取的均先求C通道的均值 caffe_mul(temp_.count(),top_data,top_diff,bottom_diff); //NC*HW* HW*1 = NC*1 caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,channels_*num,spatial_dim,1., bottom_diff,spatial_sum_multiplier_.cpu_data(),0, num_by_chans_.mutable_cpu_data()); // (NC)^T*1 * N*1 = C*1 caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,1., num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(), 0,mean_.mutable_cpu_data()); //reshape broadcast // N*1 * 1* C = N* C caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num,channels_,1,1, batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),0, num_by_chans_.mutable_cpu_data()); // N*C *1 * 1* HW = NC* HW caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num*channels_,spatial_dim, 1,1.,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(),0, bottom_diff); //相当与 sum (DE/DY .\cdot Y) // sum(dE/dY \cdot Y) \cdot Y caffe_mul(temp_.count(), top_data, bottom_diff, bottom_diff); // 完成了右边一个部分，还有前面的 sum(DE/DY)和DE/DY // 再完成sum(DE/DY) caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans,channels_*num,spatial_dim,1, top_diff,spatial_sum_multiplier_.cpu_data(),0., num_by_chans_.mutable_cpu_data()); caffe_cpu_gemv&lt;Dtype&gt;(CBlasTrans,num,channels_,1., num_by_chans_.cpu_data(),batch_sum_multiplier_.cpu_data(),0, mean_.mutable_cpu_data()); //reshape broadcast caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num,channels_,1, 1,batch_sum_multiplier_.cpu_data(),mean_.cpu_data(),0, num_by_chans_.mutable_cpu_data()); // 现在完成了sum(DE/DY)+y*sum(DE/DY.\cdot y) caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans,CblasNoTrans,num*channels_,spatial_dim, 1,1.,num_by_chans_.cpu_data(),spatial_sum_multiplier_.cpu_data(),1, bottom_diff); //top_diff - 1/m * (sum(DE/DY)+y*sum(DE/DY.\cdot y)) caffe_cpu_axpby(bottom[0]-&gt;count(),Dtype(1),top_diff, Dtype(-1/(num*spatial_dim)),bottom_diff); // 前面还有常数项 variance_+eps caffe_div(temp_.count(),bottom_diff,temp_.cpu_data(),bottom_diff);&#125; backward的过程也是先求出通道的值，然后广播到整个feature_map,来回两次，然后调用axpby完成 top_diff - 1/m (sum(top_diff)+ysum(top_diff*y)))这里的y针对通道进行。 本文作者： 张峰本文链接：http://www.enjoyai.site/2017/11/06/版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper阅读总结Day1]]></title>
    <url>%2F2017%2F10%2F30%2FPaper%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93%20Day1%2F</url>
    <content type="text"><![CDATA[Paper阅读总结Day11.Convolutional Neural Networks For Facial Expression Recognition文章思想 简单的一篇关于表情识别的文章，运用简单的CNN结构，在文章中对比了深层次的网络结构和浅层次的网络结构的效果，同时将前向的最后一层特征与自己手动提取的Hog特征做了特征融合，并重新训练一个全连接层，得到的效果与不用特征融合效果一致。 文章使用数据集 Fer2013 Database，通过浅层次和深层次的横向对比与 加入hog与不加hog的横向对比 实验效果与结论 深层次的CNN准确率大概是65%，加入HOG与不加效果基本一致，结论是否定了Hog特征融合对表情识别有效果的提升。 2.Island Loss for Learning Discriminative Features in Facial Expression Recognition文章思想 简单的在centerLoss的基础上，添加了衡量各类别类心的loss，由于centerloss只关注了样本到类心的类内距离，而IslandLoss在关注类心距离的同时，添加了类间距离的loss，采用余弦距离衡量类心的相似程度。$$\zeta = \zeta_S+\lambda (\zeta_C+\lambda_1\zeta_{is})$$ 文章使用的数据集 Oulu-CASIA database 、 Extended Cokn-kanada和MMI database，fer2013 实验效果与结论 在各个数据集上的表现都优于SoftmaxLoss以及 CenterLoss+SoftmaxLoss.需要把握各个loss的权重调节 3.End to End Deep Learning for Single Step Real-Time Facial Expression Recognition文章思想 实现一个集合人脸检测与人脸表情分类的一体的网络—Faster-RCNN。替换了Faster-RCNN前面的预训练的网络结构，采取了VGG16和ResNet50，做对比后VGG可以达到10fps，ResNet50-5fps，感觉略有水分。 文章使用数据集 Extended Cokn-kanada 和 FER2013 实验效果与结论 能够在CK+上10折达到94.7的Accuracy 10fps 实际使用基本不可能，RPN的人脸检测稳定性很低 4.Comparative Study of Human Age Estimation Based on Hand-Crafted and Deep Face Features文章思想 自己提取的特征(LBP、Hog、BSIF)以后CNNs提取的特征(VGG-face、Image-VGG-F、VGG16、DEX-IMDB-WIKI and DEX-ChaLearn-ICCV2015 Features)，五个CNNs网络，有包含图像分类，人脸识别，目标检测与年龄预估。实际上就是特征做融合，然后用PLS regression 偏最小二乘法回归分析。 文章使用数据集 MORPH和PAL database 实验效果与结论 实验对比了几种特征单独的实验效果以及crop后的效果，实验说明了最后的回归很重要，然后CNN的特征比这些自己提取的特征好。 本文作者： 张峰本文链接：http://www.enjoyai.site/2017/11/06/版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Facial Expression</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>Facial Expression</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe VisionLayer分析]]></title>
    <url>%2F2017%2F10%2F24%2FCaffe%20VisionLayer%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Caffe VisionLayer 老版本中Caffe有$VisionLayer$,其中主要包含了卷积层，采样层，$im2col$层等，本文将结合自己的理解对这些层次进行分析，在自己学习总结的同事，写下对源码的理解。 $(1) \, im2colLayer$ 为了提高$conv$计算的速度，$caffe$采取了$im2col$的方式，通过对滤波器$kernel$和$feature map$做形式上的改变，从而达到提高计算的作用,因此在进行$im2col$前，必须要知道$kernel$的尺度，卷积方式，输入输出的通道数目以及$batch_size$的大小。1.基本数据成员]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Loss分析]]></title>
    <url>%2F2017%2F10%2F20%2FCaffe%20Loss%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Caffe_Loss 损失函数为深度学习中重要的一个组成部分，各种优化算法均是基于Loss来的，损失函数的设计好坏很大程度下能够影响最终网络学习的好坏。派生于 $LossLayer$,根据不同的Loss层有不同的参数; 1.基本函数 主要包含构造函数，前向、后向以及Reshape，部分有SetUp的函数，每层都有Loss参数123456789101112explicit XXXLossLayer(const LayerParameter&amp; param):LossLayer&lt;Dtype&gt;(param),diff_() &#123;&#125;virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); 2.常用损失函数 通常在训练过程中，采用mini_batch的方式 (1) EuclideanLoss (欧式损失函数，L2损失) $EuclideanLoss$的公式表达为 $loss = \frac{1}{2n}\sum_{i=1}^n{(y_{i}-\hat{y}_{i})^2}$12345678910111213141516171819202122232425262728293031323334353637383940414243444546 //reshape函数，完成层次的reshape,diff_与输入的N*C维度相同 template &lt;typename Dtype&gt; void EuclideanLossLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::Reshape(bottom,top);//先调用基类的Reshape函数 CHECK_EQ(bottom[0]-&gt;count(1),bottom[1]-&gt;count(1));//label类别 diff_.Reshape(*bottom[0]);//一般是N*C*1*1 &#125; // Forward_cpu 前向 主要计算loss template &lt;typename Dtype&gt; void EuclideanLossLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const int count = bottom[0]-&gt;count(); caffe_sub(count, bottom[0]-&gt;cpu_data(),//网络的输出 N*C bottom[1]-&gt;cpu_data(),//对应label N*C diff_.mutable_cpu_data()//对应的loss差分 );//完成 y_&#123;predicy&#125;-y_&#123;label&#125; //bottom[0]-bottom[1] Dtype dot = caffe_cpu_dot(count,diff_.cpu_data(),diff_.cpu_data()); //bottom[0]-&gt;num()== bottom[0].shape(0); Dtype loss = dot/bottom[0]-&gt;num()/Dtype(2);//loss/(2*n) top[0]-&gt;mutable_cpu_data()[0] = loss; &#125;//Backward_cpu f'(x) = 1/n*(y_&#123;predict&#125;-y_&#123;label&#125;)template &lt;typename Dtype&gt;void EuclideanLossLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp;propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; for (size_t i = 0; i &lt; 2; i++) &#123; if (propagate_down[i]) &#123;//需要backward //对应predict-label 如果label为bottom[0]就需要乘以-1 const Dtype sign = (i==0) ? 1 : -1; //top[0]-&gt;cpu_diff()返回float* length = 1;下式为loss/n; const Dtype alpha = sign*top[0]-&gt;cpu_diff()[0]/bottom[0]-&gt;num(); //y = ax+by ; caffe_cpu_axpby(bottom[0]-&gt;count(),//count alpha,// loss/n diff_.cpu_data(),//y_&#123;predict&#125;-y_&#123;label&#125; Dtype(0), bottom[i]-&gt;mutable_cpu_diff() );//1/n*loss*(y_&#123;predict&#125;-y_&#123;label&#125;) &#125; &#125; //欧式损失函数形式简单，常用于做回归分析，做分类需要统一量纲。&#125; (2)SoftmaxWithLoss Softmax损失函数$\qquad softmax函数将输出的各个类别的概率值进行归一化，生成各个类别的prob$$\qquad 常用的分类损失函数，Softmax输出与Multinomial Logistic Loss的结合。公式如下:$$$ y_i = softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}}$$$$loss = -log(y_k) ,k为实际的样本label$$$\qquad 损失函数的推导:\frac{\partial Loss}{\partial x_i}=\sum_{j=1}^{n}{\frac{\partial loss}{\partial y_j}*\frac{\partial y_j}{\partial x_i}}=-\frac{1}{y_k}*\frac{\partial y_k}{\partial x_i} \quad k为实际的label,其他的\frac{\partial loss}{\partial y_j} =0 \\$$$\qquad \frac{\partial y_k}{\partial x_i} = \frac{\partial softmax(x_k)}{\partial x_i}=\begin{cases} y_k*(1-y_k) \qquad k == i \\\\\ -y_k*y_i \qquad \qquad k \,\,!=\,i\end{cases}$$$$整理后可以发现\frac{\partial loss}{\partial x_i}=\begin{cases} y_k-1 \qquad k \,== \,i ，即i为实际label\\\\\ y_i \qquad \qquad k \,\,!=\,i,即i不是实际label\end{cases}$$ 具体代码的实现如下所示:1.SoftmaxWithLossLayer的输入:bottom123456789// bottom[0]为前层的特征输出，一般维度为N*C*1*1// bottom[1]为来自data层的样本标签，一般维度为N*1*1*1;// 申明const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom;//backward部分代码Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();const Dtype* prob_data = prob_.cpu_data();caffe_copy(prob_.count(), prob_data, bottom_diff);const Dtype* label = bottom[1]-&gt;cpu_data();//label 2.SoftmaxWithLossLayer层的输出:top123456789// SoftmaxWithLossLayer的输出其实就是1*1*1*1的最终loss// 如果有多个的话实际就是也会保存softmax的输出，但是需要注意的是内部包含了//Softmax的FORWAR过程，产生的概率值保存在prob_内const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top;//forward部分代码 ,top[0]-&gt;mutable_cpu_data()[0] = loss / get_normalizer(normalization_, count);if (top.size() == 2) &#123; top[1]-&gt;ShareData(prob_);//top[1]保存softmax的前向概率&#125; 3.SoftmaxWithLossLayer的关键变量: $softmax_top_vec_,prob_$ 记录中间值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;/// prob stores the output probability predictions from the SoftmaxLayer.Blob&lt;Dtype&gt; prob_;/// bottom vector holder used in call to the underlying SoftmaxLayer::Forwardvector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;/// top vector holder used in call to the underlying SoftmaxLayer::Forwardvector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;/// Whether to ignore instances with a certain label.bool has_ignore_label_;/// The label indicating that an instance should be ignored.int ignore_label_;/// How to normalize the output loss.LossParameter_NormalizationMode normalization_;int softmax_axis_, outer_num_, inner_num_;//softmax的输出与Loss的维度template &lt;typename Dtype&gt;void SoftmaxWithLossLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::Reshape(bottom,top);//先调用基类的reshape softmax_layer_-&gt;Reshape(softmax_bottom_vec,softmax_top_vec_); int axis = this-&gt;layer_param_.softmax_param().axis();//softmaxproto参数(1) softmax_axis_ = bottom[0]-&gt;CanonicalAxisIndex(axis);//正不变负倒数 outer_num_ = bottom[0]-&gt;count(0,softmax_axis_);// N mini_batch_size inner_num_ = bottom[0]-&gt;count(softmax_axis_+1);// H*W 一般为1*1 //保证outer_num_*inner_num_ = bottom[1]-&gt;count();//bottom[1]为label N if (top.size() &gt;= 2) &#123;//多个top实际上是并列的，prob_值完全一致 top[1]-&gt;Reshapelike(*bottom[0]); &#125;&#125;//forward是一个计算loss的过程，loss为-log(p_label)//由于softmaxWithLoss包含了Softmax所以需要经过Softmax的前向，并得到每个类别概率值template &lt;typename Dtype&gt;void SoftmaxWithLossLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; //调用Softmax的前向 softmax_layer_-&gt;Forward(softmax_bottom_vec_,softmax_top_vec_); //这里等同于softmax_top_vec_[0]-&gt;cpu_data(); const Dtype* prob_data = prob_.cpu_data(); const Dtype* label = bottom[1]-&gt;cpu_data();//label 一般来自Data层 // 一般是N*C(n个样本，每个C个预测概率)/ N == 类别数目 int dim = prob_.count()/out_num_; int count = 0;//统计实际参与loss的样本个数 Dtype loss = 0; for (size_t i = 0; i &lt; outer_num_; i++) &#123;//每个样本遍历 for (size_t j = 0; j &lt; inner_num_; j++) &#123; //可以认为j == 0 绝大多数成立 const int label_value = static_cast&lt;int&gt;(label[i*inner_num_+j]); if(has_ignore_label_ &amp;&amp; label_value == ignore_label_)&#123; // softmaxLayer的参数，可以选择不参与loss的类别 continue; &#125; else&#123;//实际需要判断label_value &gt; 0 ,&lt; prob_.shape(1) // -= 因为loss = -log(p_label),prob_data 是n*c的 loss -= log(std::max(prob_data[i*dim+label_value*inner_num_+j)], Dtype(FLT_MIN)));//防止溢出或prob出现NAN ++count; &#125; &#125; &#125; //全部样本遍历完成后，可以进行归一，其实也挺简单， // top[0]-&gt;mutable_cpu_data[0] = loss/归一化&#125;// Backward_cpu,这里的Backward实际需要更新的是softmax的输入接口的数据，// 中间有个y的转化，具体公式上面已经写出// bottom_diff = top_diff * softmaxWithloss' = top_diff * &#123;p -1 或者 p&#125;template &lt;typename Dtype&gt;void SoftmaxWithLossLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; //fc输出与label的位置固定了，因此不需要如同欧式loss去判断label和fc的输入位置 if (propagate_down[1]) &#123; //label不需要backpropagate &#125; if (propagate_down[0]) &#123;//输入，需要更新 Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();//需要修改的 const Dtype* prob_data = prob_.cpu_data();//N*C //这里把diff先确定为softmax输出的y值，即bottom_diff[t] = y_t ; caffe_copy(prob_.count(),prob_data,bottom_diff); const Dtype* label = bottom[1]-&gt;cpu_data(); // 也可以替换为bottom[1]-&gt;count(),实际就是类别C int dim = prob_.count()/ outer_num_;//NC/C == N int count = 0; for (size_t i = 0; i &lt; outer_num_; i++) &#123; //n个样本 for (size_t j = 0; j &lt; inner_num_; j++) &#123; // 实际j == 0 const int label_value = static_cast&lt;int&gt;(label[i*inner_num_+j]); if (has_ignore_label_ &amp;&amp; label_value == ignore_label_) &#123; //正好是忽略loss的类别 bottom_diff[i*dim+label_vale*inner_num_+j] = 0; &#125; else&#123; //这里需要考虑为什么，实际上之前所有的diff初始为y_t， //根据softmax的偏导知道真实label是y_t -1; bottom_diff[i*dim+label_vale*inner_num_+j] -= 1; ++count; &#125; &#125; &#125; //这里只完成了loss的一部分，还差top_diff即Loss //如果归一化，就进行归一，同cpu_forward //cpu_diff可以认为是Loss // Dtype loss_weight = top[0]-&gt;cpu_diff()[0]/归一化 caffe_scal(prob_count(),loss_weight,bottom_diff); &#125;&#125; (3) SmoothL1Loss (RCNN提出的Loss) $SmoothL1Loss$为欧式均方误差的修改版，为分段函数，对离散点不敏感,具体的公式如下:$$SmoothL1Loss(x) =\begin{cases} 0.5*(sigma*x)^2 \qquad 其他\\ \left|x\right|-0.5/sigma^2 \qquad \left|x\right| &lt; 1./sigma^2\end{cases}$$整体的公式为:$x_{new} = x_{input}*w_{in},output = w_{out}*SmoothL1loss(x_{new});$1.基本的数据类型和意义:12345Blob&lt;Dtype&gt; diff_;// y_Blob&lt;Dtype&gt; error_;//lossBlob&lt;Dtype&gt; ones_;bool has_weights_; // weight权值Dtype sigma2_ ;// sigma 默认为1，此处sigma2_ = sigma*simga; 2.基本的功能函数 基本包含了LayerSetup Reshape Forward 和 Backward四个函数,具体实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687//构建layer层次,SmoothL1LossLayer的参数有sigma，默认为1template &lt;typename Dtype&gt;void SmoothL1LossLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; SmoothL1LossParameter loss_param = this-&gt;layer_param_.smooth_l1_loss_param(); sigma2_ = loss_param.sigma()*loss_param.sigma(); has_weights_ = (bottom.size() &gt;= 3);//bottom[3]---为weights if (has_weights_) &#123; //bottom[3] == out_weight;//w_out //bottom[2] == in_weight;// w_in &#125;&#125;// Reshape 根据输入输出调节结构，计算过程进行了拆分template &lt;typename Dtype&gt;void SmoothL1LossLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::Reshape(bottom,top);//基函数 //这里判断参数维度, if (has_weights_) &#123; CHECK_EQ(bottom[0]-&gt;count(1) == bottom[2]-&gt;count(1) == bottom[3].count(1)) ;//w_in和w_out的权值 &#125; diff_.Reshape(bottom[0].shape());// diff_ = w_in*(bottom[0]-bottom[1]); error_.Reshape(bottom[0].shape());// error_ = w_out*smoothL1(w_in*diff_); ones_.Reshape(bottom[0].shape());// one_ = error_*w_out; for (size_t i = 0; i &lt; ones_-&gt;count(); i++) &#123; one_s.mutable_cpu_data()[i] = Dtype(1); &#125;&#125;// Forward过程，一步一步操作template &lt;typename Dtype&gt;void SmoothL1LossLayer&lt;Dtype&gt;::Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; int count = bottom[0]-&gt;count(); //bottom[0]和bottom[1]不确定标签和特征的顺序 caffe_gpu_sub( // 计算diff_ = bottom[0]-bottom[1]; count, bottom[0]-&gt;gpu_data(), bottom[1]-&gt;gpu_data(), diff_.mutable_cpu_data() ); if (has_weights_) &#123; x_new = x_input*in_weight,xinput==diff_ caffp_gpu_mul( count, bottom[2]-&gt;gpu_data(), diff_.gpu_data(), diff_.mutable_gpu_data(); ); &#125; //此处为SmoothL1的函数前向过程GPU实现 SmoothL1Forward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;( count, diff_.gpu_data(), errors_.mutable_gpu_data(), sigma2_); CUDA_POST_KERNEL_CHECK; if (has_weights_) &#123; //x_out= SmoothL1(w_in*x_input) * w_out caffe_gpu_mul( count, bottom[3]-&gt;gpu_data(), error_.gpu_data(), error_.mutable_gpu_data(); ); // error _ = w_out* error_ &#125; Dtype loss; caffe_gpu_dot(count,ones_.gpu_data().error_gpu_data(),&amp;loss);//类似于asum top[0]-&gt;mutable_gpu_data()[0] = loss/bottom[0]-&gt;num();// mini_batch&#125;// GPU的实现SmoothL1loss,根据公式实现即可template &lt;typename Dtype&gt;__global__ void SmoothL1Forward(const int n, const Dtype* in, Dtype* out,Dtype sigma2) &#123;// f(x) = 0.5 * (sigma * x)^2 if |x| &lt; 1 / sigma / sigma// |x| - 0.5 / sigma / sigma otherwise CUDA_KERNEL_LOOP(index, n) &#123; //for loop Dtype val = in[index]; Dtype abs_val = abs(val); if (abs_val &lt; 1.0 / sigma2) &#123; out[index] = 0.5 * val * val * sigma2; &#125; else &#123; out[index] = abs_val - 0.5 / sigma2; &#125; &#125;&#125; 反向过程中根据求导公式可以得到如下式子，Backward的过程也如下所示$$\frac{\partial Loss}{\partial x} = w_{in}*w_{out}*\frac{\partial SmoothL1(x)}{\partial x}$$ cpu版本可以自己实现，只需要把$GPU_data_diff$换成$cpu$,以及$gpu$的$smoothL1$写成$CPU$的即可。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 //backward过程，根据导函数 // f'() template &lt;typename Dtype&gt; void SmoothL1LossLayer&lt;Dtype&gt;::Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; int count = diff_.count(); // 反向即公式的smoothL1的偏导 SmoothL1Backward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS &gt;&gt;&gt;( count, diff_.gpu_data(), diff_.mutable_gpu_data(), sigma2_); CUDA_POST_KERNEL_CHECK; //此处的循环loop如同欧式损失函数，因为无法确认bottom[0]和bottom[1]，fc和label //的顺序，forward默认是0-1，因此如果0为label，则sign = -1; for (size_t i = 0; i &lt; 2; i++) &#123; if (propagate_down[i]) &#123; const Dtype sign = (i == 0) ? 1:-1;//代码默许了label为bottom[1] //sign* loss/n; const Dtype alpha = sign*top_diff-&gt;gpu_diff()[0]/bottom[i]-&gt;num(); //smoothL1输入的是diff_.gpu_data() caffe_cpu_axpby( count, alpha, diff_.gpu_data(),//此处的data已经是SmoothL1返回的导数了 Dtype(0), bottom[i]-&gt;mutable_gpu_diff() ); if (has_weights_) &#123; caffe_gpu_mul( count, bottom[2]-&gt;gpu_data(), bottom[i]-&gt;gpu_diff(), bottom[i]-&gt;mutable_gpu_diff() ); 乘以了内层的weight caffe_gpu_mul( count, bottom[3]-&gt;gpu_data(), bottom[i]-&gt;gpu_diff(), bottom[i]-&gt;mutable_gpu_diff() ); 乘以了外层的weight &#125; &#125; &#125; &#125; template &lt;typename Dtype&gt;__global__ void SmoothL1Backward(const int n, const Dtype* in, Dtype* out, Dtype sigma2) &#123; // f'(x) = sigma * sigma * x if |x| &lt; 1 / sigma / sigma // = sign(x) otherwise CUDA_KERNEL_LOOP(index, n) &#123; Dtype val = in[index]; Dtype abs_val = abs(val); if (abs_val &lt; 1.0 / sigma2) &#123; out[index] = sigma2 * val; &#125; else &#123; out[index] = (Dtype(0) &lt; val) - (val &lt; Dtype(0));//1或者-1 &#125; &#125; &#125; cpu版本的SmoothL1前向和后向实现如下,cpu版本速度过慢，不建议使用123456789101112131415161718192021222324252627282930 //前向 替换前向GPU中一部分 const Dtype* in = diff_.cpu_data(); Dtype* out = errors_.mutable_cpu_data(); for (size_t i = 0; i &lt; diff_.count(); i++) &#123; Dtype val = in[index]; Dtype abs_val = abs(val); if(abs_val &lt; 1.0 / sigma2_)&#123; out[index] = 0.5 * val * val * sigma2_; &#125; else&#123; out[index] = abs_val - 0.5 / sigma2_; &#125; &#125; //反向，替换反向GPU的一部分 const Dtype* in = diff_.cpu_data(); Dtype* out = diff_.mutable_cpu_data(); for (size_t i = 0; i &lt; diff_.count(); i++) &#123; Dtype val = in[index]; Dtype abs_val = abs(val); if(abs_val &lt; 1.0 / sigma2_)&#123; out[index] = sigma2_ * val; &#125; else&#123; out[index] = (Dtype(0) &lt; val) - (val &lt; Dtype(0)); &#125; &#125;// smoothL1在目标检测的时候效果良好，由于多损失函数以及回归点的变换，bottom[2]和// bottom[3]基本都存在，由于其函数特性，对偏远的点不敏感，因此可以替换L2loss (4) SigmoidCrossEntropyLoss (交叉熵) 交叉熵应用广泛，常作为二分类的损失函数，在$logistic$中使用，由于$sigmoid$的函数的输出特性，能够很好的以输出值代表类别概率。具体的公式如下所示: $$loss = -\frac{1}{n}\sum_{1}^{n}(\hat{p_i}*log(p_i)+(1-\hat{p_i})*log(1-p_i)))$$$$p_i = \frac{1}{1.+exp(-x_i)}$$$$ \frac{\partial loss}{\partial x_i} = -\frac{1}{n}*\sum_{i=1}^{n}((\hat{p_i}*\frac{1}{p_i}*p_i*(1-p_i)-(1-\hat{p_i})*\frac{1}{1-p_i}*(1-p_i)*p_i)) $$$$= -\frac{1}{n}\sum_{i=1}^{n}(\hat{p_i}-p_i)$$1.基本的数据成员1234shared_ptr&lt;SigmoidLayer&lt;Dtype&gt;&gt;sigmoid_layer_;//layer参数shared_ptr&lt;Blob&lt;Dtype&gt; &gt; sigmoid_output_; // sigmoid输出的值N*C C一般==1shared_ptr&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_bottom_vec_;// sigmoid函数的输入xshared_ptr&lt;Blob&lt;Dtype&gt;* &gt; sigmoid_top_vec_;// sigmoid函数的输出 2.基本的成员函数 基本的成员函数为LayerSetup，Reshape ,Forward和Backward,实现如下:12345678910111213141516171819//构建layer 中间有sigmoid函数过度，所以如同softmaxLoss类似过程template &lt;typename Dtype&gt;void SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::LayerSetup( const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::LayerSetup(bottom,top); sigmoid_bottom_vec_.clear(); sigmoid_bottom_vec_.push_back(bottom[0]); sigmoid_top_vec_.clear(); sigmoid_top_vec_.push_back(sigmoid_output_.get());//sigmoid的输出 sigmoid_layer_-&gt;Setup(sigmoid_bottom_vec_,sigmoid_top_vec_);&#125;//Reshape函数 比较简单template &lt;typename Dtype&gt;void SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Reshape( const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::Reshape(bottom,top);//步骤1 sigmoid_layer_-&gt;Reshape(sigmoid_bottom_vec_,sigmoid_top_vec_);//步骤2&#125; 这里Caffe实现的前向计算代码与公式有差异，具体原因如下$\qquad \hat{p}*log(p)+(1-\hat{p})*log(1-p) \\\qquad \,= \hat{p}*log(\frac{1}{1+e^{-x}})+(1-\hat{p})*log(\frac{e^{-x}}{1+e^{-x}}) \\\qquad =\hat{p}*log(\frac{1}{1+e^{-x}})-\hat{p}*log(\frac{e^{-x}}{1+e^{-x}})+log(\frac{e^{-x}}{1+e^{-x}}) \\\qquad =\hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})$ 当$e^{-x}很大时, \frac{e^{-x}}{1+e^{-x}}$ 计算不准确，因此采用下种计算方式,当 $x&lt;0$时,分子分母同时乘以$e^{x}$,有: $$\frac{e^{-x}}{1+e^{-x}}=\begin{cases} \frac{e^{-x}}{1+e^{-x}} \qquad x\ge0\\ \frac{1}{1+e^{x}} \qquad \,\,\, x&lt;0\end{cases}$$ 从而得到:$$\hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})=\begin{cases} \hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}}) = (\hat{p}-1)*x-log(1+e^{-x}) \quad x\ge0\\ \hat{p}*x+log(\frac{e^{-x}}{1+e^{-x}})=\hat{p}*x-log(1+e^{x}) \quad\quad \qquad x&lt;0\end{cases}$$ 12345678910111213141516171819202122232425262728293031323334353637383940// Forward_cpu 前向函数，分布保存临时值，得到losstemplate &lt;typename Dtype&gt;void SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Forward_cpu( const vector&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; sigmoid_bottom_vec_[0] = bottom[0];//这一步多余，setup时已经保持一致了 sigmoid_layer_-&gt;Forward_cpu(sigmoid_bottom_vec_,sigmoid_top_vec_);//Sigmoid const int count = bottom[0]-&gt;count();//N*1*1*1，输出一个概率值为预测1的 const int num = bottom[0]-&gt;num(); const Dtype* input_data = bottom[0]-&gt;cpu_data(); const Dtype* target = bottom[1]-&gt;cpu_data();//真实label Dtype loss = 0; for (size_t i = 0; i &lt; count; i++) &#123;//遍历mini_batch loss -= input_data[i]*(target[i]-(input_data[i]&gt;=0))- log(1.+exp(input_data[i]-2*(input_data[i]&gt;=0))); &#125; top[0]-&gt;mutable_cpu_data()[0] = loss/num;//mini_batch&#125;//backward的反向更新比较简单，-(target-predict)template &lt;typename Dtype&gt;void SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Backward_cpu( const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom)&#123; if (propagate_down[1]) &#123; //label 不需要更新 &#125; if (propagate_down[0]) &#123; const int count = bottom[0]-&gt;count();//N*1*1*1 const int num = bottom[0]-&gt;num();// N const Dtype* sigmoid_output_data = sigmoid_output_.cpu_data();//预测值 const Dtype* target = bottom[1]-&gt;cpu_data(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); // bottom_diff = predict - target_label caffe_sub(count,sigmoid_output_data,target,bottom_diff); const Dtype loss_weight = top[0]-&gt;cpu_diff()[0]; //bottom_diff = bottom_diff*loss_weight/n caffe_scal(count,loss_weight/num,bottom_diff); &#125;&#125; (5) CenterLoss (ECCV2016) ECCV2016年提出的新loss，让softmax能够训练出更好的内聚性的特征，思路比较简单，在SoftmaxLoss的基础上，添加了一个新的loss，Loss的表达式: $$\zeta_C = \frac{1}{2}*\sum_{i=1}^{n}||x_i-c_{yi}||_2^2$$ 思路比较好理解，增加一个loss衡量样本特征与该类类心的距离，更新的公式如下: $$\frac{\partial \zeta_c}{\partial x_i} = x_i - c_{yi} \\ \triangle c_j = \frac{\sum_{i=1}^{n}\delta{(y_i=j)}*(c_j-x_i)}{1+\sum_{i=1}^{n}\delta{(y_i=j)}}$$ $$c_j^{t+1} = c_j^t-\alpha*\triangle{c_j^t}$$ 第二步骤类心特征更新仅仅更新当前样本所属的类别，分母加1为了防止分母为0，因此和softmax整合后整体的Loss如下所示： $$\zeta = \zeta_S+\lambda \zeta_C$$ 1.基本数据成员12345678//基本数据用以保存center_Loss的layer paramsint N_;// 对应params的num_output,分类类别int K_;// 对应fc层的输出特征,int M_;// 对应于batch_sizeBlob&lt;Dtype&gt; distance_;//样本与类心的距离，distance为x - x_center重点Blob&lt;Dtype&gt; variation_sum_;// distance的负数， x_center- xBlob&lt;Dtype&gt; count_; // 类心的个数string distance_type_; // 距离的衡量 默认L2 2.基本的成员函数 与一般的Loss层一样，有LayerSetup,Reshape,Forward,Backward,具体实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116// layersetup过程，center是N个中心，每个类心feature长度Ktemplate &lt;typename Dtype&gt;void CenterLossLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; CenterLossParameter loss_param = this-&gt;layer_param_.center_loss_param(); N_ = loss_pram.num_output();//分类的类别，类心的个数,prototxt内设置 distance_type_ = loss_pram.distance_type(); const int axis = bottom[0]-&gt;CanonicalAxisIndex(loss_pram.axis()); K_ = bottom[0].count(axis);//axis 默认为1，K_= fc*1*1,特征的长度 M_ = bottom[0]-&gt;num(); // batch_size的大小 if (this-&gt;blobs_.size() &gt; 0) &#123; //层内无参数. &#125; else&#123; this-&gt;blobs_.resize(1);//这里放center，各个类别的fc中心 vector&lt;int&gt; center_shape(2); center_shape[0] = N_; center_shape[1] = K_; // 代表中心是N个中心，每个中心的feature长度为K_ this.blobs_[0].resize(new Blob&lt;Dtype&gt;(center_shape)); // 初始中心的填充方式 shared_ptr&lt;Filler&lt;Dtype&gt;&gt;center_filler(GetFiller&lt;Dtype&gt;( loss_param.center_filler())); ) center_filler-&gt;Fill(this-&gt;blobs_[0].get()); &#125; this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(),true);//类心也更新&#125;// Reshape函数template &lt;typename Dtype&gt;void CenterLossLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt; &amp;bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; LossLayer&lt;Dtype&gt;::Reshape(bottom,top); distance_.ReshapeLike(*bottom[0]);//bottom长度为N_*K_ variation_sum_.ReshapeLike(*this-&gt;blobs_[0]);//一样的N_*K_ vector&lt;int&gt;count_reshape(1); count_reshape[0]= N_; count_.Reshape(count_reshape);//N_类心的个数&#125;//Forward_cpu ，得到loss// N_类别数，K_特征长度,M_mini_batch的样本个数template &lt;typename Dtype&gt;void CenterLossLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data();//N_*K_ const Dtype* label = bottom[1]-&gt;cpu_data();//N_*1; const Dtype* center = this-&gt;blobs_[0]-&gt;cpu_data();//N_K_ Dtype* distance_data = distance_.mutable_cpu_data();// // i-t样本的距离 for (size_t i = 0; i &lt; M_; i++) &#123; const int label_value = static_cast&lt;int&gt;(label[i]);//真是的样本类别 //对应特征相减，用fc特征减去该类的类心，保存在distance_data上 caffe_sub(K_,bottom+i*K_,center+label_value*K_,distance_data+i*K_); &#125; Dtype dot; Dtype loss; if (distance_type_ == "L1") &#123; //L1 loss,distance_ sum即可 // 也可以写caffe_cpu_asum(M_*K_,distance_data); dot = caffe_cpu_asum(M_*K_,distance_.cpu_data()); loss = dot/M_; &#125; //L2,loss,distance_data*distance_data,然后M_样本sum else if(distance_type_ == "L2")&#123; dot = caffe_cpu_dot(M_*K_,distance_.cpu_data(),distance_.cpu_data()); loss = dot/M_/Dtype(2); &#125; else&#123; //不支持其他的距离衡量 &#125; top[0]-&gt;mutable_cpu_data()[0] = loss;&#125;// Backward_cpu,更新data和center，template &lt;typename Dtype&gt;void CenterLossLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; if (this-&gt;param_propagate_down_[0]) &#123;//表示更新类心 const Dtype* label = bottom[1]-&gt;cpu_data(); Dtype* center_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff(); Dtype* variation_sum_data = variation_sum_.mutable_cpu_data(); int* count_data = count_.mutable_cpu_data(); const Dtype* distance_data = distance_.cpu_data();//fc_center-fc_pre if (distance_type_ == "L1") &#123; caffe_cpu_sign(M_*K_,distance_data,distance_.mutable_cpu_data()); &#125; caffe_set(N_*K_,Dtype(0),variation_sum_.mutable_cpu_data()); caffe.set(N_,0,count_.mutable_cpu_data());//统计每个类别的个数 for (size_t i = 0; i &lt; M_; i++) &#123;//样本循环 const int label_value = static_cast&lt;int&gt;(label[i]); //variation_sum_data 初始为0，distance保存的即使x_i-x_center caffe_sub(K_,variation_sum_data+label_value*K_, distance_data+i*K,variation_sum_data+label_value*K); count_data[label_value]++: &#125; for (size_t i = 0; i &lt; M_; i++) &#123; const int label_value = static_cast&lt;int&gt;(label[0]); //1/(count+1)*(x_center-x_i) caffe_cpu_axpby(K_,Dtype(1)/(count_data[label_value]+1), variation_sum_data+label_value*K,1.,center_diff+label_value*K_); &#125; &#125; //类心更新完成后,跟新x if (propagate_down[0]) &#123;//更新输入x //loss * 1/M * (x - x_center) caffe_copy(M_*K_,distance.cpu_data(),bottom[0]-&gt;mutable_cpu_diff()); cafe_scal(M_*K_,top[0]-&gt;cpu_diff()[0]/M_, bottom[0]-&gt;mutable_cpu_diff()); &#125; if (propagate_down[1]) &#123; // label不更新 &#125;&#125; $CenterLoss$在多分类上较$Softmax$有提高，$loss _weight$的设置可以确定$center _loss$和$softmaxloss$的比重，能够很有效的使得网络能够最小化类内距离，加大区分度。 本文作者： 张峰本文链接：https://zhanglaplace.github.io/2017/10/20版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe NeuronLayer分析]]></title>
    <url>%2F2017%2F10%2F20%2FCaffe%20%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0(Activation)%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Caffe_NeuronLayer 一般来说，激励层的输入输出尺寸一致，为非线性函数，完成非线性映射，从而能够拟合更为复杂的函数表达式激励层都派生于NeuronLayer: class XXXlayer : public NeuronLayer 1.基本函数 激励层的基本函数较为简单，主要包含构造函数和前向、后向函数1234567891011explicit XXXLayer(const LayerParameter&amp; param) :NeuronLayer&lt;Dtype&gt;(param)&#123;&#125;virtual inline const char* type() const &#123; return "layerNane"; &#125;virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); 2.常用$Neuron$层(1) Relu/PRelu Rectufied Linear Units ReLU的函数表达式为 $f(x) = x*(x&gt;0) + negative_slope*x*(x &lt;= 0)$ 具体实现如下1234567891011121314151617181920212223242526272829303132 //forward_cpu template &lt;typename Dtype&gt; void ReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; // 根据bottom求解top const Dtype* bottom_data = bottom[0]-&gt;cpu_data();//const 不可修饰 Dtype* top_data = top[0]-&gt;mutable_cpu_data();//可修饰 const int count = bottom[0]-&gt;count();//因为count_一致，也可用top Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope(); for (size_t i = 0; i &lt; count; i++) &#123; top_data[i] = bottom_data[i]*(bottom_data[i] &gt; 0) + negative_slope*bottom_data[i]*(bottom_data[i] &lt;= 0); &#125; &#125; //Backward_cpu // 导数形式 f'(x) = 1 x&gt;0 ; negative_slope*x x&lt;0 template &lt;typename Dtype&gt; void ReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; const Dtype* top_diff = top[0].cpu_diff();//top diff const Dtype* bottom_data = bottom[0].cpu_data();//用以判断x是否大于0 Dtype* bottom_diff = bottom[0].cpu_diff();//bottom diff const int count = bottom[0].count(); for (size_t i = 0; i &lt; count; i++) &#123; bottom_diff[i] = top_diff[i]*(bottom_data[i] &gt; 0) +negative_slope*(bottom_data[i] &lt;= 0); &#125; &#125;// Relu 函数形式简单，导函数简单，能有效的解决梯度弥散问题，但是当x小于0时，易碎// 但是网络多为多神经元，所以实际应用中不会影响到网络的正常训练。 (2) Sigmoid (S曲线) Sigmoid函数表达式为$f(x) = 1./(1+exp(-x))$;值域0-1，常作为BP神经网络的激活函数由于输出为0-1，也作为logistic回归分析的概率输出函数。具体实现如下;1234567891011121314151617181920212223242526272829303132333435 //定义一个sigmoid函数方便计算 template &lt;typename Dtype&gt; inline Dtype sigmoid(Dtype x)&#123; return 1./(1.+exp(-x)); &#125; //前向 直接带入sigmoid函数即可 template &lt;typename Dtype&gt; void SigmoidLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data();//需要计算 const int count = bottom[0]-&gt;count();//N*C*H*W; for (size_t i = 0; i &lt; count; i++) &#123; top_data[i] = sigmoid(bottom_data[i]); &#125; &#125; //Backward_cpu 由于f'(x) = f(x)*(1-f(x))，所以需要top_data // bottom_diff = top_diff*f'(bottom_data) = top_diff*top_data*(1-top_data) template &lt;typename Dtype&gt; void SigmoidLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; const Dtype* top_diff = top[0]-&gt;cpu_diff(); const Dtype* top_data = top[0]-&gt;cpu_data(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); //需要计算 const int count = bottom[0]-&gt;count(); for (size_t i = 0; i &lt; count; i++) &#123; //top_data[i] == sigmoid(bottom_data[i]); bottom_diff[i] = top_diff[i]*top_data[i]*(1.-top_data[i]); &#125; &#125;// Sigmoid函数可以作为二分类的概率输出，也可以作为激活函数完成非线性映射，但是网络// 增加时，容易出现梯度弥散问题，目前在CNN中基本不使用 (3)TanH,双正切函数 TanH函数的表达式为 $f(x) =\frac{(1.-exp(-2x))}{(1.+exp(-2x))}$;值域0-1,与sigmoid函数有相同的问题,但是TanH在RNN中使用较为广泛,理由参考，具体实现如下所示。 12345678910111213141516171819202122232425262728293031//定义一个tanH的函数表达式,实际已经封装inline Dtype TanH(Dtype x)&#123; return (1.-exp(-2*x))/(1.+exp(-2*x));&#125;//Forward_cputemplate &lt;typename Dtype&gt;void TanHLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); const int count = bottom[0]-&gt;count(); for (size_t i = 0; i &lt; count; i++) &#123; top[i] = TanH(bottom_data[i]); &#125;&#125;//Backward_cpu f'(x) = 1-f(x)*f(x);// bottom_diff = top_diff(1-top_data*top_data);template &lt;typename Dtype&gt;void TanHLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; const Dtype* top_diff = top[0]-&gt;cpu_diff(); const Dtype* top_data = top[0]-&gt;cpu_data(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); //需要计算 const int count = bottom[0]-&gt;count(); for (size_t i = 0; i &lt; count; i++) &#123; //top_data[i] == TanH(bottom_data[i]); bottom_diff[i] = top_diff[i]*(1.-top_data[i]*top_data[i]); &#125;&#125; 其他的激励函数就不在枚举，可以查看具体的caffe源码，实现大致相同 3.说明(1) 梯度弥散和梯度爆炸 网络方向传播时，loss经过激励函数会有$loss*\partial{f(x)}$,而如sigmoid的函数，max($\partial{f(x)}$)只有1/4因此深层网络传播时loss越来越小，则出现前层网络未完整学习而后层网络学习饱和的现象 (2) Caffe激励层的构建 如上述的代码所示，激励层主要完成forward和Bacward的函数实现即可，由构建的函数表达式推导出它的导函数形式，弄懂bottom_data,top_data,bottom_diff,top_diff即可 本文作者： 张峰本文链接：https://zhanglaplace.github.io/2017/10/20%E5%88%86%E6%9E%90/)版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe CommonLayer分析]]></title>
    <url>%2F2017%2F10%2F20%2FCaffe%20CommonLayer%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Caffe CommonLayer分析 $Caffe$中包含了很多通用的功能层，包含了$concat$,$slice$,$split$,$crop$,$flip$,$scale_layer$等,这些层在网络中经常被使用，本文也将对其中的常见layer进行说明与源码分析。 1.常用$Layer$(1) $CropLayer$ CropLayer完成数据的裁剪，输入两个 $bottom,bottom[0]$ 为原始数据，$bottom[1]$ 为裁剪后的输出尺寸，输出 $top[0]$ 为裁剪后的数据，尺寸与 $bottom[1]$ 相同，其中有$axis 控制裁剪的起始轴,offset表示对应裁剪轴的起始位置。举例说明：$$bottom[0]的shape:[32,64,512,512],bottom[1]的shape:[32,32,256,256] \\axis = 1 \qquad offset = [:,16,128,128] \\则:top[1]的为bottom[0][:,16+bottom[1].shape(1),128+bottom[1].shape(2),128+bottom[1].shape(3)))$$下面会进行具体的代码解释说明 1.基本成员变量 基本成员变量，记录开始的axis和每个shape的起始偏移 12vector&lt;int&gt;offsets_;int axis; 2.基本成员函数 基本成员函数包括LayerSetup,Reshape,forward,Backward,crop_copy，具体实现如下12345678910111213141516171819202122232425262728293031323334353637383940414243//LayerSetup 主要完成proto的参数提取过程template &lt;typename Dtype&gt;void CropLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const CropParameter&amp; crop_param = this-&gt;layer_param_.crop_param(); CHECK_EQ(bottom.size(),2);//必须是2个 int input_dim = bottom[0]-&gt;num_axes();// 一般为4， 即shape_.size() const int start_axis = bottom[0]-&gt;CanonicalAxisIndex(crop_param.axis()); // 这里的axis要判断是否小于Input_dim if (crop_param.offset_size() &gt; 1) &#123; // offset_size() == offset.size() CHECK_EQ(start_axis+crop_param.offset_size(),input_dim); //保证起始后的axis均有offset &#125;&#125;// Reshape,确定offsets和crop_size的尺寸template &lt;typename Dtype&gt;void CropLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const CropParameter param = this-&gt;layer_param_.crop_param(); const int dim = bottom[0]-&gt;num_axes(); const int start_axis = param.axis(); offsets_ = std::vector&lt;&lt;int&gt;(input_dim,0); vector&lt;int&gt; new_shape(bottom[0]-&gt;shape()); for (size_t i = 0; i &lt; dim; i++) &#123; int crop_offset = 0; //偏移量 int new_size = bottom[0]-&gt;shape(i);//每个shape的size // i &gt;= start_axis的时候才crop,否则不改变shape if ( i &gt;= start_axis) &#123; new_size = bottom[1].shape(i); if (param.offset_size() == 1) &#123;//如果只给出一个offset默认都一样 crop_offset = param.offset(0); &#125; else if(param.offset_size() &gt; 1)&#123; crop_offset = param.offset(i-start_axis); &#125; CHECK_GE(bottom[0]-&gt;shape(i),crop_offset+bottom[1]-&gt;shape(i)); &#125; new_shape[i] = new_size; offsets_[i] = crop_offset; &#125; top[0]-&gt;Reshape(new_shape); &#125; $Forward$ 的前向过程设计到元素复制的问题，使用 $crop_copy$ 函数单独实现，1234567891011121314151617181920template &lt;typename Dtype&gt;void CropLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123; vector&lt;int&gt;indices(top[0].num_axes(),0); const Dtype* bottom_data = bottom[0]-&gt;cpu_data();//输入 Dtype* top_data = top[0]-&gt;mutable_cpu_data();//输出 crop_copy(botoom,top,offsets,indices,0,botton_data,top_data,true);&#125;template &lt;typename Dtype&gt;void CropLayer&lt;Dtype&gt;::crop_copy(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vecotr&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,const vector&lt;int&gt;&amp; offsets, vector&lt;int&gt;&amp; indices,int cur_dim,const Dtype* src_data, const Dtype* dst_data,bool is_forward)&#123; //循环赋值每个维度 for (size_t i = 0; i &lt; top[0]-&gt;shape(cur_dim); i++) &#123; &#125;&#125; (2) $AccracyLayer$ Accracy_layer用以统计训练过程中样本预测的准确率，根据label值与top_K的得分标签的对比，来计算准确率，因此可以在prototxt中设置top_k参数，观察训练状况。1.基本数据成员1234567int label_axis_;//实际上就是第一个channels是labelint outer_num_;//BATCH_SIZEint inner_num_;//一般为 1 即H*Wint top_k;bool has_ignore_label;int ignore_label;Blob&lt;Dtype&gt;nums_buffer_;//统计每个类别的样本数量 2.基本成员函数 基本成员函数包括$LayerSetup$,$Reshape$,$forward$,其中参数的设置和读取发生在$LayerSetup$和$Reshape$上,acuracy可以显示训练中的信息，稍加修改也可以显示$Recall,F1$值等信息,同时 类别较少的时候，加入一个输出$top$，即可显示出每个类别的训练中的$accuracy$情况，具体实现如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990//layersetup 仅仅完成参数的读取template &lt;typename Dtype&gt;void AccuracyLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&gt;*&gt;&amp; bottom, const vector&lt;Blob&gt;*&gt;&amp; top)&#123; const AccuracyParameter&amp; param = this-&gt;layer_param_.accuracy_param(); top_k = param.top_k(); label_axis_ = bottom[0]-&gt;CanonicalAxisIndex(param.axis());; has_ignore_label = param.has_ignore_label(); if (has_ignore_label) &#123; ignore_label = param.ignore_label(); &#125;&#125;//Reshape// 多个top的时候，第一个top为整体的AC，第二个top为每个类别的actemplate &lt;typename Dtype&gt;void AccuracyLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&gt;*&gt;&amp; bottom, const vector&lt;Blob&gt;*&gt;&amp; top)&#123; outer_num_ = bottom[0]-&gt;count(0,label_axis_);//N inner_num_ = bottom[0]-&gt;count(label_axis_+1);//1*1 (H*W) vector&lt;int&gt;top_shape(0); top[0]-&gt;Reshape(top_shape); if (top.size() &gt; 1) &#123; //每个类别是一个向量，每个类别都需要统计单独的accuracy，而不是整体的 vector&lt;int&gt;top_shape_pre_class(1); top_shape_pre_class[0] = bottom[0]-&gt;shape(label_axis_);//N个类别 top[1]-&gt;Reshape(top_shape_pre_class); nums_buffer_.Reshape(top_shape_pre_class); &#125;&#125;//Forward_cpu,top[1]为C*1 ,Top[0]为1*1*1*1//前向过程，如果多个top则需要统计每个类别的accuracy保存到top[1]中template &lt;typename Dtype&gt;void AccracyLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); const Dtype* label = bottom[1]-&gt;cpu_data(); const int dim = bottom[0]-&gt;count()/outer_num_;//类别数目 if (top.size() &gt; 1) &#123; caffe_set(nums_buffer_.count(),0,nums_buffer_.mutable_cpu_data()); caffe_set(top[1]-&gt;count(),0,top[1]-&gt;mutable_cpu_data()); &#125; Dtype accuracy = 0; int count = 0; for (size_t i = 0; i &lt; outer_num_; i++) &#123; //N for (size_t j = 0; j &lt; inner_num_; j++) &#123; //1*1 const int label_value = static_cast&lt;int&gt;(label[i*inner_num_+j]); if (has_ignore_label &amp;&amp; ignore_label == label_value) &#123; continue;//当前label是忽略的label &#125; if (top-&gt;size() &gt; 1) &#123; nums_buffer_.mutable_cpu_data()[label_value]++;//类别数目+1 &#125; //看top_k的最大 std::vector&lt;std::pair&lt;Dtype,int&gt;&gt; bottom_data_vector; for (size_t k = 0; k &lt; dim ; k++) &#123; bottom_data_vector.push_back( std::make_pair(bottom_data[i*dim+k*inner_num_+j])); &#125; //最大堆排序 std::partial_sort( bottom_data_vector.begin(),bottom_data_vector.begin()+top_k, bottom_data_vector.end(),std::greater&lt;std::pair&lt;Dtype, int&gt;&gt;()); //查找top_k有没有真实的label for (size_t i = 0; i &lt; top_k; i++) &#123; if (bottom_data_vector[i].second == label_value) &#123; ++accuracy; if (top.size() &gt; 1) &#123; //每类样本预测正确的数目+1 top[1]-&gt;mutable_cpu_data()[label_value]++; &#125; break; &#125; &#125; count++; &#125; &#125; //全部mini的样本循环完成后 top[0]-&gt;mutable_cpu_data()[0] = accuracy/count; if (top.size() &gt; 1) &#123; for (size_t i = 0; i &lt; dim; i++) &#123;//dim表示类别 top[1]-&gt;mutable_cpu_data()[i] = nums_buffer_.cpu_data()[i] == 0 ? 0; top[1]-&gt;cpu_data()[i]/nums_buffer_.cpu_data()[i]; &#125; &#125;&#125; (3) $EltwiseLayer$ $EltwiseLayer$在深度网络中运用非常广泛，常用与多$layer$的合并，在$ResidualNet$中用以连接$block$与$x$部分，其组合方式有$prod,sum,max$,最常见的为$sum$和$max$,由于组合的方式有多种，因此在进行前向和后向的分析的时候需要按照多种情况进行分析，详细的代码解析如下所示：1.基本数据成员1234EltwiseParameter_EltwiseOp op_;// sum,prod,max 实际是个enum数据vector&lt;Dtype&gt; coeffs_; // 代表操作参数 如果-1，代表a-bBlob&lt;int&gt; max_idx;bool stable_prod_grad_;//只针对PROD，点乘模式 2.基本成员函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136 //LayerSetup 完成参数的读取 template &lt;typename Dtype&gt; void EltwiseLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const EltwiseParameter&amp; param = this-&gt;layer_param_.eltwise_param(); op_ = param.operation(); coeffs_ = vector&lt;Dtype&gt;(bottom.size(),1); if (param.coeff_size()) &#123;//每个layer的前面的标量 for (size_t i = 0; i &lt; param.coeff_size(); i++) &#123; coeffs_[i] = param.coeff(i); //1 -1等参数 &#125; &#125; stable_prod_grad_ = param.stable_prod_grad(); &#125; //Reshape过程，完成topshape的构造 template &lt;typename Dtype&gt; void EltwiseLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; //bottom的shape要完全一样 for (size_t i = 1; i &lt; bottom.size(); i++) &#123; CHECK_EQ(bottom[i]-&gt;shape() == bottom[0]-&gt;shape()) &#125; top[0]-&gt;ReshapeLike(*bottom[0]);//当然输出也要一样 if (this_-&gt;layer_param_.eltwise_param().operation()== EltwiseParameter_EltwiseOp_Max &amp;&amp; top.size() == 1) &#123; max_idx_.Reshape(bottom[0]-&gt;shape());//记录每个的maxid &#125; &#125;//Forward_cpu,完成layer的前向操作template &lt;typename Dtype&gt;void EltwiseLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; //临时变量,用以MAX操作 const Dtype* bottom_data = NULL; const int count = top[0]-&gt;count();// Dtype* top_data = top[0]-&gt;mutable_cpu_data(); switch (op_) &#123; case EltwiseParameter_EltwiseOp_PROD: caffe_mul(count,bottom[0]-&gt;cpu_data(),bottom[1]-&gt;cpu_data(),top_data); for (size_t i = 2; i &lt; bottom.size(); i++) &#123; caffe_mul(count,top_data,bottom[i]-&gt;cpu_data(),top_data); &#125; break; case EltwiseParameter_EltwiseOp_SUM: caffe_set(count,Dtype(0),top_data);//先初始输出为0 for (size_t i = 0; i &lt; bottom.size(); i++) &#123; caffe_axpy(count,coeffs_[i],bottom[i]-&gt;cpu_data(),top_data); &#125; break; case EltwiseParameter_EltwiseOp_MAX: caffe_set(count,-1,max_idx_.mutable_cpu_data()); caffe_set(count,Dtype(-FLT_MIN),top_data); for (size_t i = 0; i &lt; bottom.size(); i++) &#123; bottom_data = bottom[i]-&gt;cpu_data(); for (size_t j = 0; j &lt; count; j++) &#123; //整体遍历 if (bottom_data[j] &gt; top_data[j]) &#123; top_data[j] = bottom_data[j]; max_idx_.mutable_cpu_data()[j] = i; &#125; &#125; &#125; default: //Not Support; &#125;&#125;//Backward_cpu,完成layer的反向操作// 当method == SUM的时候，bottom_diff[i] = coeffs_[i]* top_diff;// 当method == product的时候,bottom_diff[i] = top_diff*top_data/bottom_data[i]// 当method == Max 的时候，bottom_diff[i] = top_diff*(j==max_idx_?1:0)template &lt;typename Dtype&gt;void EltwiseLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; const Dtype* top_diff = top[0]-&gt;cpu_diff(); const Dtype* top_data = top[0]-&gt;cpu_data(); const int cont = top[0]-&gt;count(); for (size_t i = 0; i &lt; bottom.size(); i++) &#123; if (propagate_down[i]) &#123; //需要backward才考虑 const Dtype* bottom_data = bottom[i]-&gt;cpu_data(); Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff(); switch (op_) &#123; //bottom_diff[i] = top_diff*top_data/bottom_data[i] case EltwiseParameter_EltwiseOp_PROD://点成操作 if (stable_prod_grad_) &#123; //渐进梯度的实现top_data/bottom[i] bool initiaized = false; for (size_t j = 0; j &lt; bottom.size(); j++) &#123; if(i == j) continue; //top/bottom[i] == bottom[j]连乘 if (!initiaized) &#123; //初始化 //用bottom[j]初始一下bottom_diff caffe_copy(count,bottom[j]-&gt;cpu_data(),bottom_diff); initiaized = true; &#125; else&#123; caffe_mul(count,bottom[j]-&gt;cpu_dat(),bottom_diff, bottom_diff); &#125; &#125; &#125; else&#123; caffe_div(count,top_data,bottom_data,bottom_diff); &#125; caffe_mul(count,bottom_diff,top_diff,bottom_diff); break; //bottom_diff[i] = coeffs_[i]* top_diff; case EltwiseParameter_EltwiseOp_SUM://sum操作 if (coeffs_[i] == Dtype(1)) &#123; caffe_copy(count,top_diff,bottom_diff); &#125; else&#123; caffe_scale(count,coeffs_[i],top_diff,bottom_diff); &#125; break; //bottom_diff[i] = top_diff*(j==max_idx_?1:0) case EltwiseParameter_EltwiseOp_MAX: //max操作 for (size_t j = 0; j &lt; count; j++) &#123; if (max_idx_.cpu_data()[j] == i) &#123; bottom_diff[j] = top_diff[j]; &#125; else&#123; bottom_diff[j] = Dtype(0); &#125; &#125; break; default: // Not Support &#125; &#125; &#125;&#125; $Eltwise$的$backward$的$product$模式有两种实现方式:(1) top_data/bottom[i];(2) $\prod_{j=0,j!=i}^{n-1}bottom[i]$ (4)$ConcatLayer$ 同$Eltwise$类似，$Concat$在多特征图的融合方面使用也极为广泛(denseNet,Dpn),$concat$中有$axis$和$concat_dim$控制的特征拼接的准则:通常做通道间融合,例如: $A：[32,112,256,256];B:[32,32,256,256]$,$concat_dim$为1,则输出的尺寸为 $[32,144,256,256]$1.基本数据成员123int num_concats_; // 合并通道前的值，一般为mini——batchint concat_input_size_; //合并通道后的SIze一般为H*Wint concat_axis_; // 合并的shape值，一般为1，即Channels合并 2.基本成员函数 基本成员函数包含$LayerSetup$,$Reshape$,$Forward$,$Backward$,具体实现如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// LayerSetup 主要包含参数的读取和判断是否合理template &lt;typename Dtype&gt;void ConcatLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const ConcatParameter&amp; param = this-&gt;layer_param_.concat_param(); // axis和concat_dim必须要有一个 CHECK(!(concat_param.has_axis() &amp;&amp; concat_param.has_concat_dim()));&#125;// Reshape，根据prototxt的参数 ,决定top的shapetemplate &lt;typename Dtype&gt;void ConcatLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const int num_axes = bottom[0]-&gt;num_axes();// 基本认为4 NCHW const ConcatParameter&amp; param = this-&gt;layer_param_.concat_param(); if (param.has_concat_dim()) &#123; concat_axis_ = static_cast&lt;int&gt;(param.concat_dim());// 拼接的Channel &#125; else&#123; concat_axis_ = bottom[0]-&gt;CanonicalAxisIndex(param.axis());//默认C融合 &#125; vector&lt;int&gt;top_shape = bottom[0]-&gt;shape();//先初始一下bottom[0]--top_shape num_concats_ = bottom[0]-&gt;count(0,concat_axis_); //N; concat_input_size_ = bottom[0]-&gt;count(concat_axis_+1);//H*W int bottom_count_sum = bottom[0]-&gt;count(); // 输出count for (size_t i = 1; i &lt; bottom.size(); i++) &#123; // 决定输出的Size CHECK_EQ(bottom[i]-&gt;num_axes(),num_axes);//NCHW 四维 for (size_t j = 0; j &lt; num_axes; j++) &#123; if (j == concat_axis_) &#123; top_shape[j] += bottom[i]-&gt;shape(j);//拼接 &#125; //除了合并的通道shape要求可以不同，其余的都要相同 CHECK_EQ(bottom[i].shape(j),top_shape[j]); &#125; bottom_count_sum += bottom[i]-&gt;count(); &#125; top[0]-&gt;Reshape(top_shape); //其实没必要，这不出错则之前就会报错 CHECK_EQ(bottom_count_sum,top[0]-&gt;count()); if (bottom.size() == 1) &#123;//类似于一个layer的拷贝 top[0]-&gt;ShareData(*bottom[0]); top[0]-&gt;ShareDiff(*bottom[0]); &#125;&#125;// Forward_cpu 前向过程，比较简单，// for循环完成整个的copy过程template &lt;typename Dtype&gt;void ConcatLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; if (bottom.size() == 1) &#123; return ;// Reshape 的时候完成了top的复制 &#125; Dtype* top_data = top[0]-&gt;mutable_cpu_data(); int offset = 0; // 合并channel的偏移 const int top_concat_axis = top[0]-&gt;shape(concat_axis_);// for (size_t i = 0; i &lt; bottom.size(); i++) &#123; const Dtype* bottom_data = bottom[i]-&gt;cpu_data(); const int bottom_concat_axis = bottom[i]-&gt;shape(concat_axis_);//当前C for (size_t n = 0; n &lt; num_concats_; n++) &#123; //样本的循环 //加入N*C*H*W,N循环，每次copy C*(H*W) 到top的 bottom_concat_axis*(H*W) // bottom是n*C_bottom*(H*W) top是( n*C_top + c)*(H*W) caffe_copy(bottom_concat_axis*concat_input_size_,//C_bottom*(H*W) bottom_data+n*bottom_concat_axis*concat_input_size_, top_data+(n*top_concat_axis+offset)*concat_input_size_); &#125; offset += bottom_concat_axis;//处理完一个bottom,offset+C_bottom &#125;&#125;// BackFord_cpu过程，由于使用的是concat,输出只是输入的拼接，因此// 只需要将top.diff 拆分为多块，每一块的bottom.diff对应top.diff// offset 每次加上bottom的Ctemplate &lt;typename Dtype&gt;void ConcatLayer&lt;Dtype&gt;::BackFord_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp;propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; if (bottom.size() == 1) &#123; return ;// Reshape的室友ShareDiff已经copy &#125; const Dtype* top_diff = cpu_diff(); // top层的loss int offset = 0; const int top_concat_axis = top[0]-&gt;shape(concat_axis_);//C_top for (size_t i = 0; i &lt; bottom.size(); i++) &#123; if (propagate_down[i]) &#123; int bottom_concat_axis = bottom[i]-&gt;shape(concat_axis_);//C_bottom Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff(); for (size_t n = 0; n &lt; num_concats_; n++) &#123; //样本遍历 caffe_copy(bottom_concat_axis*concat_input_size_,//count top_diff+(n*top_concat_axis+offset)*concat_input_size_, bottom_diff+n*bottom_concat_axis*concat_input_size_); &#125; offset += bottom_concat_axis; &#125; &#125;&#125; $(5) \, SliceLayer$ $SliceLayer$与$concat$是一个相反的过程，只不过$slice$是$bottom$分层，而$concat$是$bottom$的组合，通过$slice_point$来控制切片的格局,$axis$控制切片的通道.1.基本数据成员1234int num_slice_; //一般为N，即slice_axis的前面的乘积int slice_size_; //切成几片int slice_axis_; // NCHW哪一个开始切;vector&lt;int&gt;slice_point_;//prototxt的slice_point是134表示0-1 1-3 3-4 2.基本成员函数 基本成员函数包含$LayerSetup$,$Reshape$,$Forward$,$Backward$,具体实现如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115//LayerSetup 读取prototxt的参数template &lt;typename Dtype&gt;void SliceLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const SliceParameter&amp; param = this-&gt;layer_param_.slice_param(); //类似concat axis或者 slice_dim prototxt中选一个 CHECK(!(slice_param.has_axis() &amp;&amp; slice_param.has_slice_dim())); slice_point_.clear(); //其实就是把prototxt的slice_point参数push到slice_point_中 //可以写成for i:slice_point_size(),slice_point_.push_back() std::copy(param.slice_point().begin(), param.slice_point().end(),std::back_inserter(slice_point_));&#125;//Reshape ,top的size对应 slice_point_.size()+1 slice_point记录indextemplate &lt;typename Dtype&gt;void SliceLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const int num_axes = bottom[0]-&gt;num_axes(); const SliceParameter&amp; param = this-&gt;layer_param_.slice_param(); //这里判断slice_dim的原因是，axis有default = 1 if (param.has_slice_dim()) &#123; slice_axis_ = static_cast&lt;int&gt;(param.slice_dim()); //slice_axis_满足0----num_axes &#125; else&#123; slice_axis_ = bottom[0]-&gt;CanonicalAxisIndex(param.axis());//一般为1 &#125; //原始shape,后续只需要修改slice_axis_的shape即可 vector&lt;int&gt;top_shape = bottom[0]-&gt;shape(); const int bottom_slice_axis = bottom[0]-&gt;shape(slice_axis_);//切分的通道容量 num_slice_ = bottom[0]-&gt;count(0,slice_axis_);//一般为N slice_size = bottom[0]-&gt;count(slice_axis_+1);//一般为H*W int count = 0; if (slice_point_.size() != 0) &#123; CHECK_EQ(slice_point_.size(),top.size()-1); int prev = 0; vector&lt;int&gt; slices;//存放每个slice的Channels的大小 for (size_t i = 0; i &lt; slice_point_.size(); i++) &#123; CHECK_GT(slice_point_[i],prev);//slice_point_的值要递增 slices.push_back(slice_point_[i] - prev); prev = slice_point[i]; &#125; slices.push_back(bottom_slice_axis-prev); for (size_t i = 0; i &lt; top.size(); i++) &#123; top_shape[slice_axis_] = slices[i]; top[i]-&gt;Reshape(top_shape); count += top[i]-&gt;count(); &#125; &#125; // slice_point_ = 0,则根据top的size来进行均分 else&#123; CHECK_EQ(bottom_slice_axis % top.size(),0);//要整除 top_shape[slice_axis_] = bottom_slice_axis / top.size();//每块的Channel for (size_t i = 0; i &lt; top.size(); i++) &#123; top[i]-&gt;Reshape(top_shape); count += top[i].count(); &#125; &#125; CHECK_EQ(count,bottom[0]-&gt;count());//累加的top和bottom[0] count相同 if (top.size() == 1) &#123; top[0]-&gt;ShareData(*bottom[0]);// 类似于copy top[0]-&gt;ShareDiff(*bottom[0]);// 类似于copy &#125;&#125;// forward的过程，需要for top.size然后copy bottom的值// forward过程和concat的backward过程一致，只是diff--&gt;datatemplate &lt;typename Dtype&gt;void SliceLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; if (top.size() == 1) &#123; return ; &#125; int offset = 0; // 每次offset 加上 一个top的channels Const Dtype* bottom_data = bottom-&gt;cpu_data(); const int bottom_slice_axis = bottom[0]-&gt;shape(slice_axis_);//总C_bottom for (size_t i = 0; i &lt; top.size(); i++) &#123; const int top_slice_axis = top[i]-&gt;shape(slice_axis_);//当前C_top Dtype* top_data = top[i]-&gt;mutable_cpu_data(); for (size_t n = 0; n &lt; num_slice_; n++) &#123; //其实就是样本N caffe_copy(top_slice_axis*slice_size_, // 每次的copy_size C_TOP*H*W bottom+(n*bottom_slice_axis+offset)*slice_size_,//bottom地址 top+n*top_slice_axis*slice_size_);//top的第n个样本的起始地址 &#125; offset += top_slice_axis;// 每次bottom的channels偏移一个top的C_top &#125;&#125;// backward 反向的top的梯度对应bottom的一部分，因此反向类似于// concat的前向，因此可以写出如下代码template &lt;typename Dtype&gt;void SliceLayer&lt;Dtype&gt;::BackFord_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)&#123; if (top.size() == 1) &#123; return ; &#125; const int bottom_slice_axis = bottom[0]-&gt;shape(slice_axis_); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); int offset = 0; for (size_t i = 0; i &lt; top.size(); i++) &#123; if (propagate_down[i]) &#123; const int top_slice_axis = top[i]-&gt;shape(slice_axis_); const Dtype* top_diff = top[i]-&gt;cpu_diff(); for (size_t n = 0; n &lt; num_slice_; n++) &#123; // 样本数目 copy(top_slice_axis* slice_size_, // 每次copy的数据量 top_diff+n*top_slice_axis*slice_size_,//top的地址 bottom_diff+(n*bottom_slice_axis+offset)*slice_size_); &#125; offset += top_slice_axis; &#125; &#125;&#125; $(6)\,FlatternLayer$ $flatternLayer$实际完成输入维度的压缩，主要在$reshape$的操作上，123456789101112131415161718192021// Reshape 64*10*30*30 axis = 1，end_axis =2则 10*300*30template &lt;typename Dtype&gt;void FlatternLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)&#123; const FlatternParameter&amp; param = this&gt;layer_param_.flattern_param(); const int start_axis = bottom[0]-&gt;CanonicalAxisIndex(param.axis()); const int end_axis = bottom[0]-&gt;CanonicalAxisIndex(param.end_axis()); vector&lt;int&gt;top_shape; for (size_t i = 0; i &lt; start_axis; i++) &#123; top_shape.push_back(bottom[0]-&gt;shape(i));//前面的shape保持不变 &#125; const int flattern_dim = bottom[0]-&gt;count(start_axis,end_axis+1); top_shape.push_back(flattern_dim); for (size_t i = end_axis+1; i &lt; bottom[0].num_axes(); i++) &#123; top_shape.push_back(bottom[0]-&gt;shape(i)); &#125; top[0]-&gt;Reshape(top_shape);&#125;//forward 和 backward同正常的feedforwad相同 HCHW展开式相同的top[0]-&gt;ShareData(*bottom[0]);bottom[0]-&gt;ShareDiff(*top[0]); $(7)\, DropoutLayer$ $DropoutLayer$在深度学习的网络结构中对网络过拟合起到很大的作用，通过设置$drop_ratio$来控制网络的结点开闭，从而产生网络的异构多样性，降低网络的过拟合。1.基本数据成员 在训练阶段，结点是随机开闭的，但是在预测阶段，结点均开，但是输出会乘以概率p12345Blob&lt;unsigned int&gt; rand_vec_;//存放bottom对应位置随机出来的值Dtype threshold_;// drop的阈值Dtype scale_ ;// scale因子,由于结点闭合的原因，开放的结点需要乘以的因子// 这里判断是否参数训练的时候乘以了scale,结点少了每个结点权重提高bool scale_train_; 2.基本成员函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 //LayerSetup,类似于activation，读取参数 template &lt;typename Dtype&gt; void DropoutLayer&lt;Dtype&gt;::LayerSetup(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123; NeuronLayer&lt;Dtype&gt;::LayerSetUp(bottom, top); const DropoutParamter&amp; param = this-&gt;layer_param_.dropout_param(); threshold_ = param.dropout_ratio(); scale_ = 1./(1-threshold_);// 测试的时候开放的scale scale_train_ = param.scale_train(); unit_thres_ = static_cast&lt;unsigned int&gt;(UINT_MAX*threshold_); &#125; // Reshape // 类似于激励函数，只是有的toplayer会闭合置零 template &lt;typename Dtype&gt; void DropoutLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123; NeuronLayer&lt;Dtype&gt;::Reshape(botom,top); rand_vec_.Reshape(bottom[0]-&gt;shape()); &#125; template &lt;typename Dtype&gt; void DropoutLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; top)&#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); const int count = bottom[0]-&gt;count(); Dtype top_data = top[0]-&gt;mutable_cpu_data(); if (this-&gt;phase_ == "TRAIN") &#123; // 如果是训练阶段 cafe_rng_bernoulli(count,1.-threshold_,rand_vec_.mutable_cpu_data()); if (scale_train_) &#123; //训练时是否每个结点都提高权重 for (size_t i = 0; i &lt; count; i++) &#123; //rand_vec_为1表示保留，为0表示闭合drop top_data[i] = bottom_data[i]*rand_vec_.cpu_data()[i]*scale_; &#125; &#125; else&#123; for (size_t i = 0; i &lt; count; i++) &#123; top_data[i] = bottom_data[i]*rand_vec_.cpu_data()[i]; &#125; &#125; &#125; //测试阶段全开，如果训练提高权重则不处理，反之则测试的时候除以权重 else&#123; caffe_copy(count,bottom_data,top_data); if (!scale_train_) &#123; caffe_scal&lt;Dtype&gt;(count,1./scale,top_data); &#125; &#125; &#125; // backward_cpu过程,开的才会有偏导，根据rand_vec_的值来确定 template &lt;typename Dtype&gt; void DropoutLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,const vector&lt;Blob&lt;Dtype&gt;* &gt;&amp; bottom)&#123; const int count = bottom[0]-&gt;count(); const Dtype* top_diff = top[0]-&gt;cpu_diff(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); if (propagate_down[0]) &#123; const unsigned int* mask = rand_vec_-&gt;cpu_data(); if (this-&gt;phase_ == "TRAIN") &#123; if (scale_train_) &#123; for (size_t i = 0; i &lt; count; i++) &#123; bottom_diff[i] = top_diff[i]*mask[i]*scale_; &#125; &#125; else&#123;//训练的时候没有乘以扩增因子 for (size_t i = 0; i &lt; count; i++) &#123; bottom_diff[i] = top_diff[i]*mask[i]; &#125; &#125; &#125; // 测试的时候 else&#123; caffe_copy(count,top_diff,bottom_diff); if (!scale_train_) &#123; caffe_scal&lt;Dtype&gt;(count,1./scale_,bottom_diff); &#125; &#125; &#125;&#125; 本文作者： 张峰本文链接：https://zhanglaplace.github.io/2017/10/20版权声明：本博客所有文章，均采用CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Net分析]]></title>
    <url>%2F2017%2F10%2F19%2FCaffe_Net%2F</url>
    <content type="text"><![CDATA[Caffe_Net1.基本数据1234567891011vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_; // 记录每一层的layer参数vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_;vector&lt;vector&lt;int&gt; &gt; bottom_id_vecs_;vector&lt;vector&lt;bool&gt; &gt; bottom_need_backward_;/// top_vecs stores the vectors containing the output for each layervector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_;vector&lt;vector&lt;int&gt; &gt; top_id_vecs_;vector&lt;vector&lt;int&gt; &gt; param_id_vecs_;vector&lt;string&gt; layer_names_;//learnable_params_[learnable_param_ids_[i]] == params_[i].get()vector&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;//层间权重与bias 2. 常用的函数介绍了Caffe内的Net的常用函数: 12345678910111213141516171819202122232425262728293031323334353637383940414243 const string&amp; name()&#123;return name_;&#125;//网络的名称 const vector&lt;string&gt;&amp; layer_names&#123;return layer_names_;&#125;// net每层的layer名称 // net内每层的layer的Blob名称 const vector&lt;string&gt;&amp; blob_names()&#123;return blob_names_;&#125; //net内层次间的权值与bias const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt;&gt;&gt;&amp; blobs()&#123;return blob_;&#125;; //net内的layers const vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt;&gt;&gt;&amp; layers()&#123;return layers_;&#125;; //net-&gt;bottom_vecs() 返回该layer的输入，输出向量， //以及具体的 top_id_vecs[layer_id][top_id]; const vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; bottom_vecs()&#123; return bottom_vecs_;&#125; const vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; top_vecs() &#123; return top_vecs_;&#125; const vector&lt;vector&lt;int&gt; &gt;&amp; bottom_id_vecs()&#123; return bottom_id_vecs_;&#125; const vector&lt;vector&lt;int&gt; &gt;&amp; top_id_vecs() &#123; return top_id_vecs_;&#125; void CopyTrainedLayersFrom(const string trained_filename);//加载权重 //网络的输入输出 //感觉等效于bottom_vecs_[0] const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; input_blobs()&#123;return net_input_blobs_;&#125; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; output_blobs() &#123;return net_output_blobs;&#125;//top_vecs[top_vecs.size()-1]; const int num_input()&#123;return net_input_blobs_.size()&#125;;//输入blob的size //has_blob()然后find return const shared_ptr&lt;Blob&lt;Dtype&gt;&gt;blob_by_name(const string&amp; blob_name);// 前向计算loss和网络的输出const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; forward(Dtype* loss = NULL);// --- *loss = ForwardFromTo(0.layers_.size()-1);// --- 此处调用 Dtype* Net&lt;Dtype&gt;::ForwardFrom(int start,int end)for (size_t i = start; i &lt; end; i++)&#123; //多态，调用具体的Layer的Forward函数,并返回该层次的loss Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i],top_vecs_[i]); loss += layer_loss;&#125;return loss;// backward反向，更新权值void Net&lt;Dtype&gt;::Backward()&#123; // BackwardFromTo(layers_size()-1,0); // 具体函数实现如第三部分 if (debug_info_) &#123; /*层次的参数*/ &#125;&#125; 3.具体函数实现1234567891011121314151617181920212223242526272829303132333435template &lt;typename Dtype&gt; const int Net&lt;Dtype&gt;::AppendBottom(const NetParamter&amp; param, int layer_id,int bottom_id,set&lt;string&gt;* availabel_blobs,map&lt;string,int&gt;* blob_name_to_idx)&#123; const LayerParammeter&amp; layer_param = param.layer(layer_id); const string&amp; blob_name = layer_param.bottom(bottom_id); const int blob_id = (*blob_name_to_idx)[blob_name]; //layer输入的shape等 bottom_vecs_[layer_id].push_back(blobs_[blob_id].get()); bottom_id_vecs_[layer_id].push_back(blob_id); //LOG CONV&lt;--data 等,只要是丢入输入 &#125; // learnable_params_ //conv的shape一般为num_output*input_channels*kernel_width*kernel_height //bias的shape一般为Num_output template &lt;typename Dtype&gt; void Net&lt;Dtype&gt;::AppendParam(const NetParameter&amp; param, const int layer_id, const int param_id) &#123; const int learnable_param_id = learnable_params_.size(); learnable_params_.push_back(params_[net_param_id].get()); learnable_param_ids_.push_back(learnable_param_id); has_params_lr_.push_back(param_spec-&gt;has_lr_mult()); has_params_decay_.push_back(param_spec-&gt;has_decay_mult()); params_lr_.push_back(param_spec-&gt;lr_mult()); params_weight_decay_.push_back(param_spec-&gt;decay_mult()); &#125; template &lt;typename Dtype&gt; void Net&lt;Dtype&gt;::BackwardFromTo(int start,int end)&#123; for(int i = start;i &gt;= end;--i)&#123; //backward 调用各层次的backward更新权值和bias layers_[i].Backward(top_vecs_[i],bottom_need_backward_[i], bottom_vecs_[i]); &#125; &#125; 4.基本流程 基本流程：Net构造函数开始 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 递归更新变量vectot&lt;string&gt;*stage ;int level;//起始调用net_.reset(new Net&lt;float&gt;(model_file, TEST));//送入prototxt文件和Test OR Trainexplicit Net(const string&amp; param_file,Phase phase,const int level = 0, vector&lt;string&gt;* stage = NULL,const Net* root_net = NULL);// 解析保存在NetParamter param内,这里用到了//protobuf::TextFormat::Parse(FileInputStream*,param)ReadNetParamsFromTextFileOrDie(param_file,&amp;param);// 读取了NetParamter 后需要进行整个网络的初始化工作Init(param); //初始化网络的接口，下续为具体实现FilterNet(param, &amp;filtered_param);// 打印网络结构/*内部会完成split added 如果有必要(残差结构),记录层与层之间的联系关系与层次的名称等，是否有loss_weight，layer的size等*/InsertSplits(filtered_param,&amp;param);for (size_t i = 0; i &lt; param.layer_size(); i++) &#123; //遍历setupLayer const LayerParammeter&amp; layer_param = param.layer(i);//层次的参数 layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param)); // CreateLayer会走layer_factory的CreateLayer的注册 ,比如input,conv,bn... layer_names_.push_back(layer_param.name()); //开始继续遍历每层输入的具体的细节,第i个layer的第botom_id个输入 for (size_t bottom_id = 0; bottom_id &lt; layer_param.bottom_size(); bottom_id++) &#123; const int blob_id = AppendBottom(param,i,bottom_id,&amp;availabel_blobs,&amp;blob_name_to_idx); &#125; //开始继续遍历每层输出的具体细节，第i个layer的第 top_id的输出 for (size_t top_id = 0; top_id &lt; layer_param.top_size(); top_id++) &#123; AppendTop(param,i,top_id,&amp;availabel_blobs,&amp;blob_name_to_idx); if (layer_param.type()== "Input") &#123;//输入 const int blob_id = blobs_.size() - 1; net_input_blob_indices_.push_back(blob_id); net_input_blobs_.push_back(blobs_[blob_id].get()); &#125; &#125; //多态，具体调用具体的layer的Setup函数 layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]); //每个输出遍历 for (size_t top_id = 0; top_id &lt; top_vecs_[layer_id].size(); top_id++) &#123; /*完成层次的blob_loss_weights,并统计memory_used_*/; memory_used_ += top_vecs_[layer_id][top_id]-&gt;count(); &#125; //总的memory_used_: memory_used_*sizeof(Dtype); //如果层次间有学习权值和偏置，则需要再次设置，比如conv //num_param_blobs weights And bias // relu pooling等层无中间权值参数，则num_param_blobs = 0 for (int param_id = 0; param_id &lt; num_param_blobs; ++param_id) &#123; AppendParam(param, layer_id, param_id); &#125;&#125;/*接下来需要研究网络的backwards问题，决定哪些层次对loss有贡献，并且检查哪些层次不需要back_propagate_down操作，遍历是反向的操作一个layer是否需要回溯计算，主要依据两个方面：(1)该layer的top blob 是否参与loss的计算；(2):该layer的bottomblob 是否需要回溯计算，比如Data层一般就不需要backward computation */for (size_t layer_id = layers_.size()-1; layer_id &gt;= 0; --layer_id)&#123; bool layer_contributes_loss = false;//默认是无贡献的 bool layer_skip_propagate_down = true;// 默认不参与backwards的loss贡献 //Layer内的输出遍历 for (size_t top_id = 0; top_id &lt; top_vecs_[layer_id].size(); top_id++) &#123; //blob_name_[index]名字 string&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]]; if (layer_[layer_id]-&gt;loss(top_id)|| blobs_under_loss.find(blob_name) != blobs_under_loss.end()) &#123; //该层次的layerloss不为0或者loss_weight = 1; layer_contributes_loss = true; &#125; if (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) &#123; layer_skip_propagate_down = false; &#125; &#125; //同理 Layer内的输入遍历 for (size_t bottom_id = 0; bottom_id &lt; bottom_vecs_[layer_id].size(); bottom_id++) &#123; if (layer_contributes_loss) &#123; string* blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]]; blobs_under_loss.insert(blob_name); &#125; else&#123; bottom_need_backward_[layer_id][bottom_id] = false; &#125; if (!bottom_need_backward_[layer_id][bottom_id]) &#123; string&amp;blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]]; blok_skip_backp.insert(blob_name); &#125; &#125; /*code*/&#125;//init函数尾 5.说明 blob_name_to_idx是一个局部变量，其实它是在当前layer的top blob 和下一层的bottom blob间起着一个桥梁作用。 blob_name_to_idx中元素的pair是从网络最开始一层一层搭建的过程中压入map的，其中的name和id都是不重复的。name是关键字，不重复是map数据结构的必然要求，id也是不重复的，—0,1,2…blob_name_to_idx和blobs_一样，在”Normal output”的情形下，每次遍历到一个top blob的时候都会更新。 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/10/19/Caffe_Net/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Layer分析]]></title>
    <url>%2F2017%2F10%2F19%2FCaffe_layer%2F</url>
    <content type="text"><![CDATA[Caffe_Layers1.基本数据结构1234//Layer层主要的的参数LayerParamter layer_param_; // protobuf内的layer参数vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt;*&gt;&gt;blobs_;//存储layer的参数，vector&lt;bool&gt;param_propagate_down_;//表示是否计算各个blobs反向误差。 2.主要函数接口123456virtual void SetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom, vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);Dtype Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;bottom, vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;top);void Backward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;top,const vector&lt;bool&gt;param_propagate_down,vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); 3.具体的Layer分析具体的常用Layer分析 (1) 数据层(DataLayer)数据通过数据层进入Layer,可以来自于数据库(LevelDB或者LMDB),也可以来自内存，HDF5等12345678910111213//Database：类型 Database//必须参数 source,batch_size//可选参数：rand_skip,mirror,backend[default LEVELDB]// In-Memory：类型 MemoryData// 必选参数：batch_size，channels,height,width//HDF5 Input:类型 HDF5Data//必选参数: source,batch_size//Images : 类型 ImageData//必要参数：source(文件名label),batch_size//可选参数：rand_skip,shuffle,new_width,new_height; (2) 激励层(neuron_layers)一般来说，激励层是element-wise，输入输出大小相同，一般非线性函数 输入：n\*c\*h\*w 输出：n\*c\*h\*w 12345678910111213141516171819//ReLU/PReLU//可选参数 negative_slope 指定输入值小于零时的输出。// f(x) = x*(x&gt;0)+negative_slope*(x&lt;=0)//ReLU目前使用最为广泛，收敛快，解决梯度弥散问题layer&#123; name:"relu" type:"ReLU" bottom:"conv1" top:"conv1"&#125;//Sigmoid//f(x) = 1./(1+exp(-x)); 负无穷--正无穷映射到-1---1layer&#123; name:"sigmoid-test" bottom:"conv1" top:"conv1" type:"Sigmoid"&#125; (3) 视觉层(vision_layer)常用layer操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 //卷积层(Convolution):类型Convolution //包含学习率，输出卷积核，卷积核size，初始方式，权值衰减 //假使输入n*ci*hi*wi,则输出 // new_h = ((hi-kernel_h)+2*pad_h)/stride+1; // new_w = ((wi-kernel_w)+2*pad_w)/stride+1; //输出n*num_output*new_h*new_w; layer&#123; name: "conv1" type: "CONVOLUTION" bottom: "data" top: "conv1" blobs_lr: 1 blobs_lr: 2 weight_decay: 1 weight_decay: 0 convolution_param &#123; num_output: 96 kernel_size: 11 stride: 4 weight_filler &#123; type: "gaussian" std: 0.01 &#125; bias_filler &#123; type: "constant" value: 0 &#125; &#125; &#125;//池化层(Pooling) 类型 Pooling// (hi-kernel_h)/2+1;layer&#123; name:"pool1" type:"POOLING" bottom:"conv1" top:"conv1" pooling_param&#123; pool:MAX //AVE,STOCHASTIC kernel_size:3 stride:2 &#125;&#125;//BatchNormalization// x' = (x-u)/δ ;y = α*x'+β; (4) 损失层(Loss_layer)最小化输出于目标的LOSS来驱动学习更新 1//Softmax 4.说明SetUp函数需要根据实际的参数设置进行实现，对各种类型的参数初始化；Forward和Backward对应前向计算和反向更新，输入统一都是bottom，输出为top，其中Backward里面有个propagate_down参数，用来表示该Layer是否反向传播参数。在Forward和Backward的具体实现里，会根据Caffe::mode()进行对应的操作，即使用cpu或者gpu进行计算，两个都实现了对应的接口Forward_cpu、Forward_gpu和Backward_cpu、Backward_gpu，这些接口都是virtual，具体还是要根据layer的类型进行对应的计算（注意：有些layer并没有GPU计算的实现，所以封装时加入了CPU的计算作为后备）。另外，还实现了ToProto的接口，将Layer的参数写入到protocol buffer文件中。 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/10/19/Caffe_layer/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Blob分析]]></title>
    <url>%2F2017%2F10%2F18%2FCaffe_blob%2F</url>
    <content type="text"><![CDATA[Caffe_blob1.基本数据结构 Blob为模板类，可以理解为四维数组，n * c * h * w的结构,Layer内为blob输入data和diff，Layer间的blob为学习的参数.内部封装了SyncedMemory类,该类负责存储分配和主机与设备的同步12345678protected: shared_ptr&lt;SyncedMemory&gt; data_; // data指针 shared_ptr&lt;SyncedMemory&gt; diff_; // diff指针 vector&lt;int&gt; shape_; // blob形状 int count_; // blob的nchw // 当前的Blob容量，当Blob reshape后count&gt; capacity_时，capacity_ = count_; // 重新new 然后 reset data和 diff int capacity_; 2.常用函数Blob类中常用的函数如下所示 1234567891011121314151617181920212223242526272829Blob&lt;float&gt;test;//explicit关键字的作用是禁止单参数构造函数的隐式转换explicit Blob(const int num, const int channels, const int height, const int width);test.shape_string();//初始为空 0 0 0 0//Reshape函数将num,channels,height,width传递给vector shape_test.Reshape(1,2,3,4);// shape_string() 1,2,3,4test.shape(i);// NCHWtest.count(int start_axis,int end_axis); // start_axis---end_axis .x* shape[i]test.count();// nchw count(1) chw count(2) hw.....//shared_ptr&lt;SyncedMemory&gt; data_-&gt;cpu_data();const float* data = test.cpu_data();const float* diff = test.cpu_diff();float* data_1 = test.mutable_cpu_data();//mutable修饰的表示可以修改内部值float* diff_1 = test.mutable_cpu_diff();test.asum_data();//求和 L1范数test.sumsq_data();//平方和 L2范数test.Update();//data = data-diff;a.ToProto(BlobProto&amp; bp,true/false);//(FromProto)// if &lt; 0 ,return num_axis()+axis_index;//索引序列int index = a.CanonicalAxisIndex(int axis_index);int offset(n,c,h,w);//((n*channels()+c)*height()+h)*width()+wfloat data_at(n,c,h,w);//return cpu_data()[offset(n,c,h,w)];float diff_at(n,c,h,w);//return cpu_diff()[offset(n,c,h,w)];inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const&#123;return _data&#125;;void scale_data(Dtype scale_factor);// data乘以一个标量。同理 scale_diff();void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false); // copy_diff是否复制diff 3.写入磁盘操作123456789101112131415161718192021222324//Blob内部值写入到磁盘Blob&lt;float&gt;a;a.Reshape(1,2,3,4);const int count = a.count();for (size_t i = 0; i &lt; count; i++) &#123; a[i] = i;//init the test Blob&#125;BlobProto bp,bp2;a.ToProto(&amp;bp,true);//写入data和diff到bp中WriteProtoToBinaryFile(bp,"a.blob");//写入磁盘ReadProtoFromBinaryFile("a.blob",&amp;bp2);//从磁盘读取blobBlob&lt;float&gt;b;b.FromProto(bp2,true);//序列化对象bp2中克隆b，完整克隆for (size_t n = 0; n &lt; b.num(); n++) &#123; for (size_t c = 0; c &lt; b.channels(); c++) &#123; for (size_t h = 0; h &lt; b.height(); h++) &#123; for (size_t w = 0; w &lt; b.width(); w++) &#123; cout&lt;&lt;"b["&lt;&lt;n&lt;&lt;"]["&lt;&lt;c&lt;&lt;"]["&lt;&lt;h&lt;&lt;"]["&lt;&lt;w&lt;&lt;"]["&lt;&lt;w&lt;&lt;"]="&lt;&lt; b[(((n*b.channels()+c)*b.height)+h)*b.width()+w]&lt;&lt;endl; //(((n*c+ci)*h+hi)*w+wi) &#125; &#125; &#125;&#125; 4.部分函数的具体实现本部分的实现未考虑参数是否合理。一般操作blob需要分CPU和GPU,采用math_functions具体计算 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 template &lt;typename Dtype&gt; void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape)&#123;//reshape操作 count_ = 1;//初始count_ NCHW; shape_.resize(shape.size()); for (size_t i = 0; i &lt; shape.size(); i++) &#123; count_ *= shape[i]; shape_[i] = shape[i]; if (count_ &gt; capacity_) &#123; //reshape的size大于了目前的最大容量 capacity_ = count_; data_.reset(new SyncedMemory(capacity_*sizeof(Dtype))); diff_.reset(new SyncedMemory(capacity_*sizeof(Dtype))); &#125; &#125; &#125; template &lt;typename Dtype&gt; void Blob&lt;Dtype&gt;::Reshape(int n,int c,int h ,int w)&#123;//reshape操作 vector&lt;int&gt;shape(4); shape[0] = n; shape[1] = c; shape[2] = h; shape[3] = w; Reshape(shape); &#125; template &lt;typename Dtype&gt; const Dtype* Blob&lt;Dtype&gt;::cpu_data()&#123; //实际调用的shared_ptr&lt;SyncedMemory&gt;data_-&gt;cpu_data();,同理cpu_diff(); CHECK(data_); return (const Dtype*)data_-&gt;cpu_data(); &#125;template &lt;typename Dtype&gt;void Blob&lt;Dtype&gt;::Updata()&#123; //data = data-diff;需要判断cpu OR gpu switch (data_-&gt;head()) &#123; case SyncedMemory::HEAD_AT_CPU: caffe_axpy&lt;Dtype&gt;(count_,Dtype(-1), static_cast&lt;const&lt;Dtype*&gt;(diff_-&gt;cpu_data()), static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data())); &#125; case SyncedMemory::HEAD_AT_GPU://在gpu或者CPU/GPU已经同步 case SyncedMemory::SYNCED: #ifndef CPU_ONLY caffe_gpu_axpy&lt;Dtype&gt;(count_.Dtype(-1), static_cast&lt;const&lt;Dtype*&gt;(diff_-&gt;gpu_data()), static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()))&#125;template &lt;typename Dtype&gt; //从source 拷贝数据,copy_diff控制是拷贝diff还是datavoid Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) &#123; if (source.count() != count_ || source.shape() != shape_) &#123; if (reshape) &#123; ReshapeLike(source); &#125; &#125; switch (Caffe::mode()) &#123; case Caffe::GPU: if (copy_diff) &#123; //copy diff caffe_copy(count_, source.gpu_diff(), static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data())); &#125; else &#123; caffe_copy(count_, source.gpu_data(), static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data())); &#125; break; case Caffe::CPU: if (copy_diff) &#123; caffe_copy(count_, source.cpu_diff(), static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data())); &#125; else &#123; caffe_copy(count_, source.cpu_data(), static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data())); &#125; break; default: LOG(FATAL) &lt;&lt; "Unknown caffe mode."; &#125;&#125;template &lt;typename Dtype&gt;void Blob&lt;Dtype&gt;::ToProto(BlobProto* proto,bool write_diff)&#123; proto-&gt;clear_shape(); for (size_t i = 0; i &lt; shaoe_.size(); i++) &#123; proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]); &#125; proto-&gt;clear_data(); proto-&gt;clear_diff(); const Dtype* data_vec = cpu_data(); for (size_t i = 0; i &lt; count_; i++) &#123; proto-&gt;add_data(data_vec[i]);//data写入proto &#125; if (write_diff) &#123; const Dtype* diff_vec = cpu_diff(); for (size_t i = 0; i &lt; count_; i++) &#123; proto-&gt;add_diff(diff_vec[i]);//diff写入proto &#125; &#125;&#125; 5.说明123456/*Blob作为一个最基础的类，其中构造函数开辟一个内存空间来存储数据，Reshape函数在Layer中的reshape或者forward操作中来调整top的输出维度。同时在改变Blob大小时， 内存将会被重新分配如果内存大小不够了，并且额外的内存将不会被释放。对input的blob进行reshape, 若立马调用Net::Backward是会出错的，因为reshape之后，要么Net::forward或者Net::Reshape就会被调用来将新的input shape传播到高层 */ 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/10/18/Caffe_blob/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法 1-统计学习算法概述]]></title>
    <url>%2F2017%2F09%2F14%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[统计学习的主要特点统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。 分类监督学习(supervised learning) 无监督学习(unsupervised learning) 半监督学习(self-supervised learning) 增强式学习(reinfoucement learning) 监督学习(supervised learning)输入实际x的特征向量记做$x = (x^{(1)},x^{(2)},x^{(3)}, \cdots ,x^{(n)})^T$训练集：$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),\cdots (x_n,y_n)}$输入变量与输出变量均为连续变量的预测问题为回归问题；输出变量为有限个离散变量的预测问题为分类问题; 本文作者： 张峰本文链接：https://zhanglaplace.github.io/2017/09/14版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2017%2F09%2F10%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[Model and Cost Function(模型和损失函数)对于model，给出如下定义 $y = \theta x$损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$Gradient descent algorithmrepeat until convergence{ $\quad \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$} SVM寻找两类样本正中间的划分超平面，因为该超平面对训练样本的布局扰动的容忍度最好，是最鲁棒的划分超平面方程:$$wx+b = 0$$我们假使$$\begin{cases}wx_i+b &gt;= 1 \qquad\quad y_i = +1 \\\\\wx_i+b &lt;=-1 \qquad\, y_i = -1\end{cases}$$则距离超平面最近的几个点使得下列式子成立$$\max\limits_{w,b}(\frac{2}{||w||}) \rightarrow \min_{w,b}\frac{1}{2}||w||^2$$$$s.t. y_i(wx_i+b)\ge 1 i = 1,2,…,m.$$通用表达式: $f(x)=w\psi(x)+b = \sum_{i=1}^{m}a_iy_i\psi(x_i)^T\psi(x)+b=\sum_{i=1}^{m}a_iy_i\kappa(x,x_i)+b$$\kappa 为核函数.$ 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/09/10/Linear-Regression/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++随笔]]></title>
    <url>%2F2017%2F09%2F08%2FC%2B%2B%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[重写，重定义、重载的区别 重写$\qquad 子类(派生类)重新定义基类的虚函数方法，要求函数名，函数参数，返回类型完全相同.并\\$$且基于必须是虚函数，不能有static关键字,重写函数的访问修饰符可以与基类的不同。\\$$\qquad 基类指针指向派生类，若实现了重写，则调用派生类，若没，则调用基类,即实现多态$ 重定义$\qquad 子类(派生类)重新申明和定义基类的函数，要求函数名相同，但是返回值可以不同，参数\\$$不同，无论有无virtual，基类的都将被隐藏，参数相同，基类如果没有virtual，则基类的函被\\$$隐藏$ 重载$\qquad函数名相同，但是他们的参数列表个数或者顺序，类型不同，且不能仅有返回类型不同，要\\$$求再同一个作用于.$ 多态的实现方式概念$\qquad 多态: 即程序运行中，系统根据对象指针所指向的类别对相同的消息进行不同的方法处理$ 动态多态$\qquad 通过类的继承和虚函数机制，在程序运行期实现多态,虚函数表$ 静态多态$\qquad 函数重载；运算符重载$ 常用排序算法快速排序$\qquad 快速排序的实现:$12345678910111213141516171819void quickSort(int a[],int l ,int r)&#123;//或者vector if(l &lt; r)&#123; int i = l ,j = r ; int sed = a[i];//种子点 while(i &lt; j )&#123; while(i &lt; j &amp;&amp; a[j] &gt; sed ) --j; if(i &lt; j) a[i++] = a[j]; while(i &lt; j &amp;&amp; a[i] &lt; sed ) ++i; if(i &lt; j) a[j--] = a[i]; &#125; a[i] = sed; quickSort(a,l,i-1); qucikSort(a,i+1,r); &#125;&#125; 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/09/08/C++%E9%9A%8F%E7%AC%94/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归分析]]></title>
    <url>%2F2017%2F09%2F07%2Flogistic%2F</url>
    <content type="text"><![CDATA[Logistic回归分析$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$ $\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i} (其中 b等于w_{0}，x_{0}等于1)则:$$$f(x) = \frac{1}{1+exp(-z)}$$ $\quad \quad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$ $$trainingData \quad\, x^1 \quad\, x^2 \quad\, x^3 \quad\,\cdots \quad\, x^n $$ $\qquad \qquad \qquad \qquad \qquad \qquad labels \qquad \quad C_{1} \quad C_{1} \quad C_{2} \quad \cdots \quad C_{1} \\$$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^n)$来寻找最佳的$w$和$b$$$w^{*},b^{*} = arg\max\limits_{w,b}(L(w,b))\Longrightarrow w^{*},b^{*} = arg\min\limits_{w,b}(-ln{L(w,b)})$$ 随机梯度下降法$\qquad 我们需要优化的函数:-ln{L(w,b)} = -{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)}\quad \\$$$\qquad 假设：\begin{cases}\hat{y} = 1 \qquad x\in1 \\\\\\hat{y} = 0 \qquad x\in0\end{cases}\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n w_{i}x_{i} 则$$$\qquad 我们需要优化的函数简化为：ln{L(w,b)} =\sum_{j=1}^{n}{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))} \\$ $\qquad 当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x) \\$$\qquad 当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x)) \qquad \\$$\qquad 即均满足上式 , 因此:$ $\qquad \qquad \quad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$ $\qquad \quad \quad 而 \, \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\$ $\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\$ $\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\$ $\qquad \qquad \qquad \qquad \quad=(1-f_{w,b}(x))*x_i \\$ $\quad \quad 同理 \quad \frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i \qquad 则化简后:\\$$\qquad \quad\,\, \qquad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$ $\qquad \qquad \qquad \quad \qquad = \sum_{j=1}^{n}{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i} \\$ $\qquad \qquad \quad\qquad \qquad = \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i \\$ $\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i \\$ 思考题1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数$答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，\\$$由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：\\$$$\frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i $$$则当\,\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad \\$$但是当\,\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对 \\$ 2. $logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数$答: 我们假设样本的分布服从二次高斯分布，即\\$ $f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)},其中\mu为均值，\Sigma为协方差矩阵 \\$ $输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma, \\$$因此需要求取最佳的高斯分布来满足样本的分布 \\$ $$Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots f_{\mu,\Sigma}(x^{N})$$$$\mu^{*}，\Sigma^{*} = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma)$$$$\mu^{*} = \frac{1}{N}\sum_{i=0}^{N}{x^i}$$$$\Sigma^{*} = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^{*})(x^i-\mu^{*})^T}$$ $对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma \\$$$\mu^1 = \sum_{i=1}^{n_1} x_i\qquad (x_i \in C_1) \quad ;\quad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) $$$$\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ;\quad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T ;\quad \Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2 $$ $对于样本x，如果属于C_1则有：\\$ $\qquad \qquad\qquad \qquad P(C_{1}|x) \,\,= \frac{P(C_{1},x)}{P(x)} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+\frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+exp(-\alpha)} \\$ $其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})$ $将P(x|C_i)带入高斯分布的公式:\\$$$P(C_1)=\frac{n_1}{n_1+n_2}\quad , \quad P(C_2)=\frac{n_2}{n_1+n_2} $$$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)} $$$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)} $$$\alpha= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\$$\quad =-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\$$\quad =-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad = (u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad = wx+b\\$$\quad w = (u^1-u^2)^T(\Sigma)^{-1} \quad ; \quad b=-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的 \\$ 生成模型与判别模型生成模型基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候， 都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss 判别模型基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其 效果一般比生成模型好(基于事实 not 基于猜测) 小扩展多分类基于先验概率得出的每个类别的后验概率为softmax函数，即： $\\$$\qquad \qquad \qquad \qquad \, P(C_i|x) = \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\$ $\qquad \qquad \qquad \qquad \qquad \qquad = \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}\\$ 待续未完待续 本文作者： 张峰本文链接： https://zhanglaplace.github.io/2017/09/07/logistic/版权声明： 本博客所有文章，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
</search>
